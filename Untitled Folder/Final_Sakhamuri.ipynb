{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamsi Sakhamuri\n",
    "# MIDS Machine Learning at Scale\n",
    "# W261-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ET1:c\n",
    "\n",
    "ET2:b\n",
    "\n",
    "ET3:c\n",
    "\n",
    "ET4:a\n",
    "\n",
    "ET5:c\n",
    "\n",
    "ET6:a\n",
    "\n",
    "ET7:c\n",
    "\n",
    "ET8:a,b,c\n",
    "\n",
    "ET9:c\n",
    "\n",
    "ET10:a,c\n",
    "\n",
    "ET11:d\n",
    "\n",
    "ET12:b,c,d\n",
    "\n",
    "ET13:b\n",
    "\n",
    "ET14:d\n",
    "\n",
    "ET15:a\n",
    "\n",
    "ET16:c\n",
    "\n",
    "ET17:d\n",
    "\n",
    "ET18:b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET1\n",
    "\n",
    "## Since there are four distinct values (described by 2 bits), we would need 4 features to one-hot encode them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET2\n",
    "\n",
    "Both sample one and sample three contain the feature (1,black). So it is expected that they would both have the same index for this feature even after hashing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET3\n",
    "\n",
    "The fact that we go from 4 buckets to 100 buckets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET4\n",
    "\n",
    "A hash collision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET5\n",
    "\n",
    "Use matrix factorization to remap your input vectors to latent concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete records that have missing input values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of a click event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b,c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be framed as minimizing a convex function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.38717472605879716, 0.70601285717302287)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from scipy import stats\n",
    "\n",
    "control = [0.5,0.5,3,3,3,3,3,3,4]\n",
    "treatment = [1.5,0.5,3,4]\n",
    "\n",
    "total_transactions = 100000\n",
    "ctrl_rate = [9./total_transactions]\n",
    "treat_rate = [4./total_transactions]\n",
    "\n",
    "#Based on revenue\n",
    "scipy.stats.ttest_ind(control,treatment)\n",
    "\n",
    "#Based on transaction rates\n",
    "#scipy.stats.ttest_ind(ctrl_rate,treat_rate)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy computes 2 sided t-test. For a one-sided t-test, we can just divide the p-value by 2.\n",
    "\n",
    "The p-value for a 2 sided t-test is 0.706\n",
    "\n",
    "The p-value for a 1 sided t-test is 0.706/2 = 0.353\n",
    "\n",
    "Since the p-value is >0.05, there is NO statistically significant difference between the control model and the treatment model (based on revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer is none of the above. It will require 5 iterations to discover the shortest distances to all nodes from node 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b),(c),(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.3.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, Oct 19 2015 18:31:17)\n",
      "SparkContext available as sc, HiveContext available as sqlCtx.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "#Escape L for line numbers\n",
    "spark_home = os.environ['SPARK_HOME'] = '/Users/Vamsi/Downloads/spark-1.3.0-bin-hadoop2.4'\n",
    "\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME enviroment variable is not set')\n",
    "sys.path.insert(0,os.path.join(spark_home,'python'))\n",
    "sys.path.insert(0,os.path.join(spark_home,'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "execfile(os.path.join(spark_home,'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD1 is [(1, 2), (3, 4), (3, 6)]\n",
      "RDD2 is [(3, 9), (3, 6)]\n",
      "Inner join of RDD1 and RDD2 is [(3, (4, 9)), (3, (4, 6)), (3, (6, 9)), (3, (6, 6))]\n"
     ]
    }
   ],
   "source": [
    "#Creating RDD\n",
    "RDD1 = sc.parallelize([(1,2),(3,4),(3,6)])\n",
    "RDD2 = sc.parallelize([(3,9),(3,6)])\n",
    "\n",
    "#Action\n",
    "print \"RDD1 is\" ,RDD1.collect()\n",
    "print \"RDD2 is\" ,RDD2.collect()\n",
    "\n",
    "#Inner join\n",
    "RDD12 = RDD1.join(RDD2)\n",
    "\n",
    "#Action \n",
    "print \"Inner join of RDD1 and RDD2 is\" ,RDD12.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET15\n",
    "\n",
    "(a) 200%\n",
    "\n",
    "Work shown in Linear-Regression-on-Beer-Data notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET16\n",
    "\n",
    "(c)20%\n",
    "\n",
    "Work shown in Linear-Regression-on-Beer-Data notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET17\n",
    "\n",
    "(d) 3.9%\n",
    "\n",
    "Work shown in the Linear-Regression-on-Beer-Data in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ET18\n",
    "\n",
    "(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
