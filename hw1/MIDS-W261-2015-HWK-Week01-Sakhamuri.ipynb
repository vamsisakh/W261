{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name : Vamsi Sakhamuri\n",
    "## E-mail : vamsi@ischool.berkeley.edu\n",
    "## Class Name : W261-3\n",
    "## Week Number : 1\n",
    "## Date of submission : 01/19/2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data is a broad term for datasets so large that traditional data processing techniques are inadequate. The data is too big (volume) , moves too fast(velocity) and consists of different types (structured and unstructured - variety) for traditional techniques to be useful and therefore new state-of-the-art methods have to be used to process and derive insights from such big data.\n",
    "\n",
    "I work as an ASIC(Application Specific Integrated Circuit) Design Engineer at a semiconductor firm in the San Francisco Bay Area. I design ASIC's for Solid State Drives (SSDs). Owing to several benefits over hard-disks, solid state drives are nowadays ubiquitous in laptops,desktops, data-centers etc. Solid State Drive is a storage device and under the hood consists of a ASIC controller chip and several NAND Flash memory chips. These NAND Flash memory chips are what store the bits. At a more granular level , there is a special type of transistor (called the floating-gate transistor) which stores a bit. There are several billion of these floating gate type transistors in a solid state drive system. Each Solid state drive can easily have a density of 1 TB these days (approximately 4096 billion floating gate transistors). Each of these transistors exhibit slightly different physical characteristics and the challenge is to analyze the reliability of each of these transistors before shipping them to the customer. And on top of it, there are hundreds of thousands of these solid state drives being shipped. So, the amount of data to be analyzed can get \"big\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Introduction:*\n",
    "\n",
    "Bias and variance are closely related to overfitting and underfitting of data. Our goal should be to select a model which has a low bias as well as a low variance. However, there is a tradeoff between bias and variance and our goal should be to minimize both of them simultaneouly.\n",
    "\n",
    "#### *Bias:*\n",
    "\n",
    "Bias is the difference between the expected value of an estimator and the true value of the function\n",
    "\n",
    "Bias = E[g(x)] - f(x)\n",
    "\n",
    "where g(x) is the estimator and f(x) is the true function.\n",
    "\n",
    "In order to determine bias, we should repeatedly sample the data and then fit our model to all the different samples.\n",
    "Once we fit our model to all the different samples, then we should calculate the mean predicted value obtained from our model across all these samples ,i.e expected value of the estimator for all the samples. This is E[g(x)]. \n",
    "\n",
    "Then, simply subtracting f(x) from E[g(x)] gives us the bias of our model\n",
    "\n",
    "Pseudo Code:\n",
    "\n",
    "Lets assume we will sample our data N times:\n",
    "\n",
    "1) for (m =1;m<6; m=m+1) (outer loop - choose a polynomial model of degree m)\n",
    "\n",
    "2) for (i=0;i<N;i=i+1) (inner loop)\n",
    "\n",
    "        data from the ith sample\n",
    "        \n",
    "        fit our model to the ith sample\n",
    "        \n",
    "        store this fit in g(xi)\n",
    "        \n",
    "    Compute the mean of all the fits, g(x0),g(x1),g(xN) where the data samples go from 0 to N\n",
    "\n",
    "    This gives us E[g(x)].\n",
    "    \n",
    "    Bias[m] = E[g(x)] - f(x)     // bias of polynomial model of degree m \n",
    "    \n",
    "    Return to outer loop\n",
    "    \n",
    "\n",
    "#### *Variance:*\n",
    "\n",
    "Variance tells us how much our estimator is expected to vary for different samples of data.\n",
    "\n",
    "Variance = E[g(x) - E[g(x)]]^2\n",
    "\n",
    "where g(x) is the estimator\n",
    "\n",
    "Pseudo Code:\n",
    "\n",
    "Lets assume we will sample our data N times:\n",
    "\n",
    "1)for (m =1;m<6; m=m+1) (outer loop - choose a polynomial model of degree m)\n",
    "\n",
    "2) for (i=0;i<N;i=i+1) (inner loop)\n",
    "\n",
    "        data from the ith sample\n",
    "        \n",
    "        fit our model to the ith sample\n",
    "        \n",
    "        store this fit in g(xi)\n",
    "        \n",
    "    Compute the mean of all the fits, g(x0),g(x1),g(xN) where the data samples go from 0 to N\n",
    "\n",
    "    This gives us E[g(x)].\n",
    "    \n",
    "    Variance[m] = E[g(x) - E[g(x)]]^2     // variance of polynomial model of degree m\n",
    "\n",
    "    Return to outer loop\n",
    "    \n",
    "    \n",
    "#### *Irreducible error:*\n",
    "\n",
    "Irreducible error is the noise component of our prediction and does not depend on the estimator.\n",
    "\n",
    "Variance of noise  = E[(y - f(x)]^2\n",
    "\n",
    "where y is the output value from our sample and f(x) is the true function\n",
    "\n",
    "In general , y = f(x) + e , where e is the noise term\n",
    "\n",
    "\n",
    "### Model Selection:\n",
    "\n",
    "From the bias and variance pseudocode described above, we have the bias and variance of all the polynomial models of degrees 1,2,3,4 and 5.\n",
    "\n",
    "We will plot these bias variance values on the y-axis against the model complexity (degree) on the x-axis and choose the model where the bias^2 +variance value is the smallest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\",re.IGNORECASE)\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    mylist = []\n",
    "    for line in myfile:\n",
    "        mylist = line.split()\n",
    "        for each_word in mylist:\n",
    "            hit = re.search(findword,each_word,re.IGNORECASE)\n",
    "            if(hit):\n",
    "                count = count + 1\n",
    "\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "filename = sys.argv[1:]\n",
    "\n",
    "for f in filename:\n",
    "    myList = []\n",
    "    with open (f,\"r\") as fn:\n",
    "        for line in fn:\n",
    "            myList = line.split()\n",
    "            for line in myList:\n",
    "                sum = sum + int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\"assistance\" occurs 10 times in the email corpus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\",re.IGNORECASE)\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    mylist = []\n",
    "    master_list = []\n",
    "    for line in myfile:\n",
    "        count_hit = 0\n",
    "        count_all = 0\n",
    "        doc_list = []\n",
    "        email = re.split(r'\\t+',line)\n",
    "        if len(email)==4:\n",
    "            i=0\n",
    "            for each_field in email:\n",
    "                if(i==0):\n",
    "                    doc_list.append(each_field)   #doc ID\n",
    "                elif (i==1):\n",
    "                    doc_list.append(each_field)   #spam=1 or 0=ham\n",
    "                elif (i==2):\n",
    "                    all_words = each_field.split()\n",
    "                    for w in all_words:\n",
    "                        hit = re.search(findword,w,re.IGNORECASE)\n",
    "                        if(hit):\n",
    "                            count_hit = count_hit+1          #search in the subject of the message\n",
    "                    all_words = each_field.split()\n",
    "                    count_all = count_all + len(all_words)\n",
    "                   \n",
    "                elif (i==3):\n",
    "                    all_words = each_field.split()\n",
    "                    for w in all_words:\n",
    "                        hit = re.search(findword,w,re.IGNORECASE)\n",
    "                        if(hit):\n",
    "                            count_hit = count_hit+1          #search in the body of the message\n",
    "                    count_all = count_all + len(all_words)\n",
    "                   \n",
    "                i=i+1\n",
    "            doc_list.append(count_hit)\n",
    "            doc_list.append(count_all)\n",
    "            doc_list.append(len(email))       \n",
    "            #Each email is transformed into a list [docID,spam or not,occurences of selected word,\n",
    "            #                                       total word count in email,number of parts in email]\n",
    "            \n",
    "        master_list.append(doc_list)  \n",
    "for doc in master_list:         #master_list is a list of lists of doc_lists\n",
    "    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "filename = sys.argv[1:]\n",
    "\n",
    "total_email_count = 0\n",
    "spam_email_count = 0\n",
    "spam_total_word_count = 0\n",
    "spam_user_word_count = 0\n",
    "ham_email_count = 0\n",
    "ham_total_word_count = 0\n",
    "ham_user_word_count = 0\n",
    "\n",
    "#Step 1 - Figure out the priors and conditionals from all the mapper outputs\n",
    "for f in filename:\n",
    "    with open (f,\"r\") as fn:\n",
    "        for line in fn:\n",
    "            r0=re.sub('\\[','',line)\n",
    "            r1=re.sub('\\]','',r0)\n",
    "            r2=re.sub('\\\"','',r1)\n",
    "            r3=re.sub('\\'','',r2)            \n",
    "            my_list = r3.split(',')\n",
    "            if(int(my_list[1])==1):\n",
    "                spam_email_count = spam_email_count+1\n",
    "                spam_total_word_count = spam_total_word_count + int(my_list[3])\n",
    "                spam_user_word_count = spam_user_word_count + int(my_list[2])\n",
    "            elif(int(my_list[1])==0):\n",
    "                ham_email_count = ham_email_count+1\n",
    "                ham_total_word_count = ham_total_word_count + int(my_list[3])\n",
    "                ham_user_word_count = ham_user_word_count + int(my_list[2])                \n",
    "            total_email_count = total_email_count+1\n",
    "\n",
    "spam_prior = float(spam_email_count)/float(total_email_count)\n",
    "ham_prior = float(ham_email_count)/float(total_email_count)\n",
    "conditional_word_given_spam = float(spam_user_word_count)/float(spam_total_word_count)\n",
    "conditional_word_given_ham = float(ham_user_word_count)/float(ham_total_word_count)\n",
    "\n",
    "tt = 0\n",
    "#Step 2 - Classification\n",
    "prediction = []\n",
    "for f in filename:\n",
    "    with open (f,\"r\") as fn:\n",
    "        for line in fn:\n",
    "            r0=re.sub('\\[','',line)\n",
    "            r1=re.sub('\\]','',r0)\n",
    "            r2=re.sub('\\\"','',r1)\n",
    "            r3=re.sub('\\'','',r2)            \n",
    "            my_list = r3.split(',')\n",
    "            \n",
    "            P_spam_given_word = spam_prior * ((conditional_word_given_spam)**int(my_list[2])) #P(spam|word)\n",
    "            P_ham_given_word = ham_prior * ((conditional_word_given_ham)**int(my_list[2]))  # P(ham|word)\n",
    "            \n",
    "            if(P_spam_given_word > P_ham_given_word):\n",
    "                prediction.append([1,int(my_list[1]),P_spam_given_word,P_ham_given_word,int(my_list[2])])\n",
    "                tt = tt+int(my_list[2])\n",
    "            else:\n",
    "                prediction.append([0,int(my_list[1]),P_spam_given_word,P_ham_given_word,int(my_list[2])])\n",
    "                tt = tt+int(my_list[2])\n",
    "good = 0\n",
    "total = 0\n",
    "for p in prediction:\n",
    "    if(p[0]==p[1]):\n",
    "        good = good+1\n",
    "    total = total+1\n",
    "    \n",
    "accuracy = 100*float(good)/float(total)\n",
    "print accuracy\n",
    "print conditional_word_given_spam\n",
    "print conditional_word_given_ham\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy : 60.0%\n",
    "\n",
    "P(word|SPAM) = 0.000421585160202\n",
    "\n",
    "P(word|HAM)  = 0.000143513203215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\",re.IGNORECASE)\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    mylist = []\n",
    "    master_list = []\n",
    "    for line in myfile:\n",
    "        count_hit_w0 =0\n",
    "        count_hit_w1 = 0\n",
    "        count_hit_w2 = 0\n",
    "        count_all = 0\n",
    "        doc_list = []\n",
    "        email = re.split(r'\\t+',line)\n",
    "        if len(email)==4:\n",
    "            i=0\n",
    "            for each_field in email:\n",
    "                if (i==1):\n",
    "                    doc_list.append(each_field)   #spam=1 or 0=ham\n",
    "                elif (i==2):\n",
    "                    all_words = each_field.split()\n",
    "                    for w in all_words:\n",
    "                        findword_split = findword.split()\n",
    "                        w_int = 0\n",
    "                        for user_word in findword_split:\n",
    "                            hit = re.search(user_word,w,re.IGNORECASE)\n",
    "                            if(hit):\n",
    "                                if(w_int==0):\n",
    "                                    count_hit_w0 = count_hit_w0 +1\n",
    "                                elif(w_int==1):\n",
    "                                    count_hit_w1 = count_hit_w1 +1\n",
    "                                elif(w_int==2):\n",
    "                                    count_hit_w2 = count_hit_w2 +1\n",
    "                            w_int = w_int+1\n",
    "                    count_all = count_all + len(all_words)                   \n",
    "                elif (i==3):\n",
    "                    all_words = each_field.split()\n",
    "                    for w in all_words:\n",
    "                        findword_split = findword.split()  \n",
    "                        w_int = 0\n",
    "                        for user_word in findword_split:\n",
    "                            hit = re.search(user_word,w,re.IGNORECASE)\n",
    "                            if(hit):\n",
    "                                if(w_int==0):\n",
    "                                    count_hit_w0 = count_hit_w0 +1\n",
    "                                elif(w_int==1):\n",
    "                                    count_hit_w1 = count_hit_w1 +1\n",
    "                                elif(w_int==2):\n",
    "                                    count_hit_w2 = count_hit_w2 +1\n",
    "                            w_int = w_int+1\n",
    "                    count_all = count_all + len(all_words)      \n",
    "                i=i+1\n",
    "            \n",
    "            doc_list.append(count_hit_w0)\n",
    "            doc_list.append(count_hit_w1)\n",
    "            doc_list.append(count_hit_w2)\n",
    "\n",
    "            doc_list.append(count_all)\n",
    "            #Each email is transformed into a list [spam or not,occurences of selected words,\n",
    "            #                                       total word count in email]\n",
    "            \n",
    "        master_list.append(doc_list)  \n",
    "for doc in master_list:         #master_list is a list of lists of doc_lists\n",
    "    print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "filename = sys.argv[1:]\n",
    "\n",
    "total_email_count = 0\n",
    "\n",
    "spam_email_count = 0\n",
    "spam_total_word_count = 0\n",
    "spam_user_word_count_w0 = 0\n",
    "spam_user_word_count_w1 = 0\n",
    "spam_user_word_count_w2 = 0\n",
    "\n",
    "ham_email_count = 0\n",
    "ham_total_word_count = 0\n",
    "ham_user_word_count_w0 = 0\n",
    "ham_user_word_count_w1 = 0\n",
    "ham_user_word_count_w2 = 0\n",
    "\n",
    "#Step 1 - Figure out the priors and conditionals from all the mapper outputs\n",
    "for f in filename:\n",
    "    with open (f,\"r\") as fn:\n",
    "        for line in fn:      #Each line from the mapper is [spam or not, count_w0,count_w1,count_w2,total_word_count_in email]\n",
    "            r0=re.sub('\\[','',line)\n",
    "            r1=re.sub('\\]','',r0)\n",
    "            r2=re.sub('\\\"','',r1)\n",
    "            r3=re.sub('\\'','',r2)            \n",
    "            my_list = r3.split(',')\n",
    "            if(int(my_list[0])==1):\n",
    "                spam_email_count = spam_email_count+1\n",
    "                spam_total_word_count = spam_total_word_count + int(my_list[4])\n",
    "                spam_user_word_count_w0 = spam_user_word_count_w0 + int(my_list[1])\n",
    "                spam_user_word_count_w1 = spam_user_word_count_w1 + int(my_list[2])\n",
    "                spam_user_word_count_w2 = spam_user_word_count_w2 + int(my_list[3])\n",
    "            elif(int(my_list[0])==0):\n",
    "                ham_email_count = ham_email_count+1\n",
    "                ham_total_word_count = ham_total_word_count + int(my_list[4])\n",
    "                ham_user_word_count_w0 = ham_user_word_count_w0 + int(my_list[1])\n",
    "                ham_user_word_count_w1 = ham_user_word_count_w1 + int(my_list[2])\n",
    "                ham_user_word_count_w2 = ham_user_word_count_w2 + int(my_list[3])\n",
    "                \n",
    "            total_email_count = total_email_count+1\n",
    "\n",
    "spam_prior = float(spam_email_count)/float(total_email_count)\n",
    "ham_prior = float(ham_email_count)/float(total_email_count)\n",
    "conditional_word_given_spam_w0 = float(spam_user_word_count_w0)/float(spam_total_word_count)\n",
    "conditional_word_given_ham_w0 = float(ham_user_word_count_w0)/float(ham_total_word_count)\n",
    "conditional_word_given_spam_w1 = float(spam_user_word_count_w1)/float(spam_total_word_count)\n",
    "conditional_word_given_ham_w1 = float(ham_user_word_count_w1)/float(ham_total_word_count)\n",
    "conditional_word_given_spam_w2 = float(spam_user_word_count_w2)/float(spam_total_word_count)\n",
    "conditional_word_given_ham_w2 = float(ham_user_word_count_w2)/float(ham_total_word_count)\n",
    "\n",
    "\n",
    "#Step 2 - Classification\n",
    "prediction = []\n",
    "for f in filename:\n",
    "    with open (f,\"r\") as fn:\n",
    "        for line in fn:\n",
    "            r0=re.sub('\\[','',line)\n",
    "            r1=re.sub('\\]','',r0)\n",
    "            r2=re.sub('\\\"','',r1)\n",
    "            r3=re.sub('\\'','',r2)            \n",
    "            my_list = r3.split(',')\n",
    "        \n",
    "            c0s = ((conditional_word_given_spam_w0)**int(my_list[1]))\n",
    "            c1s = ((conditional_word_given_spam_w1)**int(my_list[2]))\n",
    "            c2s = ((conditional_word_given_spam_w2)**int(my_list[3]))\n",
    "\n",
    "            c0h = ((conditional_word_given_ham_w0)**int(my_list[1]))\n",
    "            c1h = ((conditional_word_given_ham_w1)**int(my_list[2]))\n",
    "            c2h = ((conditional_word_given_ham_w2)**int(my_list[3]))\n",
    "                                     \n",
    "            P_spam_given_word = (spam_prior) * c0s*c1s*c2s\n",
    "            P_ham_given_word = (ham_prior) * c0h*c1h*c2h\n",
    "            \n",
    "            if(P_spam_given_word > P_ham_given_word):\n",
    "                prediction.append([1,int(my_list[0]),P_spam_given_word,P_ham_given_word])\n",
    "            else:\n",
    "                prediction.append([0,int(my_list[0]),P_spam_given_word,P_ham_given_word])\n",
    "\n",
    "good = 0\n",
    "total = 0\n",
    "for p in prediction:\n",
    "    if((p[0])==int(p[1])):\n",
    "        good = good+1\n",
    "    total = total+1\n",
    "    \n",
    "accuracy = 100*float(good)/float(total)\n",
    "print accuracy\n",
    "print conditional_word_given_spam_w0\n",
    "print conditional_word_given_ham_w0\n",
    "print conditional_word_given_spam_w1\n",
    "print conditional_word_given_ham_w1\n",
    "print conditional_word_given_spam_w2\n",
    "print conditional_word_given_ham_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance valium enlargementWithATypo\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy - 63.0%\n",
    "\n",
    "P(Word0|SPAM) = 0.000421585160202\n",
    "\n",
    "P(Word0|HAM)  = 0.000143513203215\n",
    "\n",
    "P(Word1|SPAM) = 0.000158094435076\n",
    "\n",
    "P(Word1|HAM)  = 0.0\n",
    "\n",
    "P(Word2|SPAM) = 0.0\n",
    "\n",
    "P(Word2|HAM)  = 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
