{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W261 Machine Learning at Scale\n",
    "\n",
    "**Names** Safyre Anderson, Howard Wen , Vamsi Sakhamuri\n",
    "\n",
    "**Emails** safyre@berkeley.edu, howard.wen1@gmail.com, vamsi@ischool.berkeley.edu\n",
    "\n",
    "**Time** of Initial Submission: March 10th, 2016 8am PST\n",
    "\n",
    "**Section** W261-3, Spring 2016\n",
    "\n",
    "Week 9 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====================================\n",
    "## ===HW 9.0: Short answer questions===\n",
    "\n",
    "*What is PageRank and what is it used for in the context of web search?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "PageRank is a query-independent algorithm and metric for measuring the quality of web pages with respect to a web search query.  PageRanks are calculated based on the hyperlink structure of linked webpages. On a high level, PageRanks are determined via the distribution of times spent on each page. The times spent on a page are correlated to the number of links going into and out of a page.  Pages with more links point towards them (and therefore have higher probabilities of being landed on) are ranked higher than those that do not. Therefore, in web search, a web search engine will return results of a user query in the order of page ranks.\n",
    "\n",
    "To determine the quality of a page, each page can be seen as a state in a Markov Process and a node in a graph, with links as edges between nodes. In the Markov Process, the weights of the edges are the transition probabilities ($P_{ij}$) between one state ($i$) to another ($j$). The steady state probability distributions of landing on any given page are determined by applying the Markov process to the web graph, and power iteration methods to construct a transition matrix $P$ and compute the steady-state left eigenvector:\n",
    "\n",
    "<center> $\\mathbf{v_0} = \\mathbf{v_0}P$</center>\n",
    "\n",
    "where $\\mathbf{v_0}$ is better known as the PageRank vector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to \n",
    "compute the steady state distibuton?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "Viewing pages as (memoryless) states of the chain and the webgraph as a transition matrix, which captures the stochastic probabilities for each transition between states (pages). Furthermore, in order for the steady state probabilities to converge, we need to ensure that our probability matrix is primitive and stochastic. Primitive transition matrices imply that we have a well-behaved graph: All nodes can be reached from every other node, and our graph is aperiodic. To ensure aperiodicity to the graph, we can add self-looping edges to cyclic nodes. Additionally, to ensure stochasticity, we need to include the ability to *teleport* from dangling edges--edges that lead to dead-end nodes.  To accomplish this, we include an arbitrary probability (usually around 15%) of teleporting from a dangling edge to any other page/state within the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OPTIONAL: In topic-specific pagerank, how can we insure that the irreducible property is satified? (HINT: see HW9.4)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ====================================================\n",
    "## ===HW 9.1: MRJob implementation of basic PageRank===\n",
    "\n",
    "*Write a basic MRJob implementation of the iterative PageRank algorithm\n",
    "that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), \n",
    "and further, distributes the mass of dangling nodes with each iteration\n",
    "so that the output of each iteration is correctly normalized (sums to 1).\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page,\n",
    "chooses the next page to which it will move by clicking at random, with probability d,\n",
    "one of the hyperlinks in the current page. This probability is represented by a so-called\n",
    "‘damping factor’ d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer\n",
    "jumps to any web page in the network. If a page is a dangling end, meaning it has no\n",
    "outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform\n",
    "distribution and “teleports” to that page]*\n",
    "\n",
    "\n",
    "*As you build your code, use the test data*\n",
    "\n",
    "*s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)*\n",
    "\n",
    "*with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck\n",
    "your work with the true result, displayed in the first image\n",
    "in the Wikipedia article:*\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank\n",
    "\n",
    "*and here for reference are the corresponding PageRank probabilities:*\n",
    "\n",
    "`A,0.033\n",
    "B,0.384\n",
    "C,0.343\n",
    "D,0.039\n",
    "E,0.081\n",
    "F,0.039\n",
    "G,0.016\n",
    "H,0.016\n",
    "I,0.016\n",
    "J,0.016\n",
    "K,0.016`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-03-13 16:32:14--  https://www.dropbox.com/sh/2c0k5adwz36lkcw/AADxzBgNxNF5Q6-eanjnK64qa/PageRank-test.txt?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 108.160.172.238\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|108.160.172.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://dl.dropboxusercontent.com/content_link/ZFwPmEXTMczjMN8hKNQGMSCtObcZ1BmjuYlBroVEzX3k1pJy7Cj6PDwJdDNxek08/file [following]\n",
      "--2016-03-13 16:32:14--  https://dl.dropboxusercontent.com/content_link/ZFwPmEXTMczjMN8hKNQGMSCtObcZ1BmjuYlBroVEzX3k1pJy7Cj6PDwJdDNxek08/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 199.47.217.101\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|199.47.217.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 166 [text/plain]\n",
      "Saving to: ‘./PageRankTest.txt’\n",
      "\n",
      "100%[======================================>] 166         --.-K/s   in 0s      \n",
      "\n",
      "2016-03-13 16:32:14 (33.1 MB/s) - ‘./PageRankTest.txt’ saved [166/166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download test data as wiki.test:\n",
    "!wget -O ./PageRankTest.txt https://www.dropbox.com/sh/2c0k5adwz36lkcw/AADxzBgNxNF5Q6-eanjnK64qa/PageRank-test.txt?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\t{'C': 1}\r\n",
      "C\t{'B': 1}\r\n",
      "D\t{'A': 1, 'B': 1}\r\n",
      "E\t{'D': 1, 'B': 1, 'F': 1}\r\n",
      "F\t{'B': 1, 'E': 1}\r\n",
      "G\t{'B': 1, 'E': 1}\r\n",
      "H\t{'B': 1, 'E': 1}\r\n",
      "I\t{'B': 1, 'E': 1}\r\n",
      "J\t{'E': 1}\r\n",
      "K\t{'E': 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./PageRankTest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After drawing the graph by hand, it's apparent that A  is a dangling node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===============================================\n",
    "## ===HW 9.2: Exploring PageRank teleportation and network plots===\n",
    "\n",
    "*In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "Using the graph in HW1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha,\n",
    "so that each nodes radius is proportional to its PageRank score. In particular you should\n",
    "do this for the following damping factors: [0,0.25,0.5,0.75, 0.85, 1]. Note your plots should look like the following:*\n",
    "\n",
    "https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===============================================\n",
    "## ===HW 9.3: Applying PageRank to the Wikipedia hyperlinks network===\n",
    "\n",
    "*Run your PageRank implementation on the Wikipedia dataset for 5 iterations,\n",
    "and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "\n",
    "Run your PageRank implementation on the Wikipedia dataset for 10 iterations,\n",
    "and display the top 100 ranked nodes (with teleportation factor of 0.15). \n",
    "Have the top 100 ranked pages changed? Comment on your findings. Plot the pagerank values for the top 100 pages resulting from the 5 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ================================================================\n",
    "## ===HW 9.4: Topic-specific PageRank implementation using MRJob===\n",
    "\n",
    "*Modify your PageRank implementation to produce a topic specific PageRank implementation,\n",
    "as described in:*\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "*Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.\n",
    "This caveat lies in footnote 3 on page 3:*\n",
    "\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "\n",
    "*and must be adhered to for convergence to be guaranteed.*\n",
    "\n",
    "*Run topic specific PageRank on the following randomly generated network of 100 nodes:*\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "*which are organized into ten topics, as described in the file:*\n",
    "\n",
    "s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "*Since there are 10 topics, your result should be 11 PageRank vectors\n",
    "(one for the vanilla PageRank implementation in 9.1, and one for each topic\n",
    "with the topic specific implementation). Print out the top ten ranking nodes \n",
    "and their topics for each of the 11 versions, and comment on your result. \n",
    "Assume a teleportation factor of 0.15 in all your analyses.*\n",
    "\n",
    "*One final and important comment here:  please consider the \n",
    "requirements for irreducibility with topic-specific PageRank.\n",
    "In particular, the literature ensures irreducibility by requiring that\n",
    "nodes not reachable from in-topic nodes be removed from the network.*\n",
    "\n",
    "*This is not a small task, especially as it it must be performed\n",
    "separately for each of the (10) topics.*\n",
    "\n",
    "*So, instead of using this method for irreducibility, \n",
    "please comment on why the literature's method is difficult to implement,\n",
    "and what what extra computation it will require.\n",
    "Then for your code, please use the alternative, \n",
    "non-uniform damping vector:*\n",
    "\n",
    "$v_{ji} = \\beta*(\\frac{1}{|T_j|})$; if node $i$ lies in topic $T_j$\n",
    "\n",
    "$v_{ji} = (1-\\beta)*(\\frac{1}{(N - |T_j|)})$; if node $i$ lies outside of topic $T_j$\n",
    "\n",
    "for $\\beta \\in (0,1)$ close to 1. \n",
    "\n",
    "*With this approach, you will not have to delete any nodes.*\n",
    "*If $\\beta > 0.5$, PageRank is topic-sensitive,* \n",
    "*and if $\\beta < 0.5$, the PageRank is anti-topic-sensitive.*\n",
    "*For any value of $\\beta$ irreducibility should hold,\n",
    "so please try $\\beta=0.99$, and perhaps some other values locally,\n",
    "on the smaller networks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ===============================================\n",
    "## ===HW 9.5: Applying topic-specific PageRank to Wikipedia (Optional)===\n",
    "\n",
    "*Here you will apply your topic-specific PageRank implementation to Wikipedia,\n",
    "defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10,\n",
    "so that there are 10 topics. Once again, print out the top ten ranking nodes \n",
    "and their topics for each of the 11 versions, and comment on your result.\n",
    "Assume a teleportation factor of 0.15 in all your analyses.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ==============================\n",
    "## ===HW 9.6: TextRank (OPTIONAL)===\n",
    "\n",
    "*What is TextRank. Describe the main steps in the algorithm. Why does TextRank work?*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):*\n",
    "\n",
    "*\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" *\n",
    "\n",
    "*The extracted keywords should in the following set:*\n",
    "\n",
    "*linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
