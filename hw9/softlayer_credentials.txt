## Creating hadoop cluster for W261 HW Group
slcli vs create --datacenter=sjc01 --hostname master --domain w261.net --billing=hourly --key=week2_id --cpu=4 --memory=8192 --disk=25 --disk=100 --network=1000 --os=CENTOS_LATEST_64

:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 16702409                             :
: created : 2016-03-13T14:10:21-05:00            :
:    guid : 8ed7fa0e-68e2-48d0-88b8-9ee206dde491 :
:.........:......................................:
:..........:..........:
: username : password :
:..........:..........:
:   root   : GBLx5R49 :
:..........:..........:

master  : 198.11.204.229


slcli vs create --datacenter=sjc01 --hostname slave1 --domain w261.net --billing=hourly --key=week2_id --cpu=4 --memory=8192 --disk=25 --disk=100 --network=1000 --os=CENTOS_LATEST_64

:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 16702413                             :
: created : 2016-03-13T14:10:39-05:00            :
:    guid : 4f2b3302-3942-499f-b3c0-dac591ea0907 :
:.........:......................................:

:..........:..........:
: username : password :
:..........:..........:
:   root   : XgE6Uq2m :
:..........:..........:
slave1  : 198.11.204.228


slcli vs create --datacenter=sjc01 --hostname slave2 --domain w261.net --billing=hourly --key=week2_id --cpu=4 --memory=8192 --disk=25 --disk=100 --network=1000 --os=CENTOS_LATEST_64

:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 16702427                             :
: created : 2016-03-13T14:11:12-05:00            :
:    guid : fa234c86-4749-47e1-ac6a-ec868c250627 :
:.........:......................................:

:..........:..........:
: username : password :
:..........:..........:
:   root   : Z8eyFHfR :
:..........:..........:

slave2  : 198.11.204.226

slcli vs create --datacenter=sjc01 --hostname slave3 --domain w261.net --billing=hourly --key=week2_id --cpu=4 --memory=8192 --disk=25 --disk=100 --network=1000 --os=CENTOS_LATEST_64

:.........:......................................:
:    name : value                                :
:.........:......................................:
:      id : 16702435                             :
: created : 2016-03-13T14:11:51-05:00            :
:    guid : 8fd12dda-34c4-4d04-94aa-42a1c8ac709f :
:.........:......................................:

:..........:..........:
: username : password :
:..........:..........:
:   root   : JP7cTRWe :
:..........:..........:

slave3  : 198.11.204.230


*********************************************************************
NOTE: Assume all steps are done on each node unless noted otherwise!! 
*********************************************************************

## /etc/hosts:
cat >> /etc/hosts <<EOF
127.0.0.1 localhost.localdomain localhost
198.11.204.229 master.w261.net master
198.11.204.228 slave1.w261.net slave1
198.11.204.226 slave2.w261.net slave2
198.11.204.230 slave3.w261.net slave3
EOF

## check hard drive names
fdisk -l | grep Disk | grep GB
Disk /dev/xvdc: 107.4 GB, 107374182400 bytes, 209715200 sectors
Disk /dev/xvda: 26.8 GB, 26843545600 bytes, 52428800 sectors

## starting hadoop and yarn
cd $HADOOP_HOME/sbin
start-dfs.sh
start-yarn.sh

## starting jobhistory server
mr-jobhistory-daemon.sh start historyserver

### checking the cluster:

http://master-ip:50070/dfshealth.html
http://master-ip:8088/cluster
http://master-ip:19888/jobhistory (for Job History Server)


## install bzip2 as root
yum update
yum install bzip2

## switch back to hadoop and install anaconda
wget http://repo.continuum.io/archive/Anaconda2-2.5.0-Linux-x86_64.sh
bash Anaconda2-2.5.0-Linux-x86_64.sh

export /path to anaconda2/bin/:$PATH

## start ipython notebook
ipython notebook --no-browser --port=8889

## ssh tunnel to ipython server
# bind remote ipython port (8889) to local port 8890 (whatever isn't being used)
ssh -N -f -L 8890:127.0.0.1:8889 hadoop@198.11.204.229


