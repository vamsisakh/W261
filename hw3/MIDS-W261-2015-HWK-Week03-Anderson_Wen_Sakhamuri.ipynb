{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## HW3 DATSCI W261 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Names** Safyre Anderson, Howard Wen , Vamsi Sakhamuri\n",
    "\n",
    "**Emails** safyre@berkelye.edu, howard.wen1@gmail.com, vamsi@ischool.berkeley.edu \n",
    "\n",
    "**Time of Initial Submission:** February 4nd, 2016 8am PST\n",
    "\n",
    "**Section** W261-3, Spring 2016  \n",
    "\n",
    "**Week** 3 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.0.\n",
    "*What is a merge sort? Where is it used in Hadoop?*\n",
    "\n",
    "*How is  a combiner function in the context of Hadoop?* \n",
    "\n",
    "*Give an example where it can be used and justify why it should be used in the context of this problem.*\n",
    "\n",
    "*What is the Hadoop shuffle?*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.0.1\n",
    "*What is a merge sort? Where is it used in Hadoop?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Merge-sort is a sorting algorithm which runs in O(nlog(n)) time. In hadoop, it is used in between the map outputs and \n",
    "the reduce inputs. Output from each map output is sorted by the key and then merged with the other sorted outputs from other \n",
    "map outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"map-reduce.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.0.2\n",
    "\n",
    "*How is  a combiner function in the context of Hadoop?* \n",
    "\n",
    "*Give an example where it can be used and justify why it should be used in the context of this problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Combiner function is used after the map output is generated. The values from the same keys are then aggregated to reduce network communication in their transfer to the reduce nodes. Network communication can be a bottleneck and a combiner helps alleviate this problem. \n",
    "Whether a combiner function is run or not is entirely dependent on the execution framework. To be totally sure that intermediate map outputs are combined, in-mapper combining function can be employed. This ensures that aggregation will happen 100% before being shuffled across the network to the various reduce nodes.\n",
    "\n",
    "* An example where combiners (either the optional one provided by the execution framework or the in-mapper combiner) is helpful would be the word count application.\n",
    "Without any sort of combiners, we would be transfer the same key multiple times within the same map task.\n",
    "\n",
    "    For example, if the map input is \"hello hello hi hi hello hello\", \n",
    "\n",
    "    Without combiners, the following <key,value> pairs will be emitted across the network and into the reduce nodes:\n",
    "\n",
    "    hello,1\n",
    "    \n",
    "    hello,1\n",
    "    \n",
    "    hi,1\n",
    "    \n",
    "    hi,1\n",
    "    \n",
    "    hello,1\n",
    "    \n",
    "    hello,1\n",
    "\n",
    "    With combiners, the following <key,value> pairs will be emitted across the network and into the reduce nodes:\n",
    "\n",
    "    hello,4\n",
    "    \n",
    "    hi,2\n",
    "\n",
    "    This is an incredible reduction in network traffic.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='hadoop-tutorial-mapreduce-part-4-input-and-output-31-638.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.0.3\n",
    "\n",
    "*What is the Hadoop shuffle?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hadoop Shuffle: Map-reduce makes the guarantee that the input to every reducer is sorted by the key. And that the <key,value> pairs with the same key are routed to the same reducer. The process by which the system performs this sort and merge (across various mappers) and transfers the map outputs to the reducers is known as the hadoop shuffle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"shuffle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "*Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:*\n",
    "    \n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "*The consumer complaints dataset consists of diverse consumer complaints, \n",
    "which have been reported across the United States regarding various types of loans. \n",
    "The dataset consists of records of the form:*\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "*Hereâ€™s is the first few lines of the  of the Consumer Complaints  Dataset:*\n",
    "\n",
    "`Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:14:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 1498. Stop it first.\n",
      "localhost: datanode running as process 1042. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 1162. Stop it first.\n",
      "16/02/04 08:14:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hdfs\n",
    "\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "historyserver running as process 1275. Stop it first.\r\n"
     ]
    }
   ],
   "source": [
    "#start the jobtracker\n",
    "\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/sbin/mr-jobhistory-daemon.sh --config /Users/Vamsi/Downloads/hadoop-2.7.1/etc/hadoop/ start historyserver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042 DataNode\n",
      "1162 SecondaryNameNode\n",
      "1498 NameNode\n",
      "1275 JobHistoryServer\n",
      "4719 Jps\n"
     ]
    }
   ],
   "source": [
    "#Service check\n",
    "!jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:14:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:14:57 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hw3\n"
     ]
    }
   ],
   "source": [
    "#Start afresh\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f  /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:14:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#Create a directory for hw3\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -mkdir -p /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset for 3.1\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put Consumer_Complaints.csv /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "csv_iter = csv.reader(sys.stdin)\n",
    "    \n",
    "#Grabbing the header names\n",
    "header = next(csv_iter)\n",
    "\n",
    "for line in csv_iter:\n",
    "    if(len(line)==len(header)):   #continue only if all the rows have all the entries\n",
    "        if(line[1].lower()==\"debt collection\"):\n",
    "            sys.stderr.write(\"reporter:counter:Product,Debt,1\\n\")\n",
    "        elif(line[1].lower() == \"mortgage\"):\n",
    "            sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")\n",
    "        else:\n",
    "            sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:15:08 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:15:08 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:15:08 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:15:09 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:15:09 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:15:09 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:15:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local12979890_0001\n",
      "16/02/04 08:15:09 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:15:09 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:15:09 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:15:09 INFO mapreduce.Job: Running job: job_local12979890_0001\n",
      "16/02/04 08:15:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:10 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:15:10 INFO mapred.LocalJobRunner: Starting task: attempt_local12979890_0001_m_000000_0\n",
      "16/02/04 08:15:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:15:10 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:15:10 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:10 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:10 INFO mapreduce.Job: Job job_local12979890_0001 running in uber mode : false\n",
      "16/02/04 08:15:10 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:15:12 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:100000=100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:15:13 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:100000=200000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:15:14 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:100000=300000/3 [rec/s] out:0=0/3 [rec/s]\n",
      "16/02/04 08:15:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:15:14 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:15:14 INFO mapred.Task: Task:attempt_local12979890_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: hdfs://localhost:9000/user/hw3/Consumer_Complaints.csv:0+50906486\n",
      "16/02/04 08:15:14 INFO mapred.Task: Task 'attempt_local12979890_0001_m_000000_0' done.\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local12979890_0001_m_000000_0\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: Starting task: attempt_local12979890_0001_r_000000_0\n",
      "16/02/04 08:15:14 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:14 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:14 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:14 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@46dbb3eb\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:15:14 INFO reduce.EventFetcher: attempt_local12979890_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:15:14 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local12979890_0001_m_000000_0 decomp: 2 len: 6 to MEMORY\n",
      "16/02/04 08:15:14 INFO reduce.InMemoryMapOutput: Read 2 bytes from map-output for attempt_local12979890_0001_m_000000_0\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2\n",
      "16/02/04 08:15:14 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:15:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:14 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: Merging 1 files, 6 bytes from disk\n",
      "16/02/04 08:15:14 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:15:14 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:14 INFO mapred.Merger: Down to the last merge-pass, with 0 segments left of total size: 0 bytes\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:14 INFO mapred.Task: Task:attempt_local12979890_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:14 INFO mapred.Task: Task attempt_local12979890_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:15:14 INFO output.FileOutputCommitter: Saved output of task 'attempt_local12979890_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_1/_temporary/0/task_local12979890_0001_r_000000\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: reduce > reduce\n",
      "16/02/04 08:15:14 INFO mapred.Task: Task 'attempt_local12979890_0001_r_000000_0' done.\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local12979890_0001_r_000000_0\n",
      "16/02/04 08:15:14 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:15:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:15:15 INFO mapreduce.Job: Job job_local12979890_0001 completed successfully\n",
      "16/02/04 08:15:15 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=211842\n",
      "\t\tFILE: Number of bytes written=764874\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=0\n",
      "\t\tMap output bytes=0\n",
      "\t\tMap output materialized bytes=6\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=6\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=0\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=559415296\n",
      "\tProduct\n",
      "\t\tDebt=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142788\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/02/04 08:15:15 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_1\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/Consumer_Complaints.csv \\\n",
    "-output /user/hw3/output_3_1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer org.apache.hadoop.mapred.lib.IdentityReducer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"counters.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "*For this brief study the Input file will be one record (the next line only):*\n",
    "\n",
    "`foo foo quux labs foo bar quux`\n",
    "\n",
    "\n",
    "### 3.2.1 \n",
    "\n",
    "*Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a file \n",
    "!echo \"foo foo quux labs foo bar quux\" >input_3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#Load the file into hdfs\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put input_3_2 /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:MAPPER,mapper_calls,1\\n\")   #trigger counter\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line_s = re.split(r'[\\s]',line)   #split on whitespace\n",
    "    for l in line_s:\n",
    "        print \"%s\\t1\" %l.strip()    #Emit word,1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:REDUCER,reducer_calls,1\\n\")  #trigger counter\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line_s = re.split(r'[\\t]',line)   #split on tab\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])): # Check whether the a new key is emitted\n",
    "        print prev_string,count  # If a new key is emitted then commit the previous key and count to disk\n",
    "        count = 0   #reset the counter\n",
    "\n",
    "    count += 1    #increment the counter\n",
    "    prev_string = line_s[0] \n",
    "print prev_string,count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:15:20 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:15:20 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:15:20 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:15:20 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:15:20 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:15:20 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:15:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local167321748_0001\n",
      "16/02/04 08:15:21 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:15:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:21 INFO mapreduce.Job: Running job: job_local167321748_0001\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Starting task: attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:21 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/input_3_2:0+31\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:15:21 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: bufstart = 0; bufend = 48; bufvoid = 104857600\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600\n",
      "16/02/04 08:15:21 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:15:21 INFO mapred.Task: Task:attempt_local167321748_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/02/04 08:15:21 INFO mapred.Task: Task 'attempt_local167321748_0001_m_000000_0' done.\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Starting task: attempt_local167321748_0001_r_000000_0\n",
      "16/02/04 08:15:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:21 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@30ca148e\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:15:21 INFO reduce.EventFetcher: attempt_local167321748_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:15:21 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local167321748_0001_m_000000_0 decomp: 20 len: 24 to MEMORY\n",
      "16/02/04 08:15:21 INFO reduce.InMemoryMapOutput: Read 20 bytes from map-output for attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20\n",
      "16/02/04 08:15:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 20 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merging 1 files, 24 bytes from disk\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 13 bytes\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:15:21 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: Records R/W=2/1\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:21 INFO mapred.Task: Task:attempt_local167321748_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:21 INFO mapred.Task: Task attempt_local167321748_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:15:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local167321748_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_2_1/_temporary/0/task_local167321748_0001_r_000000\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Records R/W=2/1 > reduce\n",
      "16/02/04 08:15:21 INFO mapred.Task: Task 'attempt_local167321748_0001_r_000000_0' done.\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local167321748_0001_r_000000_0\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: Starting task: attempt_local167321748_0001_r_000001_0\n",
      "16/02/04 08:15:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:21 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:21 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:21 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ac91e98\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:15:21 INFO reduce.EventFetcher: attempt_local167321748_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:15:21 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local167321748_0001_m_000000_0 decomp: 31 len: 35 to MEMORY\n",
      "16/02/04 08:15:21 INFO reduce.InMemoryMapOutput: Read 31 bytes from map-output for attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 31, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31\n",
      "16/02/04 08:15:21 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28 bytes\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merged 1 segments, 31 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merging 1 files, 35 bytes from disk\n",
      "16/02/04 08:15:21 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:21 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 28 bytes\n",
      "16/02/04 08:15:21 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:15:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: Records R/W=4/1\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:22 INFO mapreduce.Job: Job job_local167321748_0001 running in uber mode : false\n",
      "16/02/04 08:15:22 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task:attempt_local167321748_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task attempt_local167321748_0001_r_000001_0 is allowed to commit now\n",
      "16/02/04 08:15:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local167321748_0001_r_000001_0' to hdfs://localhost:9000/user/hw3/output_3_2_1/_temporary/0/task_local167321748_0001_r_000001\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Records R/W=4/1 > reduce\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task 'attempt_local167321748_0001_r_000001_0' done.\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local167321748_0001_r_000001_0\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Starting task: attempt_local167321748_0001_r_000002_0\n",
      "16/02/04 08:15:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:22 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3507791f\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:15:22 INFO reduce.EventFetcher: attempt_local167321748_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:15:22 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local167321748_0001_m_000000_0 decomp: 10 len: 14 to MEMORY\n",
      "16/02/04 08:15:22 INFO reduce.InMemoryMapOutput: Read 10 bytes from map-output for attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 10, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->10\n",
      "16/02/04 08:15:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merged 1 segments, 10 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merging 1 files, 14 bytes from disk\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task:attempt_local167321748_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task attempt_local167321748_0001_r_000002_0 is allowed to commit now\n",
      "16/02/04 08:15:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local167321748_0001_r_000002_0' to hdfs://localhost:9000/user/hw3/output_3_2_1/_temporary/0/task_local167321748_0001_r_000002\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task 'attempt_local167321748_0001_r_000002_0' done.\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local167321748_0001_r_000002_0\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Starting task: attempt_local167321748_0001_r_000003_0\n",
      "16/02/04 08:15:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:22 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3889c6ad\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:15:22 INFO reduce.EventFetcher: attempt_local167321748_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:15:22 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local167321748_0001_m_000000_0 decomp: 11 len: 15 to MEMORY\n",
      "16/02/04 08:15:22 INFO reduce.InMemoryMapOutput: Read 11 bytes from map-output for attempt_local167321748_0001_m_000000_0\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 11, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->11\n",
      "16/02/04 08:15:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merged 1 segments, 11 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merging 1 files, 15 bytes from disk\n",
      "16/02/04 08:15:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4 bytes\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task:attempt_local167321748_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task attempt_local167321748_0001_r_000003_0 is allowed to commit now\n",
      "16/02/04 08:15:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local167321748_0001_r_000003_0' to hdfs://localhost:9000/user/hw3/output_3_2_1/_temporary/0/task_local167321748_0001_r_000003\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "16/02/04 08:15:22 INFO mapred.Task: Task 'attempt_local167321748_0001_r_000003_0' done.\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local167321748_0001_r_000003_0\n",
      "16/02/04 08:15:22 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:15:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:15:23 INFO mapreduce.Job: Job job_local167321748_0001 completed successfully\n",
      "16/02/04 08:15:23 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=531306\n",
      "\t\tFILE: Number of bytes written=1926094\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=155\n",
      "\t\tHDFS: Number of bytes written=87\n",
      "\t\tHDFS: Number of read operations=55\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=25\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=8\n",
      "\t\tMap output bytes=48\n",
      "\t\tMap output materialized bytes=88\n",
      "\t\tInput split bytes=92\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5\n",
      "\t\tReduce shuffle bytes=88\n",
      "\t\tReduce input records=8\n",
      "\t\tReduce output records=5\n",
      "\t\tSpilled Records=16\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=1418723328\n",
      "\tMAPPER\n",
      "\t\tmapper_calls=1\n",
      "\tREDUCER\n",
      "\t\treducer_calls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=34\n",
      "16/02/04 08:15:23 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_2_1\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-input /user/hw3/input_3_2 \\\n",
    "-output /user/hw3/output_3_2_1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"counters_3_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of mapper calls is 1 because the file is small enough to be processed by just 1 mapper.\n",
    "The number of reducer calls is 4 because there are 4 unique words and 4 reducers are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Following 4 cells would output each of the reducer outputs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "quux 2\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_1/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      " 1\t\n",
      "foo 3\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_1/part-00001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "bar 1\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_1/part-00002 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "labs 1\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_1/part-00003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 \n",
    "\n",
    "*Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).*\n",
    "*Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removing the header from the Consumer_Complaints.csv file\n",
    "!tail -n +2 Consumer_Complaints.csv > Consumer_Complaints_no_header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Split the file into 4 equally sized files\n",
    "!split -l 78228 Consumer_Complaints_no_header.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#Load the splitted files into hdfs\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put xa* /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "csv_iter = csv.reader(sys.stdin)\n",
    "    \n",
    "for line in csv_iter:\n",
    "    sys.stderr.write(\"reporter:counter:MAPPER,mapper_calls,1\\n\")\n",
    "    if(len(line)==14):   #continue only if all the rows have all the entries\n",
    "        issues = re.split(r'[\\s,./]+',line[3].strip())      #splitting the issues column\n",
    "        for w in issues:\n",
    "            print \"%s\\t1\" %w #Emit word,1 to the reducer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(\"reporter:counter:REDUCER,reducer_calls,1\\n\")\n",
    "    line_s = re.split(r'[\\t]',line.strip())\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])):\n",
    "        print prev_string,count\n",
    "        count = 0\n",
    "\n",
    "    count += 1\n",
    "    prev_string = line_s[0]\n",
    "print prev_string,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/output_3_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:15:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:15:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:15:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:15:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:15:49 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/04 08:15:49 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/04 08:15:49 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:15:49 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local983135479_0001\n",
      "16/02/04 08:15:50 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:15:50 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:15:50 INFO mapreduce.Job: Running job: job_local983135479_0001\n",
      "16/02/04 08:15:50 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:15:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:15:50 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:15:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xab:0+13160945\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:15:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:15:50 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: Records R/W=1552/1\n",
      "16/02/04 08:15:50 INFO streaming.PipeMapRed: R/W/S=10000/38814/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:51 INFO mapreduce.Job: Job job_local983135479_0001 running in uber mode : false\n",
      "16/02/04 08:15:51 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:15:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:52 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:15:52 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:15:52 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:15:52 INFO mapred.MapTask: bufstart = 0; bufend = 3385621; bufvoid = 104857600\n",
      "16/02/04 08:15:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24820088(99280352); length = 1394309/6553600\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:15:53 INFO mapred.Task: Task:attempt_local983135479_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:53 INFO mapred.LocalJobRunner: Records R/W=1552/1\n",
      "16/02/04 08:15:53 INFO mapred.Task: Task 'attempt_local983135479_0001_m_000000_0' done.\n",
      "16/02/04 08:15:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:15:53 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:15:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:53 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xaa:0+13123345\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:15:53 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: Records R/W=1571/1\n",
      "16/02/04 08:15:53 INFO streaming.PipeMapRed: R/W/S=10000/39416/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:15:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:55 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:15:55 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:15:55 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:15:55 INFO mapred.MapTask: bufstart = 0; bufend = 3419326; bufvoid = 104857600\n",
      "16/02/04 08:15:55 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24795408(99181632); length = 1418989/6553600\n",
      "16/02/04 08:15:56 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:15:56 INFO mapred.Task: Task:attempt_local983135479_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:15:56 INFO mapred.LocalJobRunner: Records R/W=1571/1\n",
      "16/02/04 08:15:56 INFO mapred.Task: Task 'attempt_local983135479_0001_m_000001_0' done.\n",
      "16/02/04 08:15:56 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:15:56 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:15:56 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:15:56 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:15:56 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xac:0+12615536\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:15:56 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:56 INFO streaming.PipeMapRed: Records R/W=1593/1\n",
      "16/02/04 08:15:57 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:15:57 INFO streaming.PipeMapRed: R/W/S=10000/36510/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:15:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:15:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:15:59 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:15:59 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:15:59 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:15:59 INFO mapred.MapTask: bufstart = 0; bufend = 3408082; bufvoid = 104857600\n",
      "16/02/04 08:15:59 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24883332(99533328); length = 1331065/6553600\n",
      "16/02/04 08:16:00 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:00 INFO mapred.Task: Task:attempt_local983135479_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:00 INFO mapred.LocalJobRunner: Records R/W=1593/1\n",
      "16/02/04 08:16:00 INFO mapred.Task: Task 'attempt_local983135479_0001_m_000002_0' done.\n",
      "16/02/04 08:16:00 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:16:00 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:00 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:00 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:00 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xad:0+12006486\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:16:00 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: Records R/W=1639/1\n",
      "16/02/04 08:16:00 INFO streaming.PipeMapRed: R/W/S=10000/36380/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:01 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:16:02 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:02 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:02 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:16:02 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:16:02 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:16:02 INFO mapred.MapTask: bufstart = 0; bufend = 3211690; bufvoid = 104857600\n",
      "16/02/04 08:16:02 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24965524(99862096); length = 1248873/6553600\n",
      "16/02/04 08:16:03 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:03 INFO mapred.Task: Task:attempt_local983135479_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: Records R/W=1639/1\n",
      "16/02/04 08:16:03 INFO mapred.Task: Task 'attempt_local983135479_0001_m_000003_0' done.\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_r_000000_0\n",
      "16/02/04 08:16:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c214ce7\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=368836608, maxSingleShuffleLimit=92209152, mergeThreshold=243432176, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:03 INFO reduce.EventFetcher: attempt_local983135479_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local983135479_0001_m_000000_0 decomp: 1068325 len: 1068329 to MEMORY\n",
      "16/02/04 08:16:03 INFO reduce.InMemoryMapOutput: Read 1068325 bytes from map-output for attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1068325, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1068325\n",
      "16/02/04 08:16:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local983135479_0001_m_000003_0 decomp: 644986 len: 644990 to MEMORY\n",
      "16/02/04 08:16:03 INFO reduce.InMemoryMapOutput: Read 644986 bytes from map-output for attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 644986, inMemoryMapOutputs.size() -> 2, commitMemory -> 1068325, usedMemory ->1713311\n",
      "16/02/04 08:16:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local983135479_0001_m_000001_0 decomp: 1104543 len: 1104547 to MEMORY\n",
      "16/02/04 08:16:03 INFO reduce.InMemoryMapOutput: Read 1104543 bytes from map-output for attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1104543, inMemoryMapOutputs.size() -> 3, commitMemory -> 1713311, usedMemory ->2817854\n",
      "16/02/04 08:16:03 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local983135479_0001_m_000002_0 decomp: 772090 len: 772094 to MEMORY\n",
      "16/02/04 08:16:03 INFO reduce.InMemoryMapOutput: Read 772090 bytes from map-output for attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 772090, inMemoryMapOutputs.size() -> 4, commitMemory -> 2817854, usedMemory ->3589944\n",
      "16/02/04 08:16:03 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:03 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:03 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 3589904 bytes\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: Merged 4 segments, 3589944 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: Merging 1 files, 3589942 bytes from disk\n",
      "16/02/04 08:16:03 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:03 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:03 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3589928 bytes\n",
      "16/02/04 08:16:03 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:03 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:16:03 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:03 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:04 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:05 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:16:05 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: Records R/W=346976/1\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:06 INFO mapred.Task: Task:attempt_local983135479_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:06 INFO mapred.Task: Task attempt_local983135479_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:16:06 INFO output.FileOutputCommitter: Saved output of task 'attempt_local983135479_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_2_2/_temporary/0/task_local983135479_0001_r_000000\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: Records R/W=346976/1 > reduce\n",
      "16/02/04 08:16:06 INFO mapred.Task: Task 'attempt_local983135479_0001_r_000000_0' done.\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_r_000000_0\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_r_000001_0\n",
      "16/02/04 08:16:06 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:06 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:06 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:06 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@670ddcfd\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=369203616, maxSingleShuffleLimit=92300904, mergeThreshold=243674400, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:06 INFO reduce.EventFetcher: attempt_local983135479_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:06 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local983135479_0001_m_000000_0 decomp: 1241234 len: 1241238 to MEMORY\n",
      "16/02/04 08:16:06 INFO reduce.InMemoryMapOutput: Read 1241234 bytes from map-output for attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1241234, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1241234\n",
      "16/02/04 08:16:06 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local983135479_0001_m_000003_0 decomp: 1332981 len: 1332985 to MEMORY\n",
      "16/02/04 08:16:06 INFO reduce.InMemoryMapOutput: Read 1332981 bytes from map-output for attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1332981, inMemoryMapOutputs.size() -> 2, commitMemory -> 1241234, usedMemory ->2574215\n",
      "16/02/04 08:16:06 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local983135479_0001_m_000001_0 decomp: 1243189 len: 1243193 to MEMORY\n",
      "16/02/04 08:16:06 INFO reduce.InMemoryMapOutput: Read 1243189 bytes from map-output for attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1243189, inMemoryMapOutputs.size() -> 3, commitMemory -> 2574215, usedMemory ->3817404\n",
      "16/02/04 08:16:06 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local983135479_0001_m_000002_0 decomp: 1411496 len: 1411500 to MEMORY\n",
      "16/02/04 08:16:06 INFO reduce.InMemoryMapOutput: Read 1411496 bytes from map-output for attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1411496, inMemoryMapOutputs.size() -> 4, commitMemory -> 3817404, usedMemory ->5228900\n",
      "16/02/04 08:16:06 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:06 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:06 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 5228879 bytes\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: Merged 4 segments, 5228900 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: Merging 1 files, 5228898 bytes from disk\n",
      "16/02/04 08:16:06 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:06 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:06 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5228891 bytes\n",
      "16/02/04 08:16:06 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:06 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:07 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/02/04 08:16:07 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:08 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:16:08 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:16:09 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:200000=400000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:16:09 INFO streaming.PipeMapRed: Records R/W=428414/1\n",
      "16/02/04 08:16:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:10 INFO mapred.Task: Task:attempt_local983135479_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:10 INFO mapred.Task: Task attempt_local983135479_0001_r_000001_0 is allowed to commit now\n",
      "16/02/04 08:16:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local983135479_0001_r_000001_0' to hdfs://localhost:9000/user/hw3/output_3_2_2/_temporary/0/task_local983135479_0001_r_000001\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: Records R/W=428414/1 > reduce\n",
      "16/02/04 08:16:10 INFO mapred.Task: Task 'attempt_local983135479_0001_r_000001_0' done.\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_r_000001_0\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_r_000002_0\n",
      "16/02/04 08:16:10 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:10 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:10 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:10 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2dfc1cf3\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=369570592, maxSingleShuffleLimit=92392648, mergeThreshold=243916608, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:10 INFO reduce.EventFetcher: attempt_local983135479_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:10 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local983135479_0001_m_000000_0 decomp: 1032138 len: 1032142 to MEMORY\n",
      "16/02/04 08:16:10 INFO reduce.InMemoryMapOutput: Read 1032138 bytes from map-output for attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1032138, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1032138\n",
      "16/02/04 08:16:10 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local983135479_0001_m_000003_0 decomp: 1068906 len: 1068910 to MEMORY\n",
      "16/02/04 08:16:10 INFO reduce.InMemoryMapOutput: Read 1068906 bytes from map-output for attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1068906, inMemoryMapOutputs.size() -> 2, commitMemory -> 1032138, usedMemory ->2101044\n",
      "16/02/04 08:16:10 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local983135479_0001_m_000001_0 decomp: 1011330 len: 1011334 to MEMORY\n",
      "16/02/04 08:16:10 INFO reduce.InMemoryMapOutput: Read 1011330 bytes from map-output for attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1011330, inMemoryMapOutputs.size() -> 3, commitMemory -> 2101044, usedMemory ->3112374\n",
      "16/02/04 08:16:10 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local983135479_0001_m_000002_0 decomp: 1096104 len: 1096108 to MEMORY\n",
      "16/02/04 08:16:10 INFO reduce.InMemoryMapOutput: Read 1096104 bytes from map-output for attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1096104, inMemoryMapOutputs.size() -> 4, commitMemory -> 3112374, usedMemory ->4208478\n",
      "16/02/04 08:16:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:10 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:10 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 4208454 bytes\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: Merged 4 segments, 4208478 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: Merging 1 files, 4208476 bytes from disk\n",
      "16/02/04 08:16:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4208466 bytes\n",
      "16/02/04 08:16:10 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:10 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:11 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:11 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/02/04 08:16:11 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:16:12 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:150000=300000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: Records R/W=340472/1\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:13 INFO mapred.Task: Task:attempt_local983135479_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:13 INFO mapred.Task: Task attempt_local983135479_0001_r_000002_0 is allowed to commit now\n",
      "16/02/04 08:16:13 INFO output.FileOutputCommitter: Saved output of task 'attempt_local983135479_0001_r_000002_0' to hdfs://localhost:9000/user/hw3/output_3_2_2/_temporary/0/task_local983135479_0001_r_000002\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: Records R/W=340472/1 > reduce\n",
      "16/02/04 08:16:13 INFO mapred.Task: Task 'attempt_local983135479_0001_r_000002_0' done.\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_r_000002_0\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: Starting task: attempt_local983135479_0001_r_000003_0\n",
      "16/02/04 08:16:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:13 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d3230d\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=369203616, maxSingleShuffleLimit=92300904, mergeThreshold=243674400, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:13 INFO reduce.EventFetcher: attempt_local983135479_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:13 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local983135479_0001_m_000000_0 decomp: 741088 len: 741092 to MEMORY\n",
      "16/02/04 08:16:13 INFO reduce.InMemoryMapOutput: Read 741088 bytes from map-output for attempt_local983135479_0001_m_000000_0\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 741088, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->741088\n",
      "16/02/04 08:16:13 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local983135479_0001_m_000003_0 decomp: 789263 len: 789267 to MEMORY\n",
      "16/02/04 08:16:13 INFO reduce.InMemoryMapOutput: Read 789263 bytes from map-output for attempt_local983135479_0001_m_000003_0\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 789263, inMemoryMapOutputs.size() -> 2, commitMemory -> 741088, usedMemory ->1530351\n",
      "16/02/04 08:16:13 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local983135479_0001_m_000001_0 decomp: 769768 len: 769772 to MEMORY\n",
      "16/02/04 08:16:13 INFO reduce.InMemoryMapOutput: Read 769768 bytes from map-output for attempt_local983135479_0001_m_000001_0\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 769768, inMemoryMapOutputs.size() -> 3, commitMemory -> 1530351, usedMemory ->2300119\n",
      "16/02/04 08:16:13 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local983135479_0001_m_000002_0 decomp: 793934 len: 793938 to MEMORY\n",
      "16/02/04 08:16:13 INFO reduce.InMemoryMapOutput: Read 793934 bytes from map-output for attempt_local983135479_0001_m_000002_0\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 793934, inMemoryMapOutputs.size() -> 4, commitMemory -> 2300119, usedMemory ->3094053\n",
      "16/02/04 08:16:13 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:13 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:13 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 3093997 bytes\n",
      "16/02/04 08:16:13 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: Merged 4 segments, 3094053 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: Merging 1 files, 3094051 bytes from disk\n",
      "16/02/04 08:16:13 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3094033 bytes\n",
      "16/02/04 08:16:13 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:13 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:14 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:200000=200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:16:14 INFO streaming.PipeMapRed: Records R/W=232450/1\n",
      "16/02/04 08:16:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:14 INFO mapred.Task: Task:attempt_local983135479_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:14 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:14 INFO mapred.Task: Task attempt_local983135479_0001_r_000003_0 is allowed to commit now\n",
      "16/02/04 08:16:14 INFO output.FileOutputCommitter: Saved output of task 'attempt_local983135479_0001_r_000003_0' to hdfs://localhost:9000/user/hw3/output_3_2_2/_temporary/0/task_local983135479_0001_r_000003\n",
      "16/02/04 08:16:14 INFO mapred.LocalJobRunner: Records R/W=232450/1 > reduce\n",
      "16/02/04 08:16:14 INFO mapred.Task: Task 'attempt_local983135479_0001_r_000003_0' done.\n",
      "16/02/04 08:16:14 INFO mapred.LocalJobRunner: Finishing task: attempt_local983135479_0001_r_000003_0\n",
      "16/02/04 08:16:14 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:16:15 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:16:15 INFO mapreduce.Job: Job job_local983135479_0001 completed successfully\n",
      "16/02/04 08:16:15 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=84122588\n",
      "\t\tFILE: Number of bytes written=149829219\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=332876621\n",
      "\t\tHDFS: Number of bytes written=6302\n",
      "\t\tHDFS: Number of read operations=130\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=28\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=1348312\n",
      "\t\tMap output bytes=13424719\n",
      "\t\tMap output materialized bytes=16121439\n",
      "\t\tInput split bytes=344\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=188\n",
      "\t\tReduce shuffle bytes=16121439\n",
      "\t\tReduce input records=1348312\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=2696624\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=773\n",
      "\t\tTotal committed heap usage (bytes)=3833069568\n",
      "\tMAPPER\n",
      "\t\tmapper_calls=312912\n",
      "\tREDUCER\n",
      "\t\treducer_calls=1348312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906312\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2526\n",
      "16/02/04 08:16:15 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_2_2\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-input /user/hw3/xa* \\\n",
    "-output /user/hw3/output_3_2_2 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3_2_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The next four cells output the first 10 rows of each of the 4 reducer outputs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Account 16555\t\n",
      "Applied 139\t\n",
      "Can't 1999\t\n",
      "Cash 240\t\n",
      "Closing 2795\t\n",
      "Cont'd 17972\t\n",
      "Debt 1343\t\n",
      "Delinquent 1061\t\n",
      "I 925\t\n",
      "Incorrect 29133\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_2/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "1 4\t\n",
      "ATM 2422\t\n",
      "Cancelling 2795\t\n",
      "Communication 8671\t\n",
      "Dealing 1944\t\n",
      "Improper 4966\t\n",
      "Loan 107254\t\n",
      "Payment 92\t\n",
      "Problems 9484\t\n",
      "Shopping 672\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_2/part-00001 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "APR 3431\t\n",
      "Arbitration 168\t\n",
      "Bankruptcy 222\t\n",
      "Billing 8158\t\n",
      "Convenience 75\t\n",
      "Credit 14768\t\n",
      "Deposits 10555\t\n",
      "Disclosure 7655\t\n",
      "False 3621\t\n",
      "Overlimit 127\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_2/part-00002 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Advertising 1193\t\n",
      "Application 8868\t\n",
      "Balance 597\t\n",
      "Charged 878\t\n",
      "Collection 1907\t\n",
      "Customer 2734\t\n",
      "Embezzlement 3276\t\n",
      "Forbearance 350\t\n",
      "Fraud 3842\t\n",
      "Getting 291\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_2/part-00003 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 \n",
    "\n",
    "*Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "csv_iter = csv.reader(sys.stdin)\n",
    "    \n",
    "for line in csv_iter:\n",
    "    sys.stderr.write(\"reporter:counter:MAPPER,mapper_calls,1\\n\")\n",
    "    if(len(line)==14):   #continue only if all the rows have all the entries\n",
    "        issues = re.split(r'[\\s,./]+',line[3])\n",
    "        for w in issues:\n",
    "            print \"%s\\t1\" %w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(\"reporter:counter:COMBINER,combiner_calls,1\\n\")\n",
    "    line_s = re.split(r'[\\t]',line.strip())\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])):\n",
    "        print \"%s\\t%s\" %(prev_string,count)\n",
    "        count = 0\n",
    "    count += 1\n",
    "    prev_string = line_s[0]\n",
    "print \"%s\\t%s\" %(prev_string,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    sys.stderr.write(\"reporter:counter:REDUCER,reducer_calls,1\\n\")\n",
    "\n",
    "    line_s = re.split(r'[\\t]',line.strip())\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])):\n",
    "        print prev_string,count\n",
    "        count = 0\n",
    "    count += int(line_s[1])\n",
    "    prev_string = line_s[0]\n",
    "print prev_string,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/output_3_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:16:33 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:16:33 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:16:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:16:34 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/04 08:16:34 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/04 08:16:34 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:16:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2063302883_0001\n",
      "16/02/04 08:16:34 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:16:34 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:16:34 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:16:34 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:34 INFO mapreduce.Job: Running job: job_local2063302883_0001\n",
      "16/02/04 08:16:34 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:16:34 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:34 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:34 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:34 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:34 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xab:0+13160945\n",
      "16/02/04 08:16:34 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:16:35 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:16:35 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: Records R/W=773/1\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: R/W/S=1000/938/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:35 INFO streaming.PipeMapRed: R/W/S=10000/38814/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:35 INFO mapreduce.Job: Job job_local2063302883_0001 running in uber mode : false\n",
      "16/02/04 08:16:35 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:37 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:16:37 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:16:37 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:16:37 INFO mapred.MapTask: bufstart = 0; bufend = 3385621; bufvoid = 104857600\n",
      "16/02/04 08:16:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24820088(99280352); length = 1394309/6553600\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:37 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: Records R/W=104547/1\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:38 INFO Configuration.deprecation: mapred.skip.reduce.auto.incr.proc.count is deprecated. Instead, use mapreduce.reduce.skip.proc-count.auto-incr\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:38 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: Records R/W=101901/1\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: Records R/W=85831/1\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: Records R/W=56299/1\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:40 INFO mapred.Task: Task:attempt_local2063302883_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:40 INFO mapred.LocalJobRunner: Records R/W=56299/1\n",
      "16/02/04 08:16:40 INFO mapred.Task: Task 'attempt_local2063302883_0001_m_000000_0' done.\n",
      "16/02/04 08:16:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:40 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xaa:0+13123345\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:16:40 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: Records R/W=1571/1\n",
      "16/02/04 08:16:40 INFO streaming.PipeMapRed: R/W/S=10000/39416/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:40 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:42 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:16:42 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:16:42 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:16:42 INFO mapred.MapTask: bufstart = 0; bufend = 3419326; bufvoid = 104857600\n",
      "16/02/04 08:16:42 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24795408(99181632); length = 1418989/6553600\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:42 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:42 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: Records R/W=108893/1\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:43 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: Records R/W=103027/1\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: Records R/W=84622/1\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:44 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: Records R/W=58206/1\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:45 INFO mapred.Task: Task:attempt_local2063302883_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:45 INFO mapred.LocalJobRunner: Records R/W=58206/1\n",
      "16/02/04 08:16:45 INFO mapred.Task: Task 'attempt_local2063302883_0001_m_000001_0' done.\n",
      "16/02/04 08:16:45 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:45 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:45 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:45 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:45 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xac:0+12615536\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:16:45 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: Records R/W=1593/1\n",
      "16/02/04 08:16:45 INFO streaming.PipeMapRed: R/W/S=10000/36510/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:45 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:47 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:16:47 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:16:47 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:16:47 INFO mapred.MapTask: bufstart = 0; bufend = 3408082; bufvoid = 104857600\n",
      "16/02/04 08:16:47 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24883332(99533328); length = 1331065/6553600\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:47 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:47 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: Records R/W=73207/1\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:48 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: Records R/W=114751/1\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: Records R/W=86427/1\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:49 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: Records R/W=58382/1\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:50 INFO mapred.Task: Task:attempt_local2063302883_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:50 INFO mapred.LocalJobRunner: Records R/W=58382/1\n",
      "16/02/04 08:16:50 INFO mapred.Task: Task 'attempt_local2063302883_0001_m_000002_0' done.\n",
      "16/02/04 08:16:50 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:50 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xad:0+12006486\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: numReduceTasks: 4\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:16:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: Records R/W=1639/1\n",
      "16/02/04 08:16:50 INFO streaming.PipeMapRed: R/W/S=10000/36380/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:50 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:52 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:16:52 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:16:52 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:16:52 INFO mapred.MapTask: bufstart = 0; bufend = 3211690; bufvoid = 104857600\n",
      "16/02/04 08:16:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24965524(99862096); length = 1248873/6553600\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: Records R/W=60329/1\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:52 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: Records R/W=108735/1\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:53 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: Records R/W=83592/1\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: Records R/W=59563/1\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:55 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task:attempt_local2063302883_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Records R/W=59563/1\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task 'attempt_local2063302883_0001_m_000003_0' done.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_r_000000_0\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@39d3b038\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: attempt_local2063302883_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2063302883_0001_m_000003_0 decomp: 447 len: 451 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 447 bytes from map-output for attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 447, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->447\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2063302883_0001_m_000000_0 decomp: 591 len: 595 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 591 bytes from map-output for attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 591, inMemoryMapOutputs.size() -> 2, commitMemory -> 447, usedMemory ->1038\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2063302883_0001_m_000001_0 decomp: 596 len: 600 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 596 bytes from map-output for attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 596, inMemoryMapOutputs.size() -> 3, commitMemory -> 1038, usedMemory ->1634\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2063302883_0001_m_000002_0 decomp: 526 len: 530 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 526 bytes from map-output for attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 526, inMemoryMapOutputs.size() -> 4, commitMemory -> 1634, usedMemory ->2160\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 2120 bytes\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 2160 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 1 files, 2158 bytes from disk\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2144 bytes\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:55 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:16:55 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: Records R/W=166/1\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task:attempt_local2063302883_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task attempt_local2063302883_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2063302883_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_2_3/_temporary/0/task_local2063302883_0001_r_000000\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Records R/W=166/1 > reduce\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task 'attempt_local2063302883_0001_r_000000_0' done.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_r_000000_0\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_r_000001_0\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3494e028\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: attempt_local2063302883_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2063302883_0001_m_000003_0 decomp: 508 len: 512 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 508 bytes from map-output for attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 508, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->508\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2063302883_0001_m_000000_0 decomp: 734 len: 738 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 734 bytes from map-output for attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 734, inMemoryMapOutputs.size() -> 2, commitMemory -> 508, usedMemory ->1242\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2063302883_0001_m_000001_0 decomp: 744 len: 748 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 744 bytes from map-output for attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 744, inMemoryMapOutputs.size() -> 3, commitMemory -> 1242, usedMemory ->1986\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#2 about to shuffle output of map attempt_local2063302883_0001_m_000002_0 decomp: 642 len: 646 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 642 bytes from map-output for attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 642, inMemoryMapOutputs.size() -> 4, commitMemory -> 1986, usedMemory ->2628\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 2606 bytes\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 2628 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 1 files, 2626 bytes from disk\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2618 bytes\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: Records R/W=182/1\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task:attempt_local2063302883_0001_r_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task attempt_local2063302883_0001_r_000001_0 is allowed to commit now\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2063302883_0001_r_000001_0' to hdfs://localhost:9000/user/hw3/output_3_2_3/_temporary/0/task_local2063302883_0001_r_000001\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Records R/W=182/1 > reduce\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task 'attempt_local2063302883_0001_r_000001_0' done.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_r_000001_0\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_r_000002_0\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b9e9b38\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: attempt_local2063302883_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2063302883_0001_m_000003_0 decomp: 435 len: 439 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 435 bytes from map-output for attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 435, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->435\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2063302883_0001_m_000000_0 decomp: 650 len: 654 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 650 bytes from map-output for attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 650, inMemoryMapOutputs.size() -> 2, commitMemory -> 435, usedMemory ->1085\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2063302883_0001_m_000001_0 decomp: 649 len: 653 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 649 bytes from map-output for attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 649, inMemoryMapOutputs.size() -> 3, commitMemory -> 1085, usedMemory ->1734\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#3 about to shuffle output of map attempt_local2063302883_0001_m_000002_0 decomp: 562 len: 566 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 562 bytes from map-output for attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 562, inMemoryMapOutputs.size() -> 4, commitMemory -> 1734, usedMemory ->2296\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 2272 bytes\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 2296 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 1 files, 2294 bytes from disk\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2284 bytes\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: Records R/W=157/1\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task:attempt_local2063302883_0001_r_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task attempt_local2063302883_0001_r_000002_0 is allowed to commit now\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2063302883_0001_r_000002_0' to hdfs://localhost:9000/user/hw3/output_3_2_3/_temporary/0/task_local2063302883_0001_r_000002\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Records R/W=157/1 > reduce\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task 'attempt_local2063302883_0001_r_000002_0' done.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_r_000002_0\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Starting task: attempt_local2063302883_0001_r_000003_0\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:16:55 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:16:55 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:16:55 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1d7fd286\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: attempt_local2063302883_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local2063302883_0001_m_000003_0 decomp: 476 len: 480 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 476 bytes from map-output for attempt_local2063302883_0001_m_000003_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 476, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->476\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local2063302883_0001_m_000000_0 decomp: 579 len: 583 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 579 bytes from map-output for attempt_local2063302883_0001_m_000000_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 579, inMemoryMapOutputs.size() -> 2, commitMemory -> 476, usedMemory ->1055\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local2063302883_0001_m_000001_0 decomp: 583 len: 587 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 583 bytes from map-output for attempt_local2063302883_0001_m_000001_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 583, inMemoryMapOutputs.size() -> 3, commitMemory -> 1055, usedMemory ->1638\n",
      "16/02/04 08:16:55 INFO reduce.LocalFetcher: localfetcher#4 about to shuffle output of map attempt_local2063302883_0001_m_000002_0 decomp: 573 len: 577 to MEMORY\n",
      "16/02/04 08:16:55 INFO reduce.InMemoryMapOutput: Read 573 bytes from map-output for attempt_local2063302883_0001_m_000002_0\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 573, inMemoryMapOutputs.size() -> 4, commitMemory -> 1638, usedMemory ->2211\n",
      "16/02/04 08:16:55 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 2155 bytes\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merged 4 segments, 2211 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 1 files, 2209 bytes from disk\n",
      "16/02/04 08:16:55 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:16:55 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2191 bytes\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: Records R/W=156/1\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:16:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task:attempt_local2063302883_0001_r_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task attempt_local2063302883_0001_r_000003_0 is allowed to commit now\n",
      "16/02/04 08:16:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2063302883_0001_r_000003_0' to hdfs://localhost:9000/user/hw3/output_3_2_3/_temporary/0/task_local2063302883_0001_r_000003\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Records R/W=156/1 > reduce\n",
      "16/02/04 08:16:55 INFO mapred.Task: Task 'attempt_local2063302883_0001_r_000003_0' done.\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local2063302883_0001_r_000003_0\n",
      "16/02/04 08:16:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:16:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:16:56 INFO mapreduce.Job: Job job_local2063302883_0001 completed successfully\n",
      "16/02/04 08:16:56 INFO mapreduce.Job: Counters: 38\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=953763\n",
      "\t\tFILE: Number of bytes written=3188275\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=332876621\n",
      "\t\tHDFS: Number of bytes written=6302\n",
      "\t\tHDFS: Number of read operations=130\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=28\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=1348312\n",
      "\t\tMap output bytes=13424719\n",
      "\t\tMap output materialized bytes=9359\n",
      "\t\tInput split bytes=344\n",
      "\t\tCombine input records=1348312\n",
      "\t\tCombine output records=661\n",
      "\t\tReduce input groups=188\n",
      "\t\tReduce shuffle bytes=9359\n",
      "\t\tReduce input records=661\n",
      "\t\tReduce output records=188\n",
      "\t\tSpilled Records=1322\n",
      "\t\tShuffled Maps =16\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=16\n",
      "\t\tGC time elapsed (ms)=245\n",
      "\t\tTotal committed heap usage (bytes)=3197632512\n",
      "\tCOMBINER\n",
      "\t\tcombiner_calls=1348312\n",
      "\tMAPPER\n",
      "\t\tmapper_calls=312912\n",
      "\tREDUCER\n",
      "\t\treducer_calls=661\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906312\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2526\n",
      "16/02/04 08:16:56 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_2_3\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-input /user/hw3/xa* \\\n",
    "-output /user/hw3/output_3_2_3 \\\n",
    "-mapper mapper.py \\\n",
    "-combiner combiner.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3_2_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of combiners is evident above. The reducer_calls reduces to 661!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 \n",
    "\n",
    "*Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "csv_iter = csv.reader(sys.stdin)\n",
    "    \n",
    "for line in csv_iter:\n",
    "    if(len(line)==14):   #continue only if all the rows have all the entries\n",
    "        issues = re.split(r'[\\s,./]+',line[3])\n",
    "        for w in issues:\n",
    "            print \"%s,1\" %w\n",
    "            print \"*,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line_s = re.split(r'[,]',line.strip())\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])):\n",
    "        print \"%s,%s\" %(prev_string,count)\n",
    "        count = 0\n",
    "    count += 1\n",
    "    prev_string = line_s[0]\n",
    "print \"%s,%s\" %(prev_string,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "count = 0\n",
    "\n",
    "prev_string = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line_s = re.split(r'[,]',line.strip())\n",
    "    \n",
    "    if((prev_string!=None) and (prev_string != line_s[0])):\n",
    "        print prev_string,count\n",
    "        count = 0\n",
    "    count += int(line_s[1])\n",
    "    prev_string = line_s[0]\n",
    "print prev_string,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:16:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/output_3_2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:17:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:17:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:17:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:17:02 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/02/04 08:17:03 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:17:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1641761722_0001\n",
      "16/02/04 08:17:03 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:17:03 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:17:03 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:17:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:03 INFO mapreduce.Job: Running job: job_local1641761722_0001\n",
      "16/02/04 08:17:03 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:17:03 INFO mapred.LocalJobRunner: Starting task: attempt_local1641761722_0001_m_000000_0\n",
      "16/02/04 08:17:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xab:0+13160945\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:17:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: Records R/W=773/1\n",
      "16/02/04 08:17:03 INFO streaming.PipeMapRed: R/W/S=1000/960/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:04 INFO streaming.PipeMapRed: R/W/S=10000/76488/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:04 INFO mapreduce.Job: Job job_local1641761722_0001 running in uber mode : false\n",
      "16/02/04 08:17:04 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:17:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:05 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:05 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:05 INFO mapred.MapTask: bufstart = 0; bufend = 5477089; bufvoid = 104857600\n",
      "16/02/04 08:17:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23425776(93703104); length = 2788621/6553600\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:17:06 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:06 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:07 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:07 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:07 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: Records R/W=697156/1\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:08 INFO mapred.Task: Task:attempt_local1641761722_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:08 INFO mapred.LocalJobRunner: Records R/W=697156/1\n",
      "16/02/04 08:17:08 INFO mapred.Task: Task 'attempt_local1641761722_0001_m_000000_0' done.\n",
      "16/02/04 08:17:08 INFO mapred.LocalJobRunner: Finishing task: attempt_local1641761722_0001_m_000000_0\n",
      "16/02/04 08:17:08 INFO mapred.LocalJobRunner: Starting task: attempt_local1641761722_0001_m_000001_0\n",
      "16/02/04 08:17:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:08 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xaa:0+13123345\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:08 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: Records R/W=1571/1\n",
      "16/02/04 08:17:08 INFO streaming.PipeMapRed: R/W/S=10000/79729/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:10 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:10 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:10 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:10 INFO mapred.MapTask: bufstart = 0; bufend = 5547814; bufvoid = 104857600\n",
      "16/02/04 08:17:10 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23376416(93505664); length = 2837981/6553600\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:10 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/02/04 08:17:10 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:11 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:11 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:11 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:600000=600000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:350000=700000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: Records R/W=709496/1\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:12 INFO mapred.Task: Task:attempt_local1641761722_0001_m_000001_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:12 INFO mapred.LocalJobRunner: Records R/W=709496/1\n",
      "16/02/04 08:17:12 INFO mapred.Task: Task 'attempt_local1641761722_0001_m_000001_0' done.\n",
      "16/02/04 08:17:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local1641761722_0001_m_000001_0\n",
      "16/02/04 08:17:12 INFO mapred.LocalJobRunner: Starting task: attempt_local1641761722_0001_m_000002_0\n",
      "16/02/04 08:17:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xac:0+12615536\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:12 INFO streaming.PipeMapRed: Records R/W=1593/1\n",
      "16/02/04 08:17:13 INFO streaming.PipeMapRed: R/W/S=10000/75602/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:14 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:14 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:14 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:14 INFO mapred.MapTask: bufstart = 0; bufend = 5404684; bufvoid = 104857600\n",
      "16/02/04 08:17:14 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23552264(94209056); length = 2662133/6553600\n",
      "16/02/04 08:17:14 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:14 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:15 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:15 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:15 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:16 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:16 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:16 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: Records R/W=665534/1\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:17 INFO mapred.Task: Task:attempt_local1641761722_0001_m_000002_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:17 INFO mapred.LocalJobRunner: Records R/W=665534/1\n",
      "16/02/04 08:17:17 INFO mapred.Task: Task 'attempt_local1641761722_0001_m_000002_0' done.\n",
      "16/02/04 08:17:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local1641761722_0001_m_000002_0\n",
      "16/02/04 08:17:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1641761722_0001_m_000003_0\n",
      "16/02/04 08:17:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/xad:0+12006486\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: Records R/W=1639/1\n",
      "16/02/04 08:17:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:17:17 INFO streaming.PipeMapRed: R/W/S=10000/75347/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:19 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:19 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:19 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:19 INFO mapred.MapTask: bufstart = 0; bufend = 5085004; bufvoid = 104857600\n",
      "16/02/04 08:17:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23716648(94866592); length = 2497749/6553600\n",
      "16/02/04 08:17:19 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./combiner.py]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:19 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:20 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:20 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:20 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:21 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:21 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:500000=500000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:21 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:300000=600000/2 [rec/s] out:0=0/2 [rec/s]\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: Records R/W=624438/1\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:22 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:22 INFO mapred.Task: Task:attempt_local1641761722_0001_m_000003_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Records R/W=624438/1\n",
      "16/02/04 08:17:22 INFO mapred.Task: Task 'attempt_local1641761722_0001_m_000003_0' done.\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local1641761722_0001_m_000003_0\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Starting task: attempt_local1641761722_0001_r_000000_0\n",
      "16/02/04 08:17:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:22 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@570b388c\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:17:22 INFO reduce.EventFetcher: attempt_local1641761722_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:17:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1641761722_0001_m_000001_0 decomp: 2762 len: 2766 to MEMORY\n",
      "16/02/04 08:17:22 INFO reduce.InMemoryMapOutput: Read 2762 bytes from map-output for attempt_local1641761722_0001_m_000001_0\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2762, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2762\n",
      "16/02/04 08:17:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1641761722_0001_m_000002_0 decomp: 2471 len: 2475 to MEMORY\n",
      "16/02/04 08:17:22 INFO reduce.InMemoryMapOutput: Read 2471 bytes from map-output for attempt_local1641761722_0001_m_000002_0\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2471, inMemoryMapOutputs.size() -> 2, commitMemory -> 2762, usedMemory ->5233\n",
      "16/02/04 08:17:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1641761722_0001_m_000003_0 decomp: 2002 len: 2006 to MEMORY\n",
      "16/02/04 08:17:22 INFO reduce.InMemoryMapOutput: Read 2002 bytes from map-output for attempt_local1641761722_0001_m_000003_0\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2002, inMemoryMapOutputs.size() -> 3, commitMemory -> 5233, usedMemory ->7235\n",
      "16/02/04 08:17:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1641761722_0001_m_000000_0 decomp: 2744 len: 2748 to MEMORY\n",
      "16/02/04 08:17:22 INFO reduce.InMemoryMapOutput: Read 2744 bytes from map-output for attempt_local1641761722_0001_m_000000_0\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2744, inMemoryMapOutputs.size() -> 4, commitMemory -> 7235, usedMemory ->9979\n",
      "16/02/04 08:17:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:17:22 INFO mapred.Merger: Merging 4 sorted segments\n",
      "16/02/04 08:17:22 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 9935 bytes\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: Merged 4 segments, 9979 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: Merging 1 files, 9977 bytes from disk\n",
      "16/02/04 08:17:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:17:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:17:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 9962 bytes\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:17:22 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:17:22 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: Records R/W=665/1\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:22 INFO mapred.Task: Task:attempt_local1641761722_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "16/02/04 08:17:22 INFO mapred.Task: Task attempt_local1641761722_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:17:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1641761722_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_2_4/_temporary/0/task_local1641761722_0001_r_000000\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Records R/W=665/1 > reduce\n",
      "16/02/04 08:17:22 INFO mapred.Task: Task 'attempt_local1641761722_0001_r_000000_0' done.\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local1641761722_0001_r_000000_0\n",
      "16/02/04 08:17:22 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:17:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:17:22 INFO mapreduce.Job: Job job_local1641761722_0001 completed successfully\n",
      "16/02/04 08:17:22 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=554287\n",
      "\t\tFILE: Number of bytes written=1984331\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=180157685\n",
      "\t\tHDFS: Number of bytes written=2536\n",
      "\t\tHDFS: Number of read operations=61\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312912\n",
      "\t\tMap output records=2696624\n",
      "\t\tMap output bytes=21514591\n",
      "\t\tMap output materialized bytes=9995\n",
      "\t\tInput split bytes=344\n",
      "\t\tCombine input records=2696624\n",
      "\t\tCombine output records=665\n",
      "\t\tReduce input groups=656\n",
      "\t\tReduce shuffle bytes=9995\n",
      "\t\tReduce input records=665\n",
      "\t\tReduce output records=189\n",
      "\t\tSpilled Records=1330\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=462\n",
      "\t\tTotal committed heap usage (bytes)=1824522240\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906312\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2536\n",
      "16/02/04 08:17:22 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_2_4\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/xa* \\\n",
    "-output /user/hw3/output_3_2_4 \\\n",
    "-mapper mapper.py \\\n",
    "-combiner combiner.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "* 1348312\t\n",
      " 4\t\n",
      "APR 3431\t\n",
      "ATM 2422\t\n",
      "Account 16555\t\n",
      "Advertising 1193\t\n",
      "Application 8868\t\n",
      "Applied 139\t\n",
      "Arbitration 168\t\n",
      "Balance 597\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_4/part-00000 | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_4/part-00000 > reduce_3_2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Running a second map-reduce job to sort the word counts **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\s]+',line)\n",
    "    print \"%s,%s\" %(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "i=0\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[,]',line.strip())\n",
    "\n",
    "    if(i==0):\n",
    "        total_count = int(line[1])\n",
    "        total_count_read = 1\n",
    "        i = i+1\n",
    "    if(total_count_read==1):\n",
    "        rel_freq = float(line[1])/float(total_count)\n",
    "        print \"%s,%s,%s,%s\" %(line[0],line[1],rel_freq,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put reduce_3_2_4 /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/output_3_2_4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:17:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:17:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:17:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:17:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:17:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/04 08:17:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2004021949_0001\n",
      "16/02/04 08:17:36 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:17:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:36 INFO mapreduce.Job: Running job: job_local2004021949_0001\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Starting task: attempt_local2004021949_0001_m_000000_0\n",
      "16/02/04 08:17:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/reduce_3_2_4:0+2536\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:17:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: Records R/W=189/1\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:36 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: bufstart = 0; bufend = 2536; bufvoid = 104857600\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213644(104854576); length = 753/6553600\n",
      "16/02/04 08:17:36 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:36 INFO mapred.Task: Task:attempt_local2004021949_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Records R/W=189/1\n",
      "16/02/04 08:17:36 INFO mapred.Task: Task 'attempt_local2004021949_0001_m_000000_0' done.\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Finishing task: attempt_local2004021949_0001_m_000000_0\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:17:36 INFO mapred.LocalJobRunner: Starting task: attempt_local2004021949_0001_r_000000_0\n",
      "16/02/04 08:17:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@247ecdfe\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:17:37 INFO reduce.EventFetcher: attempt_local2004021949_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:17:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2004021949_0001_m_000000_0 decomp: 2916 len: 2920 to MEMORY\n",
      "16/02/04 08:17:37 INFO reduce.InMemoryMapOutput: Read 2916 bytes from map-output for attempt_local2004021949_0001_m_000000_0\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2916, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2916\n",
      "16/02/04 08:17:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:17:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:17:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2904 bytes\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 2916 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: Merging 1 files, 2920 bytes from disk\n",
      "16/02/04 08:17:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:17:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:17:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2904 bytes\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:17:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:17:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: Records R/W=189/1\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:37 INFO mapred.Task: Task:attempt_local2004021949_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:37 INFO mapred.Task: Task attempt_local2004021949_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:17:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2004021949_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_2_4f/_temporary/0/task_local2004021949_0001_r_000000\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: Records R/W=189/1 > reduce\n",
      "16/02/04 08:17:37 INFO mapred.Task: Task 'attempt_local2004021949_0001_r_000000_0' done.\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local2004021949_0001_r_000000_0\n",
      "16/02/04 08:17:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:17:37 INFO mapreduce.Job: Job job_local2004021949_0001 running in uber mode : false\n",
      "16/02/04 08:17:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:17:37 INFO mapreduce.Job: Job job_local2004021949_0001 completed successfully\n",
      "16/02/04 08:17:37 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=217644\n",
      "\t\tFILE: Number of bytes written=786182\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5072\n",
      "\t\tHDFS: Number of bytes written=7276\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=189\n",
      "\t\tMap output records=189\n",
      "\t\tMap output bytes=2536\n",
      "\t\tMap output materialized bytes=2920\n",
      "\t\tInput split bytes=95\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=189\n",
      "\t\tReduce shuffle bytes=2920\n",
      "\t\tReduce input records=189\n",
      "\t\tReduce output records=189\n",
      "\t\tSpilled Records=378\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=495976448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2536\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7276\n",
      "16/02/04 08:17:37 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_2_4f\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.comparator.options=-k2,2nr \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/reduce_3_2_4 \\\n",
    "-output /user/hw3/output_3_2_4f \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Printing out the 50 most frequent words alongwith their frequencies and relative frequencies below. **\n",
    "\n",
    "The format of the output is:\n",
    "\n",
    "Word, Frequency , Relative Frequency, Total Count of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "*,1348312,1.0,1348312\t\n",
      "Loan,107254,0.0795468704573,1348312\t\n",
      "foreclosure,70487,0.0522779594041,1348312\t\n",
      "collection,70487,0.0522779594041,1348312\t\n",
      "modification,70487,0.0522779594041,1348312\t\n",
      "account,40893,0.0303290336361,1348312\t\n",
      "or,40508,0.0300434914174,1348312\t\n",
      "credit,40483,0.0300249497149,1348312\t\n",
      "payments,39993,0.0296615323456,1348312\t\n",
      "escrow,36767,0.0272689110532,1348312\t\n",
      "servicing,36767,0.0272689110532,1348312\t\n",
      "report,34903,0.0258864417138,1348312\t\n",
      "Incorrect,29133,0.0216070167736,1348312\t\n",
      "on,29069,0.0215595500151,1348312\t\n",
      "information,29069,0.0215595500151,1348312\t\n",
      "debt,26531,0.0196771963759,1348312\t\n",
      "not,18477,0.013703801494,1348312\t\n",
      "owed,17972,0.0133292591032,1348312\t\n",
      "collect,17972,0.0133292591032,1348312\t\n",
      "attempts,17972,0.0133292591032,1348312\t\n",
      "Cont'd,17972,0.0133292591032,1348312\t\n",
      "Account,16555,0.0122783154047,1348312\t\n",
      "and,16448,0.012198956918,1348312\t\n",
      "opening,16205,0.0120187315695,1348312\t\n",
      "closing,16205,0.0120187315695,1348312\t\n",
      "management,16205,0.0120187315695,1348312\t\n",
      "Credit,14768,0.010952954509,1348312\t\n",
      "of,13983,0.0103707450501,1348312\t\n",
      "loan,12376,0.00917888441251,1348312\t\n",
      "my,10731,0.00795884038709,1348312\t\n",
      "Deposits,10555,0.00782830680139,1348312\t\n",
      "withdrawals,10555,0.00782830680139,1348312\t\n",
      "Problems,9484,0.0070339802657,1348312\t\n",
      "Application,8868,0.00657711271575,1348312\t\n",
      "Communication,8671,0.00643100409994,1348312\t\n",
      "tactics,8671,0.00643100409994,1348312\t\n",
      "broker,8625,0.00639688736732,1348312\t\n",
      "originator,8625,0.00639688736732,1348312\t\n",
      "mortgage,8625,0.00639688736732,1348312\t\n",
      "to,8401,0.00623075371279,1348312\t\n",
      "Billing,8158,0.00605052836435,1348312\t\n",
      "Other,7886,0.005848794641,1348312\t\n",
      "Disclosure,7655,0.00567746930977,1348312\t\n",
      "verification,7655,0.00567746930977,1348312\t\n",
      "disputes,6938,0.00514569328167,1348312\t\n",
      "reporting,6559,0.00486460107156,1348312\t\n",
      "lease,6337,0.00469995075324,1348312\t\n",
      "the,6248,0.00463394229229,1348312\t\n",
      "caused,5663,0.00420006645346,1348312\t\n",
      "by,5663,0.00420006645346,1348312\t\n",
      "being,5663,0.00420006645346,1348312\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_4f/part-00000 | head -n 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "credited,92,6.82334652514e-05,1348312\t\n",
      "Payment,92,6.82334652514e-05,1348312\t\n",
      "Convenience,75,5.56251075419e-05,1348312\t\n",
      "checks,75,5.56251075419e-05,1348312\t\n",
      "wrong,71,5.26584351396e-05,1348312\t\n",
      "day,71,5.26584351396e-05,1348312\t\n",
      "amt,71,5.26584351396e-05,1348312\t\n",
      "missing,64,4.74667584357e-05,1348312\t\n",
      "disclosures,64,4.74667584357e-05,1348312\t\n",
      ",4,2.96667240223e-06,1348312\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_2_4f/part-00000 | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3. Shopping Cart Analysis\n",
    "*Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.*\n",
    "\n",
    "*For this homework use the online browsing behavior dataset located at:* \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "*Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.*\n",
    "\n",
    "*Here are the first few lines of the ProductPurchaseData* \n",
    "\n",
    "`FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444` \n",
    "\n",
    "\n",
    "**Do some exploratory data analysis of this dataset.** \n",
    "\n",
    "*How many unique items are available from this supplier?*\n",
    "\n",
    "*Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#Loading the data-set\n",
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put ProductPurchaseData.txt /user/hw3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\s]',line.strip())\n",
    "    for l in line:\n",
    "        print \"%s\\t1\" %l\n",
    "        print \"*\\t1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "prev_id = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    if((prev_id !=None) and (line[0] !=prev_id)):\n",
    "        print \"%s\\t%s\" %(prev_id,count)\n",
    "        count = 0\n",
    "    count +=1\n",
    "    prev_id = line[0]\n",
    "print \"%s\\t%s\" %(prev_id,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:17:47 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:17:47 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:17:47 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:17:48 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:17:48 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:17:48 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:17:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local38036051_0001\n",
      "16/02/04 08:17:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:17:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:17:48 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:17:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:48 INFO mapreduce.Job: Running job: job_local38036051_0001\n",
      "16/02/04 08:17:48 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:17:48 INFO mapred.LocalJobRunner: Starting task: attempt_local38036051_0001_m_000000_0\n",
      "16/02/04 08:17:49 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:49 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:49 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:17:49 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:17:49 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:49 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/04 08:17:49 INFO mapreduce.Job: Job job_local38036051_0001 running in uber mode : false\n",
      "16/02/04 08:17:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:17:50 INFO streaming.PipeMapRed: R/W/S=10000/238936/0 in:10000=10000/1 [rec/s] out:238955=238955/1 [rec/s]\n",
      "16/02/04 08:17:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:51 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:17:51 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:17:51 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:17:51 INFO mapred.MapTask: bufstart = 0; bufend = 5712360; bufvoid = 104857600\n",
      "16/02/04 08:17:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 23167808(92671232); length = 3046589/6553600\n",
      "16/02/04 08:17:52 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:17:52 INFO mapred.Task: Task:attempt_local38036051_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: Records R/W=1216/1\n",
      "16/02/04 08:17:52 INFO mapred.Task: Task 'attempt_local38036051_0001_m_000000_0' done.\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local38036051_0001_m_000000_0\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: Starting task: attempt_local38036051_0001_r_000000_0\n",
      "16/02/04 08:17:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:17:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:17:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:17:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4a8dd17f\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:17:52 INFO reduce.EventFetcher: attempt_local38036051_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:17:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local38036051_0001_m_000000_0 decomp: 7235658 len: 7235662 to MEMORY\n",
      "16/02/04 08:17:52 INFO reduce.InMemoryMapOutput: Read 7235658 bytes from map-output for attempt_local38036051_0001_m_000000_0\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 7235658, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->7235658\n",
      "16/02/04 08:17:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:17:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:17:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7235654 bytes\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 7235658 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: Merging 1 files, 7235662 bytes from disk\n",
      "16/02/04 08:17:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:17:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:17:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 7235654 bytes\n",
      "16/02/04 08:17:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:17:52 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:17:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:17:53 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:53 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:17:53 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:300000=300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:54 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:400000=400000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/02/04 08:17:54 INFO streaming.PipeMapRed: Records R/W=432991/1\n",
      "16/02/04 08:17:54 INFO streaming.PipeMapRed: R/W/S=500000/2889/0 in:250000=500000/2 [rec/s] out:1444=2889/2 [rec/s]\n",
      "16/02/04 08:17:55 INFO streaming.PipeMapRed: R/W/S=600000/5780/0 in:300000=600000/2 [rec/s] out:2890=5780/2 [rec/s]\n",
      "16/02/04 08:17:55 INFO streaming.PipeMapRed: R/W/S=700000/8675/0 in:350000=700000/2 [rec/s] out:4337=8675/2 [rec/s]\n",
      "16/02/04 08:17:55 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:17:55 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:17:55 INFO mapred.Task: Task:attempt_local38036051_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:17:55 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:17:55 INFO mapred.Task: Task attempt_local38036051_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:17:55 INFO output.FileOutputCommitter: Saved output of task 'attempt_local38036051_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_3/_temporary/0/task_local38036051_0001_r_000000\n",
      "16/02/04 08:17:55 INFO mapred.LocalJobRunner: Records R/W=432991/1 > reduce\n",
      "16/02/04 08:17:55 INFO mapred.Task: Task 'attempt_local38036051_0001_r_000000_0' done.\n",
      "16/02/04 08:17:55 INFO mapred.LocalJobRunner: Finishing task: attempt_local38036051_0001_r_000000_0\n",
      "16/02/04 08:17:55 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:17:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:17:55 INFO mapreduce.Job: Job job_local38036051_0001 completed successfully\n",
      "16/02/04 08:17:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=14683152\n",
      "\t\tFILE: Number of bytes written=22474080\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=142667\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=761648\n",
      "\t\tMap output bytes=5712360\n",
      "\t\tMap output materialized bytes=7235662\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=7235662\n",
      "\t\tReduce input records=761648\n",
      "\t\tReduce output records=12593\n",
      "\t\tSpilled Records=1523296\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=494927872\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142667\n",
      "16/02/04 08:17:55 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_3\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/ProductPurchaseData.txt \\\n",
    "-output /user/hw3/output_3_3 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:17:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_3/part-00000 > counts_3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put counts_3_3 /user/hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using a second map-reduce job to perform the sort **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "i = 0\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    print \"%s,%s\" %(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "i=0\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[,]',line.strip())\n",
    "\n",
    "    if(i==0):\n",
    "        total_count = int(line[1])\n",
    "        i = i+1\n",
    "    elif(i==1):\n",
    "        rel_freq = float(line[1])/float(total_count)\n",
    "        print \"%s,%s,%s,%s\" %(line[0],line[1],rel_freq,total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:18:03 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:18:03 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:18:03 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:18:03 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:18:03 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/04 08:18:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1594728140_0001\n",
      "16/02/04 08:18:04 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:18:04 INFO mapreduce.Job: Running job: job_local1594728140_0001\n",
      "16/02/04 08:18:04 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:18:04 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:18:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:04 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:18:04 INFO mapred.LocalJobRunner: Starting task: attempt_local1594728140_0001_m_000000_0\n",
      "16/02/04 08:18:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:04 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:18:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/counts_3_3:0+142667\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:18:04 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:18:04 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:04 INFO streaming.PipeMapRed: Records R/W=11568/1\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:18:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:18:05 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:18:05 INFO mapred.MapTask: bufstart = 0; bufend = 155260; bufvoid = 104857600\n",
      "16/02/04 08:18:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26164028(104656112); length = 50369/6553600\n",
      "16/02/04 08:18:05 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:18:05 INFO mapred.Task: Task:attempt_local1594728140_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Records R/W=11568/1\n",
      "16/02/04 08:18:05 INFO mapred.Task: Task 'attempt_local1594728140_0001_m_000000_0' done.\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1594728140_0001_m_000000_0\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Starting task: attempt_local1594728140_0001_r_000000_0\n",
      "16/02/04 08:18:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:18:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:18:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@58e4e878\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:18:05 INFO reduce.EventFetcher: attempt_local1594728140_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:18:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1594728140_0001_m_000000_0 decomp: 180448 len: 180452 to MEMORY\n",
      "16/02/04 08:18:05 INFO reduce.InMemoryMapOutput: Read 180448 bytes from map-output for attempt_local1594728140_0001_m_000000_0\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 180448, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->180448\n",
      "16/02/04 08:18:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:18:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:18:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 180437 bytes\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: Merged 1 segments, 180448 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: Merging 1 files, 180452 bytes from disk\n",
      "16/02/04 08:18:05 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:18:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:18:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 180437 bytes\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:18:05 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:18:05 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:18:05 INFO mapreduce.Job: Job job_local1594728140_0001 running in uber mode : false\n",
      "16/02/04 08:18:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: Records R/W=10578/1\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:18:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:18:05 INFO mapred.Task: Task:attempt_local1594728140_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:18:05 INFO mapred.Task: Task attempt_local1594728140_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:18:05 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1594728140_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_3_out/_temporary/0/task_local1594728140_0001_r_000000\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Records R/W=10578/1 > reduce\n",
      "16/02/04 08:18:05 INFO mapred.Task: Task 'attempt_local1594728140_0001_r_000000_0' done.\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1594728140_0001_r_000000_0\n",
      "16/02/04 08:18:05 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:18:06 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:18:06 INFO mapreduce.Job: Job job_local1594728140_0001 completed successfully\n",
      "16/02/04 08:18:06 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=572706\n",
      "\t\tFILE: Number of bytes written=1318772\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=285334\n",
      "\t\tHDFS: Number of bytes written=469371\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12593\n",
      "\t\tMap output records=12593\n",
      "\t\tMap output bytes=155260\n",
      "\t\tMap output materialized bytes=180452\n",
      "\t\tInput split bytes=93\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=180452\n",
      "\t\tReduce input records=12593\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25186\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=492830720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=142667\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=469371\n",
      "16/02/04 08:18:06 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_3_out\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.comparator.options=-k2,2nr \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/counts_3_3 \\\n",
    "-output /user/hw3/output_3_3_out \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "   12592\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_3_out/part-00000 | wc -l "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The number of unique items is 12592 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The output below is the top 50 most encountered product ID's alongwith their frequencies and relative frequencies **\n",
    "\n",
    "** The format is Product ID, Frequency, Relative Frequency, Total Number of ID's **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779,6667,0.0175067747831,380824\t\n",
      "FRO40251,3881,0.010191059387,380824\t\n",
      "ELE17451,3875,0.0101753040775,380824\t\n",
      "GRO73461,3602,0.00945843749344,380824\t\n",
      "SNA80324,3044,0.00799319370628,380824\t\n",
      "ELE32164,2851,0.0074863979161,380824\t\n",
      "DAI75645,2736,0.00718442114993,380824\t\n",
      "SNA45677,2455,0.0064465474865,380824\t\n",
      "FRO31317,2330,0.0061183118711,380824\t\n",
      "DAI85309,2293,0.00602115412894,380824\t\n",
      "ELE26917,2292,0.00601852824402,380824\t\n",
      "FRO80039,2233,0.00586360103355,380824\t\n",
      "GRO21487,2115,0.00555374661261,380824\t\n",
      "SNA99873,2083,0.00546971829507,380824\t\n",
      "GRO59710,2004,0.00526227338613,380824\t\n",
      "GRO71621,1920,0.00504169905258,380824\t\n",
      "FRO85978,1918,0.00503644728273,380824\t\n",
      "GRO30386,1840,0.00483162825872,380824\t\n",
      "ELE74009,1816,0.00476860702057,380824\t\n",
      "GRO56726,1784,0.00468457870302,380824\t\n",
      "DAI63921,1773,0.00465569396887,380824\t\n",
      "GRO46854,1756,0.00461105392517,380824\t\n",
      "ELE66600,1713,0.00449814087347,380824\t\n",
      "DAI83733,1712,0.00449551498855,380824\t\n",
      "FRO32293,1702,0.00446925613932,380824\t\n",
      "ELE66810,1697,0.0044561267147,380824\t\n",
      "SNA55762,1646,0.00432220658362,380824\t\n",
      "DAI22177,1627,0.00427231477008,380824\t\n",
      "FRO78087,1531,0.00402022981745,380824\t\n",
      "ELE99737,1516,0.0039808415436,380824\t\n",
      "ELE34057,1489,0.00390994265067,380824\t\n",
      "GRO94758,1489,0.00390994265067,380824\t\n",
      "FRO35904,1436,0.00377077074974,380824\t\n",
      "FRO53271,1420,0.00372875659097,380824\t\n",
      "SNA93860,1407,0.00369462008697,380824\t\n",
      "SNA90094,1390,0.00364998004327,380824\t\n",
      "GRO38814,1352,0.00355019641619,380824\t\n",
      "ELE56788,1345,0.00353181522173,380824\t\n",
      "GRO61133,1321,0.00346879398357,380824\t\n",
      "DAI88807,1316,0.00345566455896,380824\t\n",
      "ELE74482,1316,0.00345566455896,380824\t\n",
      "ELE59935,1311,0.00344253513434,380824\t\n",
      "SNA96271,1295,0.00340052097557,380824\t\n",
      "DAI43223,1290,0.00338739155095,380824\t\n",
      "ELE91337,1289,0.00338476566603,380824\t\n",
      "GRO15017,1275,0.0033480032771,380824\t\n",
      "DAI31081,1261,0.00331124088818,380824\t\n",
      "GRO81087,1220,0.00320357960633,380824\t\n",
      "DAI22896,1219,0.0032009537214,380824\t\n",
      "GRO85051,1214,0.00318782429679,380824\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_3_out/part-00000 | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "*Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.*\n",
    "\n",
    "*List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support ( the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2.* \n",
    "\n",
    "*Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.* \n",
    "\n",
    "*Please output records of the following form for the top 50 pairs (itemsets of size 2):*\n",
    "\n",
    "      *item1, item2, support count, support*\n",
    "\n",
    "*Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order.* \n",
    "\n",
    "*Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\s]',line.strip())\n",
    "    for l in line:\n",
    "        for p in line:\n",
    "            if(p > l):\n",
    "                print \"%s,%s\\t1\" %(l,p)\n",
    "            elif (l > p):\n",
    "                 print \"%s,%s\\t1\" %(p,l)               \n",
    "                #print \"%s,*\\t1\" %l   Use this only if you want total number of tuples in the denominator for support\n",
    "    print \"BASKET,*\\t1\" #Use this if you want basket in the denominator for support calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "count = 0\n",
    "prev_id = None\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num reducer calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    if((prev_id !=None) and (line[0] !=prev_id)):\n",
    "        print \"%s\\t%s\" %(prev_id,count)\n",
    "        count = 0\n",
    "    count +=1\n",
    "    prev_id = line[0]\n",
    "print \"%s\\t%s\" %(prev_id,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/output_3_4i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:18:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:18:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:18:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:18:15 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:18:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:18:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:18:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1172650280_0001\n",
      "16/02/04 08:18:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:18:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:18:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:18:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:16 INFO mapreduce.Job: Running job: job_local1172650280_0001\n",
      "16/02/04 08:18:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:18:16 INFO mapred.LocalJobRunner: Starting task: attempt_local1172650280_0001_m_000000_0\n",
      "16/02/04 08:18:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:18:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:18:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:18:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:18:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:18:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:17 INFO streaming.PipeMapRed: Records R/W=2349/1\n",
      "16/02/04 08:18:17 INFO mapreduce.Job: Job job_local1172650280_0001 running in uber mode : false\n",
      "16/02/04 08:18:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:18:19 INFO streaming.PipeMapRed: R/W/S=10000/1631948/0 in:5000=10000/2 [rec/s] out:815981=1631963/2 [rec/s]\n",
      "16/02/04 08:18:20 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:18:20 INFO mapred.MapTask: bufstart = 0; bufend = 46554754; bufvoid = 104857600\n",
      "16/02/04 08:18:20 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 16881572(67526288); length = 9332825/6553600\n",
      "16/02/04 08:18:20 INFO mapred.MapTask: (EQUATOR) 56141730 kvi 14035428(56141712)\n",
      "16/02/04 08:18:22 INFO mapred.LocalJobRunner: Records R/W=2349/1 > map\n",
      "16/02/04 08:18:23 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/02/04 08:18:25 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:18:25 INFO mapred.MapTask: (RESET) equator 56141730 kv 14035428(56141712) kvi 11748300(46993200)\n",
      "16/02/04 08:18:25 INFO mapred.LocalJobRunner: Records R/W=2349/1 > map\n",
      "16/02/04 08:18:27 INFO streaming.PipeMapRed: Records R/W=25559/4091103\n",
      "16/02/04 08:18:27 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:18:27 INFO mapred.MapTask: bufstart = 56141730; bufend = 102681917; bufvoid = 104857600\n",
      "16/02/04 08:18:27 INFO mapred.MapTask: kvstart = 14035428(56141712); kvend = 4698960(18795840); length = 9336469/6553600\n",
      "16/02/04 08:18:27 INFO mapred.MapTask: (EQUATOR) 7411277 kvi 1852812(7411248)\n",
      "16/02/04 08:18:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:18:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:18:28 INFO mapred.LocalJobRunner: Records R/W=2349/1 > map\n",
      "16/02/04 08:18:28 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:18:28 INFO mapred.LocalJobRunner: Records R/W=25559/4091103 > sort\n",
      "16/02/04 08:18:29 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/04 08:18:31 INFO mapred.LocalJobRunner: Records R/W=25559/4091103 > sort\n",
      "16/02/04 08:18:32 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/04 08:18:32 INFO mapred.MapTask: (RESET) equator 7411277 kv 1852812(7411248) kvi 125276(501104)\n",
      "16/02/04 08:18:32 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:18:32 INFO mapred.MapTask: bufstart = 7411277; bufend = 16020607; bufvoid = 104857600\n",
      "16/02/04 08:18:32 INFO mapred.MapTask: kvstart = 1852812(7411248); kvend = 125280(501120); length = 1727533/6553600\n",
      "16/02/04 08:18:33 INFO mapred.MapTask: Finished spill 2\n",
      "16/02/04 08:18:33 INFO mapred.Merger: Merging 3 sorted segments\n",
      "16/02/04 08:18:33 INFO mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 111902674 bytes\n",
      "16/02/04 08:18:34 INFO mapred.LocalJobRunner: Records R/W=25559/4091103 > sort > \n",
      "16/02/04 08:18:35 INFO mapreduce.Job:  map 77% reduce 0%\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: Records R/W=25559/4091103 > sort > \n",
      "16/02/04 08:18:37 INFO mapred.Task: Task:attempt_local1172650280_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: Records R/W=25559/4091103 > sort\n",
      "16/02/04 08:18:37 INFO mapred.Task: Task 'attempt_local1172650280_0001_m_000000_0' done.\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local1172650280_0001_m_000000_0\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:18:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1172650280_0001_r_000000_0\n",
      "16/02/04 08:18:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:18:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:18:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:18:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2e058ad7\n",
      "16/02/04 08:18:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:18:37 INFO reduce.EventFetcher: attempt_local1172650280_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:18:37 INFO reduce.MergeManagerImpl: attempt_local1172650280_0001_m_000000_0: Shuffling to disk since 111902691 is greater than maxSingleShuffleLimit (83584616)\n",
      "16/02/04 08:18:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1172650280_0001_m_000000_0 decomp: 111902691 len: 111902695 to DISK\n",
      "16/02/04 08:18:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:18:38 INFO reduce.OnDiskMapOutput: Read 111902695 bytes from map-output for attempt_local1172650280_0001_m_000000_0\n",
      "16/02/04 08:18:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:18:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:18:38 INFO reduce.MergeManagerImpl: finalMerge called with 0 in-memory map-outputs and 1 on-disk map-outputs\n",
      "16/02/04 08:18:38 INFO reduce.MergeManagerImpl: Merging 1 files, 111902695 bytes from disk\n",
      "16/02/04 08:18:38 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:18:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:18:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 111902680 bytes\n",
      "16/02/04 08:18:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:18:38 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:18:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:38 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:39 INFO streaming.PipeMapRed: Records R/W=46762/1\n",
      "16/02/04 08:18:39 INFO streaming.PipeMapRed: R/W/S=100000/14685/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:18:39 INFO streaming.PipeMapRed: R/W/S=200000/35899/0 in:200000=200000/1 [rec/s] out:35899=35899/1 [rec/s]\n",
      "16/02/04 08:18:40 INFO streaming.PipeMapRed: R/W/S=300000/48905/0 in:300000=300000/1 [rec/s] out:48905=48905/1 [rec/s]\n",
      "16/02/04 08:18:40 INFO streaming.PipeMapRed: R/W/S=400000/70115/0 in:400000=400000/1 [rec/s] out:70115=70115/1 [rec/s]\n",
      "16/02/04 08:18:41 INFO streaming.PipeMapRed: R/W/S=500000/88855/0 in:250000=500000/2 [rec/s] out:44427=88855/2 [rec/s]\n",
      "16/02/04 08:18:41 INFO streaming.PipeMapRed: R/W/S=600000/110063/0 in:300000=600000/2 [rec/s] out:55031=110063/2 [rec/s]\n",
      "16/02/04 08:18:41 INFO streaming.PipeMapRed: R/W/S=700000/123884/0 in:350000=700000/2 [rec/s] out:61942=123884/2 [rec/s]\n",
      "16/02/04 08:18:42 INFO streaming.PipeMapRed: R/W/S=800000/143451/0 in:266666=800000/3 [rec/s] out:47817=143451/3 [rec/s]\n",
      "16/02/04 08:18:42 INFO streaming.PipeMapRed: R/W/S=900000/163002/0 in:300000=900000/3 [rec/s] out:54334=163002/3 [rec/s]\n",
      "16/02/04 08:18:42 INFO streaming.PipeMapRed: R/W/S=1000000/180931/0 in:333333=1000000/3 [rec/s] out:60310=180931/3 [rec/s]\n",
      "16/02/04 08:18:43 INFO streaming.PipeMapRed: R/W/S=1100000/192320/0 in:275000=1100000/4 [rec/s] out:48080=192320/4 [rec/s]\n",
      "16/02/04 08:18:43 INFO streaming.PipeMapRed: R/W/S=1200000/195530/0 in:300000=1200000/4 [rec/s] out:48882=195530/4 [rec/s]\n",
      "16/02/04 08:18:43 INFO streaming.PipeMapRed: R/W/S=1300000/211821/0 in:325000=1300000/4 [rec/s] out:52955=211821/4 [rec/s]\n",
      "16/02/04 08:18:43 INFO mapred.LocalJobRunner: Records R/W=46762/1 > reduce\n",
      "16/02/04 08:18:44 INFO streaming.PipeMapRed: R/W/S=1400000/232202/0 in:280000=1400000/5 [rec/s] out:46440=232202/5 [rec/s]\n",
      "16/02/04 08:18:44 INFO streaming.PipeMapRed: R/W/S=1500000/244411/0 in:300000=1500000/5 [rec/s] out:48882=244411/5 [rec/s]\n",
      "16/02/04 08:18:44 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/04 08:18:44 INFO streaming.PipeMapRed: R/W/S=1600000/260696/0 in:320000=1600000/5 [rec/s] out:52139=260696/5 [rec/s]\n",
      "16/02/04 08:18:45 INFO streaming.PipeMapRed: R/W/S=1700000/272082/0 in:283333=1700000/6 [rec/s] out:45347=272082/6 [rec/s]\n",
      "16/02/04 08:18:45 INFO streaming.PipeMapRed: R/W/S=1800000/287541/0 in:300000=1800000/6 [rec/s] out:47923=287541/6 [rec/s]\n",
      "16/02/04 08:18:45 INFO streaming.PipeMapRed: R/W/S=1900000/305471/0 in:316666=1900000/6 [rec/s] out:50911=305471/6 [rec/s]\n",
      "16/02/04 08:18:46 INFO streaming.PipeMapRed: R/W/S=2000000/325841/0 in:285714=2000000/7 [rec/s] out:46548=325841/7 [rec/s]\n",
      "16/02/04 08:18:46 INFO streaming.PipeMapRed: R/W/S=2100000/342952/0 in:300000=2100000/7 [rec/s] out:48993=342952/7 [rec/s]\n",
      "16/02/04 08:18:46 INFO mapred.LocalJobRunner: Records R/W=46762/1 > reduce\n",
      "16/02/04 08:18:46 INFO streaming.PipeMapRed: R/W/S=2200000/351879/0 in:275000=2200000/8 [rec/s] out:43984=351879/8 [rec/s]\n",
      "16/02/04 08:18:47 INFO streaming.PipeMapRed: R/W/S=2300000/369799/0 in:287500=2300000/8 [rec/s] out:46224=369799/8 [rec/s]\n",
      "16/02/04 08:18:47 INFO streaming.PipeMapRed: R/W/S=2400000/385985/0 in:300000=2400000/8 [rec/s] out:48248=385991/8 [rec/s]\n",
      "16/02/04 08:18:47 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/04 08:18:47 INFO streaming.PipeMapRed: R/W/S=2500000/402381/0 in:277777=2500000/9 [rec/s] out:44709=402381/9 [rec/s]\n",
      "16/02/04 08:18:48 INFO streaming.PipeMapRed: R/W/S=2600000/417845/0 in:288888=2600000/9 [rec/s] out:46427=417845/9 [rec/s]\n",
      "16/02/04 08:18:48 INFO streaming.PipeMapRed: R/W/S=2700000/436592/0 in:300000=2700000/9 [rec/s] out:48510=436592/9 [rec/s]\n",
      "16/02/04 08:18:49 INFO streaming.PipeMapRed: R/W/S=2800000/459441/0 in:280000=2800000/10 [rec/s] out:45944=459441/10 [rec/s]\n",
      "16/02/04 08:18:49 INFO streaming.PipeMapRed: Records R/W=2864552/470840\n",
      "16/02/04 08:18:49 INFO streaming.PipeMapRed: R/W/S=2900000/477363/0 in:290000=2900000/10 [rec/s] out:47736=477363/10 [rec/s]\n",
      "16/02/04 08:18:49 INFO streaming.PipeMapRed: R/W/S=3000000/496102/0 in:300000=3000000/10 [rec/s] out:49610=496102/10 [rec/s]\n",
      "16/02/04 08:18:49 INFO mapred.LocalJobRunner: Records R/W=2864552/470840 > reduce\n",
      "16/02/04 08:18:50 INFO streaming.PipeMapRed: R/W/S=3100000/509111/0 in:281818=3100000/11 [rec/s] out:46282=509111/11 [rec/s]\n",
      "16/02/04 08:18:50 INFO streaming.PipeMapRed: R/W/S=3200000/526214/0 in:290909=3200000/11 [rec/s] out:47837=526214/11 [rec/s]\n",
      "16/02/04 08:18:50 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/02/04 08:18:50 INFO streaming.PipeMapRed: R/W/S=3300000/549060/0 in:300000=3300000/11 [rec/s] out:49914=549060/11 [rec/s]\n",
      "16/02/04 08:18:51 INFO streaming.PipeMapRed: R/W/S=3400000/565357/0 in:283333=3400000/12 [rec/s] out:47113=565357/12 [rec/s]\n",
      "16/02/04 08:18:51 INFO streaming.PipeMapRed: R/W/S=3500000/587379/0 in:291666=3500000/12 [rec/s] out:48948=587379/12 [rec/s]\n",
      "16/02/04 08:18:52 INFO streaming.PipeMapRed: R/W/S=3600000/608575/0 in:276923=3600000/13 [rec/s] out:46813=608575/13 [rec/s]\n",
      "16/02/04 08:18:52 INFO streaming.PipeMapRed: R/W/S=3700000/624050/0 in:284615=3700000/13 [rec/s] out:48003=624050/13 [rec/s]\n",
      "16/02/04 08:18:52 INFO mapred.LocalJobRunner: Records R/W=2864552/470840 > reduce\n",
      "16/02/04 08:18:53 INFO streaming.PipeMapRed: R/W/S=3800000/639533/0 in:271428=3800000/14 [rec/s] out:45680=639533/14 [rec/s]\n",
      "16/02/04 08:18:53 INFO streaming.PipeMapRed: R/W/S=3900000/658276/0 in:278571=3900000/14 [rec/s] out:47019=658276/14 [rec/s]\n",
      "16/02/04 08:18:53 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/04 08:18:53 INFO streaming.PipeMapRed: R/W/S=4000000/680307/0 in:285714=4000000/14 [rec/s] out:48593=680307/14 [rec/s]\n",
      "16/02/04 08:18:54 INFO streaming.PipeMapRed: R/W/S=4100000/700693/0 in:273333=4100000/15 [rec/s] out:46712=700693/15 [rec/s]\n",
      "16/02/04 08:18:54 INFO streaming.PipeMapRed: R/W/S=4200000/716164/0 in:280000=4200000/15 [rec/s] out:47744=716164/15 [rec/s]\n",
      "16/02/04 08:18:54 INFO streaming.PipeMapRed: R/W/S=4300000/734909/0 in:268750=4300000/16 [rec/s] out:45931=734909/16 [rec/s]\n",
      "16/02/04 08:18:55 INFO streaming.PipeMapRed: R/W/S=4400000/752830/0 in:275000=4400000/16 [rec/s] out:47051=752830/16 [rec/s]\n",
      "16/02/04 08:18:55 INFO streaming.PipeMapRed: R/W/S=4500000/769128/0 in:281250=4500000/16 [rec/s] out:48070=769128/16 [rec/s]\n",
      "16/02/04 08:18:55 INFO mapred.LocalJobRunner: Records R/W=2864552/470840 > reduce\n",
      "16/02/04 08:18:55 INFO streaming.PipeMapRed: R/W/S=4600000/787054/0 in:270588=4600000/17 [rec/s] out:46297=787054/17 [rec/s]\n",
      "16/02/04 08:18:56 INFO streaming.PipeMapRed: R/W/S=4700000/804154/0 in:276470=4700000/17 [rec/s] out:47303=804157/17 [rec/s]\n",
      "16/02/04 08:18:56 INFO streaming.PipeMapRed: R/W/S=4800000/817996/0 in:282352=4800000/17 [rec/s] out:48117=817996/17 [rec/s]\n",
      "16/02/04 08:18:56 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/04 08:18:57 INFO streaming.PipeMapRed: R/W/S=4900000/836739/0 in:272222=4900000/18 [rec/s] out:46485=836739/18 [rec/s]\n",
      "16/02/04 08:18:57 INFO streaming.PipeMapRed: R/W/S=5000000/857947/0 in:277777=5000000/18 [rec/s] out:47663=857947/18 [rec/s]\n",
      "16/02/04 08:18:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:18:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:18:57 INFO mapred.Task: Task:attempt_local1172650280_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:18:57 INFO mapred.LocalJobRunner: Records R/W=2864552/470840 > reduce\n",
      "16/02/04 08:18:57 INFO mapred.Task: Task attempt_local1172650280_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:18:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1172650280_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_4i/_temporary/0/task_local1172650280_0001_r_000000\n",
      "16/02/04 08:18:57 INFO mapred.LocalJobRunner: Records R/W=2864552/470840 > reduce\n",
      "16/02/04 08:18:57 INFO mapred.Task: Task 'attempt_local1172650280_0001_r_000000_0' done.\n",
      "16/02/04 08:18:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local1172650280_0001_r_000000_0\n",
      "16/02/04 08:18:57 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:18:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:18:58 INFO mapreduce.Job: Job job_local1172650280_0001 completed successfully\n",
      "16/02/04 08:18:58 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=447822632\n",
      "\t\tFILE: Number of bytes written=560286613\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=17637495\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=5099209\n",
      "\t\tMap output bytes=101704271\n",
      "\t\tMap output materialized bytes=111902695\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=111902695\n",
      "\t\tReduce input records=5099209\n",
      "\t\tReduce output records=877096\n",
      "\t\tSpilled Records=15297627\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=605552640\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum mapper calls=1\n",
      "\t\tNum reducer calls=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17637495\n",
      "16/02/04 08:18:58 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_4i\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/ProductPurchaseData.txt \\\n",
    "-output /user/hw3/output_3_4i \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:19:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_4i/part-00000 > pairs_unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:19:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put pairs_unsorted /user/hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This second Map-reduce job is used to sort the pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    print \"%s,%s\" %(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num reducer calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[,]',line.strip())\n",
    "    if(line[1]==\"*\"):\n",
    "        total = int(line[2]) \n",
    "    else:\n",
    "        rel_freq = float(line[2])/float(total)\n",
    "        print \"%s,%s,%s,%s,%s\" %(line[0],line[1],line[2],rel_freq,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:19:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:19:06 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:19:06 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:19:06 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:19:06 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:19:06 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:19:06 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/04 08:19:06 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/04 08:19:06 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:19:06 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/04 08:19:07 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local774545073_0001\n",
      "16/02/04 08:19:07 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:19:07 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:19:07 INFO mapreduce.Job: Running job: job_local774545073_0001\n",
      "16/02/04 08:19:07 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:19:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:07 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:19:07 INFO mapred.LocalJobRunner: Starting task: attempt_local774545073_0001_m_000000_0\n",
      "16/02/04 08:19:07 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:07 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:19:07 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/pairs_unsorted:0+17637495\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:19:07 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:19:07 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: Records R/W=6532/1\n",
      "16/02/04 08:19:07 INFO streaming.PipeMapRed: R/W/S=10000/5717/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:08 INFO mapreduce.Job: Job job_local774545073_0001 running in uber mode : false\n",
      "16/02/04 08:19:08 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:19:08 INFO streaming.PipeMapRed: R/W/S=100000/92927/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:08 INFO streaming.PipeMapRed: R/W/S=200000/191519/0 in:200000=200000/1 [rec/s] out:191519=191519/1 [rec/s]\n",
      "16/02/04 08:19:09 INFO streaming.PipeMapRed: R/W/S=300000/294871/0 in:300000=300000/1 [rec/s] out:294871=294871/1 [rec/s]\n",
      "16/02/04 08:19:09 INFO streaming.PipeMapRed: R/W/S=400000/392616/0 in:200000=400000/2 [rec/s] out:196308=392616/2 [rec/s]\n",
      "16/02/04 08:19:10 INFO streaming.PipeMapRed: R/W/S=500000/491207/0 in:250000=500000/2 [rec/s] out:245603=491207/2 [rec/s]\n",
      "16/02/04 08:19:10 INFO streaming.PipeMapRed: R/W/S=600000/594719/0 in:300000=600000/2 [rec/s] out:297359=594719/2 [rec/s]\n",
      "16/02/04 08:19:11 INFO streaming.PipeMapRed: R/W/S=700000/693360/0 in:233333=700000/3 [rec/s] out:231120=693360/3 [rec/s]\n",
      "16/02/04 08:19:11 INFO streaming.PipeMapRed: R/W/S=800000/791124/0 in:266666=800000/3 [rec/s] out:263708=791124/3 [rec/s]\n",
      "16/02/04 08:19:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:19:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:19:11 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:19:11 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:19:11 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:19:11 INFO mapred.MapTask: bufstart = 0; bufend = 18514591; bufvoid = 104857600\n",
      "16/02/04 08:19:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22706016(90824064); length = 3508381/6553600\n",
      "16/02/04 08:19:12 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:19:12 INFO mapred.Task: Task:attempt_local774545073_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:19:12 INFO mapred.LocalJobRunner: Records R/W=6532/1\n",
      "16/02/04 08:19:12 INFO mapred.Task: Task 'attempt_local774545073_0001_m_000000_0' done.\n",
      "16/02/04 08:19:12 INFO mapred.LocalJobRunner: Finishing task: attempt_local774545073_0001_m_000000_0\n",
      "16/02/04 08:19:12 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:19:12 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:19:12 INFO mapred.LocalJobRunner: Starting task: attempt_local774545073_0001_r_000000_0\n",
      "16/02/04 08:19:13 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:13 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:19:13 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:19:13 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@65cd4c97\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:19:13 INFO reduce.EventFetcher: attempt_local774545073_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:19:13 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local774545073_0001_m_000000_0 decomp: 20268785 len: 20268789 to MEMORY\n",
      "16/02/04 08:19:13 INFO reduce.InMemoryMapOutput: Read 20268785 bytes from map-output for attempt_local774545073_0001_m_000000_0\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20268785, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20268785\n",
      "16/02/04 08:19:13 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:19:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:19:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:19:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20268768 bytes\n",
      "16/02/04 08:19:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: Merged 1 segments, 20268785 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: Merging 1 files, 20268789 bytes from disk\n",
      "16/02/04 08:19:13 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:19:13 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:19:13 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20268768 bytes\n",
      "16/02/04 08:19:13 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:19:13 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:19:13 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: Records R/W=9316/1\n",
      "16/02/04 08:19:13 INFO streaming.PipeMapRed: R/W/S=10000/56/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:14 INFO streaming.PipeMapRed: R/W/S=100000/89070/0 in:100000=100000/1 [rec/s] out:89086=89086/1 [rec/s]\n",
      "16/02/04 08:19:15 INFO streaming.PipeMapRed: R/W/S=200000/188965/0 in:200000=200000/1 [rec/s] out:188965=188965/1 [rec/s]\n",
      "16/02/04 08:19:16 INFO streaming.PipeMapRed: R/W/S=300000/288793/0 in:150000=300000/2 [rec/s] out:144396=288793/2 [rec/s]\n",
      "16/02/04 08:19:17 INFO streaming.PipeMapRed: R/W/S=400000/388315/0 in:133333=400000/3 [rec/s] out:129438=388315/3 [rec/s]\n",
      "16/02/04 08:19:18 INFO streaming.PipeMapRed: R/W/S=500000/488481/0 in:125000=500000/4 [rec/s] out:122120=488481/4 [rec/s]\n",
      "16/02/04 08:19:18 INFO streaming.PipeMapRed: R/W/S=600000/588274/0 in:120000=600000/5 [rec/s] out:117654=588274/5 [rec/s]\n",
      "16/02/04 08:19:19 INFO mapred.LocalJobRunner: Records R/W=9316/1 > reduce\n",
      "16/02/04 08:19:19 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/04 08:19:19 INFO streaming.PipeMapRed: R/W/S=700000/688068/0 in:116666=700000/6 [rec/s] out:114678=688068/6 [rec/s]\n",
      "16/02/04 08:19:20 INFO streaming.PipeMapRed: R/W/S=800000/787861/0 in:133333=800000/6 [rec/s] out:131310=787861/6 [rec/s]\n",
      "16/02/04 08:19:21 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:19:21 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:19:21 INFO mapred.Task: Task:attempt_local774545073_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:19:21 INFO mapred.LocalJobRunner: Records R/W=9316/1 > reduce\n",
      "16/02/04 08:19:21 INFO mapred.Task: Task attempt_local774545073_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:19:21 INFO output.FileOutputCommitter: Saved output of task 'attempt_local774545073_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_4_out/_temporary/0/task_local774545073_0001_r_000000\n",
      "16/02/04 08:19:21 INFO mapred.LocalJobRunner: Records R/W=9316/1 > reduce\n",
      "16/02/04 08:19:21 INFO mapred.Task: Task 'attempt_local774545073_0001_r_000000_0' done.\n",
      "16/02/04 08:19:21 INFO mapred.LocalJobRunner: Finishing task: attempt_local774545073_0001_r_000000_0\n",
      "16/02/04 08:19:21 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:19:22 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:19:22 INFO mapreduce.Job: Job job_local774545073_0001 completed successfully\n",
      "16/02/04 08:19:22 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=40749390\n",
      "\t\tFILE: Number of bytes written=61580781\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=35274990\n",
      "\t\tHDFS: Number of bytes written=39406367\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877096\n",
      "\t\tMap output records=877096\n",
      "\t\tMap output bytes=18514591\n",
      "\t\tMap output materialized bytes=20268789\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=20268789\n",
      "\t\tReduce input records=877096\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=1754192\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=33\n",
      "\t\tTotal committed heap usage (bytes)=567279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum mapper calls=1\n",
      "\t\tNum reducer calls=1\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=17637495\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=39406367\n",
      "16/02/04 08:19:22 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_4_out\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.comparator.options=-k3,3nr \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/pairs_unsorted \\\n",
    "-output /user/hw3/output_3_4_out \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Computational Setup : **\n",
    "    \n",
    "    MacBook Air\n",
    "    Processor : 1.8 Ghz Intel Core i5 , 2 Cores\n",
    "    Memory : 4 GB 1600 Mhz DDR3\n",
    "    Disk : SSD (Flash Storage) 128 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output is shown below*\n",
    "\n",
    "*Format = Pairs, Support Count, Support, Total number of baskets *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:19:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779,ELE17451,3184,0.102376129385,31101\t\n",
      "FRO40251,SNA80324,2824,0.0908009388766,31101\t\n",
      "DAI75645,FRO40251,2508,0.0806404938748,31101\t\n",
      "FRO40251,GRO85051,2426,0.0780039227035,31101\t\n",
      "DAI62779,GRO73461,2278,0.0732452332722,31101\t\n",
      "DAI75645,SNA80324,2260,0.0726664737468,31101\t\n",
      "DAI62779,FRO40251,2140,0.0688080769107,31101\t\n",
      "DAI62779,SNA80324,1846,0.0593550046622,31101\t\n",
      "DAI62779,DAI85309,1836,0.0590334715926,31101\t\n",
      "ELE32164,GRO59710,1822,0.058583325295,31101\t\n",
      "FRO40251,GRO73461,1764,0.0567184334909,31101\t\n",
      "DAI62779,DAI75645,1764,0.0567184334909,31101\t\n",
      "DAI62779,ELE92920,1754,0.0563969004212,31101\t\n",
      "FRO40251,FRO92469,1670,0.0536960226359,31101\t\n",
      "DAI62779,ELE32164,1664,0.0535031027941,31101\t\n",
      "DAI75645,GRO73461,1424,0.0457863091219,31101\t\n",
      "DAI43223,ELE32164,1422,0.045722002508,31101\t\n",
      "DAI62779,GRO30386,1418,0.0455933892801,31101\t\n",
      "ELE17451,FRO40251,1394,0.0448217099129,31101\t\n",
      "DAI85309,ELE99737,1318,0.0423780585833,31101\t\n",
      "DAI62779,ELE26917,1300,0.0417992990579,31101\t\n",
      "GRO21487,GRO73461,1262,0.0405774733931,31101\t\n",
      "DAI62779,SNA45677,1208,0.0388411948169,31101\t\n",
      "ELE17451,SNA80324,1194,0.0383910485193,31101\t\n",
      "DAI62779,GRO71621,1190,0.0382624352915,31101\t\n",
      "DAI62779,SNA55762,1186,0.0381338220636,31101\t\n",
      "DAI62779,DAI83733,1172,0.0376836757661,31101\t\n",
      "ELE17451,GRO73461,1160,0.0372978360824,31101\t\n",
      "GRO73461,SNA80324,1124,0.0361403170316,31101\t\n",
      "DAI62779,GRO59710,1122,0.0360760104177,31101\t\n",
      "DAI62779,FRO80039,1100,0.0353686376644,31101\t\n",
      "DAI75645,ELE17451,1094,0.0351757178226,31101\t\n",
      "DAI62779,SNA93860,1074,0.0345326516832,31101\t\n",
      "DAI55148,DAI62779,1052,0.0338252789299,31101\t\n",
      "DAI43223,GRO59710,1024,0.0329249863348,31101\t\n",
      "ELE17451,ELE32164,1022,0.0328606797209,31101\t\n",
      "DAI62779,SNA18336,1012,0.0325391466512,31101\t\n",
      "ELE32164,GRO73461,972,0.0312530143725,31101\t\n",
      "DAI62779,FRO78087,964,0.0309957879168,31101\t\n",
      "DAI85309,ELE17451,964,0.0309957879168,31101\t\n",
      "DAI62779,GRO94758,958,0.030802868075,31101\t\n",
      "DAI62779,GRO21487,942,0.0302884151635,31101\t\n",
      "GRO85051,SNA80324,942,0.0302884151635,31101\t\n",
      "ELE17451,GRO30386,936,0.0300954953217,31101\t\n",
      "FRO85978,SNA95666,926,0.029773962252,31101\t\n",
      "DAI62779,FRO19221,924,0.0297096556381,31101\t\n",
      "DAI62779,GRO46854,922,0.0296453490241,31101\t\n",
      "DAI43223,DAI62779,918,0.0295167357963,31101\t\n",
      "ELE92920,SNA18336,910,0.0292595093405,31101\t\n",
      "DAI88079,FRO40251,892,0.0286807498151,31101\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_4_out/part-00000 | head -n 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.5: Stripes\n",
    "*Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.*\n",
    "\n",
    "*Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)*\n",
    "\n",
    "*Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\s]',line.strip())\n",
    "    \n",
    "    for l in line:\n",
    "        co_stripe = {}\n",
    "        for p in line:\n",
    "            if(l != p):\n",
    "                if p not in co_stripe.keys():\n",
    "                    co_stripe[p] = 1 \n",
    "                else:\n",
    "                    co_stripe[p] += 1\n",
    "        print \"%s\\t%s\" %(l,co_stripe)\n",
    "                #print \"%s,*\\t1\" %l   Use this only if you want total number of tuples in the denominator for support\n",
    "    print \"BASKET\\t{'*':1}\" #Use this if you want basket in the denominator for support calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import ast\n",
    "\n",
    "count = {}\n",
    "prev_id = None\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    co_strip = ast.literal_eval(line[1])\n",
    "    if((prev_id !=None) and (line[0] !=prev_id)):\n",
    "        for w in count.keys():\n",
    "            if(prev_id < w):\n",
    "                print \"%s,%s\\t%s\" %(prev_id,w,count[w])\n",
    "            elif(prev_id >w):\n",
    "                print \"%s,%s\\t%s\" %(w,prev_id,count[w])                \n",
    "        count = {}\n",
    "    for w in co_strip:\n",
    "        if w in count.keys():\n",
    "            count[w] +=1\n",
    "        else:\n",
    "            count[w] = 1\n",
    "    prev_id = line[0]\n",
    "\n",
    "for w in count.keys():\n",
    "    if(prev_id <w):\n",
    "        print \"%s,%s\\t%s\" %(prev_id,w,count[w])\n",
    "    elif(prev_id >w):\n",
    "        print \"%s,%s\\t%s\" %(w,prev_id,count[w])         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:19:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:19:27 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:19:27 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:19:27 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:19:27 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:19:28 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:19:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local112720032_0001\n",
      "16/02/04 08:19:28 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:19:28 INFO mapreduce.Job: Running job: job_local112720032_0001\n",
      "16/02/04 08:19:28 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:19:28 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:19:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:28 INFO mapred.LocalJobRunner: Starting task: attempt_local112720032_0001_m_000000_0\n",
      "16/02/04 08:19:28 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:19:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:19:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/ProductPurchaseData.txt:0+3458517\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:19:28 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:19:28 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:28 INFO streaming.PipeMapRed: Records R/W=1216/1\n",
      "16/02/04 08:19:29 INFO mapreduce.Job: Job job_local112720032_0001 running in uber mode : false\n",
      "16/02/04 08:19:29 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:19:31 INFO streaming.PipeMapRed: R/W/S=10000/129308/0 in:5000=10000/2 [rec/s] out:64654=129308/2 [rec/s]\n",
      "16/02/04 08:19:34 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/04 08:19:35 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/02/04 08:19:35 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:19:35 INFO mapred.MapTask: bufstart = 0; bufend = 77559378; bufvoid = 104857600\n",
      "16/02/04 08:19:35 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24632720(98530880); length = 1581677/6553600\n",
      "16/02/04 08:19:35 INFO mapred.MapTask: (EQUATOR) 79142130 kvi 19785528(79142112)\n",
      "16/02/04 08:19:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:19:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:19:36 INFO mapred.LocalJobRunner: Records R/W=1216/1 > map\n",
      "16/02/04 08:19:36 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:19:37 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: (RESET) equator 79142130 kv 19785528(79142112) kvi 19719508(78878032)\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: bufstart = 79142130; bufend = 82263698; bufvoid = 104857600\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: kvstart = 19785528(79142112); kvend = 19719512(78878048); length = 66017/6553600\n",
      "16/02/04 08:19:37 INFO mapred.MapTask: Finished spill 1\n",
      "16/02/04 08:19:37 INFO mapred.Merger: Merging 2 sorted segments\n",
      "16/02/04 08:19:37 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 81909473 bytes\n",
      "16/02/04 08:19:38 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/04 08:19:39 INFO mapred.Task: Task:attempt_local112720032_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: Records R/W=1216/1 > sort\n",
      "16/02/04 08:19:39 INFO mapred.Task: Task 'attempt_local112720032_0001_m_000000_0' done.\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local112720032_0001_m_000000_0\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: Starting task: attempt_local112720032_0001_r_000000_0\n",
      "16/02/04 08:19:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:19:39 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:19:39 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:19:39 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@e6a470d\n",
      "16/02/04 08:19:39 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:19:39 INFO reduce.EventFetcher: attempt_local112720032_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:19:39 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local112720032_0001_m_000000_0 decomp: 81909481 len: 81909485 to MEMORY\n",
      "16/02/04 08:19:39 INFO reduce.InMemoryMapOutput: Read 81909481 bytes from map-output for attempt_local112720032_0001_m_000000_0\n",
      "16/02/04 08:19:39 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 81909481, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->81909481\n",
      "16/02/04 08:19:39 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:19:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:19:39 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:19:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:19:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:19:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81909472 bytes\n",
      "16/02/04 08:19:40 INFO reduce.MergeManagerImpl: Merged 1 segments, 81909481 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:19:40 INFO reduce.MergeManagerImpl: Merging 1 files, 81909485 bytes from disk\n",
      "16/02/04 08:19:40 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:19:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:19:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81909472 bytes\n",
      "16/02/04 08:19:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:19:40 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:19:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:40 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:19:41 INFO streaming.PipeMapRed: Records R/W=32019/1\n",
      "16/02/04 08:19:45 INFO mapred.LocalJobRunner: Records R/W=32019/1 > reduce\n",
      "16/02/04 08:19:45 INFO mapreduce.Job:  map 100% reduce 68%\n",
      "16/02/04 08:19:48 INFO mapred.LocalJobRunner: Records R/W=32019/1 > reduce\n",
      "16/02/04 08:19:51 INFO mapred.LocalJobRunner: Records R/W=32019/1 > reduce\n",
      "16/02/04 08:19:51 INFO mapreduce.Job:  map 100% reduce 69%\n",
      "16/02/04 08:19:51 INFO streaming.PipeMapRed: Records R/W=57237/121015\n",
      "16/02/04 08:19:54 INFO mapred.LocalJobRunner: Records R/W=57237/121015 > reduce\n",
      "16/02/04 08:19:54 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/02/04 08:19:57 INFO mapred.LocalJobRunner: Records R/W=57237/121015 > reduce\n",
      "16/02/04 08:20:00 INFO mapred.LocalJobRunner: Records R/W=57237/121015 > reduce\n",
      "16/02/04 08:20:03 INFO mapred.LocalJobRunner: Records R/W=57237/121015 > reduce\n",
      "16/02/04 08:20:06 INFO streaming.PipeMapRed: Records R/W=79275/202758\n",
      "16/02/04 08:20:06 INFO mapred.LocalJobRunner: Records R/W=79275/202758 > reduce\n",
      "16/02/04 08:20:06 INFO mapreduce.Job:  map 100% reduce 71%\n",
      "16/02/04 08:20:09 INFO mapred.LocalJobRunner: Records R/W=79275/202758 > reduce\n",
      "16/02/04 08:20:09 INFO mapreduce.Job:  map 100% reduce 72%\n",
      "16/02/04 08:20:12 INFO mapred.LocalJobRunner: Records R/W=79275/202758 > reduce\n",
      "16/02/04 08:20:15 INFO mapred.LocalJobRunner: Records R/W=79275/202758 > reduce\n",
      "16/02/04 08:20:18 INFO mapred.LocalJobRunner: Records R/W=79275/202758 > reduce\n",
      "16/02/04 08:20:18 INFO streaming.PipeMapRed: Records R/W=99022/286063\n",
      "16/02/04 08:20:18 INFO streaming.PipeMapRed: R/W/S=100000/295034/0 in:2702=100000/37 [rec/s] out:7973=295034/37 [rec/s]\n",
      "16/02/04 08:20:18 INFO mapreduce.Job:  map 100% reduce 73%\n",
      "16/02/04 08:20:21 INFO mapred.LocalJobRunner: Records R/W=99022/286063 > reduce\n",
      "16/02/04 08:20:24 INFO mapred.LocalJobRunner: Records R/W=99022/286063 > reduce\n",
      "16/02/04 08:20:24 INFO mapreduce.Job:  map 100% reduce 74%\n",
      "16/02/04 08:20:27 INFO mapred.LocalJobRunner: Records R/W=99022/286063 > reduce\n",
      "16/02/04 08:20:27 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/02/04 08:20:30 INFO mapred.LocalJobRunner: Records R/W=99022/286063 > reduce\n",
      "16/02/04 08:20:32 INFO streaming.PipeMapRed: Records R/W=124029/388200\n",
      "16/02/04 08:20:33 INFO mapred.LocalJobRunner: Records R/W=124029/388200 > reduce\n",
      "16/02/04 08:20:36 INFO mapred.LocalJobRunner: Records R/W=124029/388200 > reduce\n",
      "16/02/04 08:20:36 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/02/04 08:20:39 INFO mapred.LocalJobRunner: Records R/W=124029/388200 > reduce\n",
      "16/02/04 08:20:42 INFO mapred.LocalJobRunner: Records R/W=124029/388200 > reduce\n",
      "16/02/04 08:20:42 INFO streaming.PipeMapRed: Records R/W=141329/468241\n",
      "16/02/04 08:20:45 INFO mapred.LocalJobRunner: Records R/W=141329/468241 > reduce\n",
      "16/02/04 08:20:45 INFO mapreduce.Job:  map 100% reduce 77%\n",
      "16/02/04 08:20:48 INFO mapred.LocalJobRunner: Records R/W=141329/468241 > reduce\n",
      "16/02/04 08:20:48 INFO mapreduce.Job:  map 100% reduce 78%\n",
      "16/02/04 08:20:51 INFO mapred.LocalJobRunner: Records R/W=141329/468241 > reduce\n",
      "16/02/04 08:20:52 INFO streaming.PipeMapRed: Records R/W=165275/582690\n",
      "16/02/04 08:20:54 INFO mapred.LocalJobRunner: Records R/W=165275/582690 > reduce\n",
      "16/02/04 08:20:54 INFO mapreduce.Job:  map 100% reduce 79%\n",
      "16/02/04 08:20:57 INFO mapred.LocalJobRunner: Records R/W=165275/582690 > reduce\n",
      "16/02/04 08:21:00 INFO mapred.LocalJobRunner: Records R/W=165275/582690 > reduce\n",
      "16/02/04 08:21:00 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/02/04 08:21:03 INFO streaming.PipeMapRed: Records R/W=185676/681585\n",
      "16/02/04 08:21:03 INFO mapred.LocalJobRunner: Records R/W=185676/681585 > reduce\n",
      "16/02/04 08:21:06 INFO mapred.LocalJobRunner: Records R/W=185676/681585 > reduce\n",
      "16/02/04 08:21:06 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/04 08:21:08 INFO streaming.PipeMapRed: R/W/S=200000/755990/0 in:2272=200000/88 [rec/s] out:8590=755990/88 [rec/s]\n",
      "16/02/04 08:21:09 INFO mapred.LocalJobRunner: Records R/W=185676/681585 > reduce\n",
      "16/02/04 08:21:09 INFO mapreduce.Job:  map 100% reduce 82%\n",
      "16/02/04 08:21:12 INFO mapred.LocalJobRunner: Records R/W=185676/681585 > reduce\n",
      "16/02/04 08:21:13 INFO streaming.PipeMapRed: Records R/W=210419/800158\n",
      "16/02/04 08:21:15 INFO mapred.LocalJobRunner: Records R/W=210419/800158 > reduce\n",
      "16/02/04 08:21:15 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/02/04 08:21:18 INFO mapred.LocalJobRunner: Records R/W=210419/800158 > reduce\n",
      "16/02/04 08:21:21 INFO mapred.LocalJobRunner: Records R/W=210419/800158 > reduce\n",
      "16/02/04 08:21:23 INFO streaming.PipeMapRed: Records R/W=221447/844241\n",
      "16/02/04 08:21:24 INFO mapred.LocalJobRunner: Records R/W=221447/844241 > reduce\n",
      "16/02/04 08:21:24 INFO mapreduce.Job:  map 100% reduce 84%\n",
      "16/02/04 08:21:27 INFO mapred.LocalJobRunner: Records R/W=221447/844241 > reduce\n",
      "16/02/04 08:21:30 INFO mapred.LocalJobRunner: Records R/W=221447/844241 > reduce\n",
      "16/02/04 08:21:30 INFO mapreduce.Job:  map 100% reduce 85%\n",
      "16/02/04 08:21:33 INFO mapred.LocalJobRunner: Records R/W=221447/844241 > reduce\n",
      "16/02/04 08:21:33 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/02/04 08:21:35 INFO streaming.PipeMapRed: Records R/W=248945/984896\n",
      "16/02/04 08:21:36 INFO mapred.LocalJobRunner: Records R/W=248945/984896 > reduce\n",
      "16/02/04 08:21:39 INFO mapred.LocalJobRunner: Records R/W=248945/984896 > reduce\n",
      "16/02/04 08:21:39 INFO mapreduce.Job:  map 100% reduce 87%\n",
      "16/02/04 08:21:42 INFO mapred.LocalJobRunner: Records R/W=248945/984896 > reduce\n",
      "16/02/04 08:21:45 INFO mapred.LocalJobRunner: Records R/W=248945/984896 > reduce\n",
      "16/02/04 08:21:46 INFO streaming.PipeMapRed: Records R/W=269514/1087881\n",
      "16/02/04 08:21:48 INFO mapred.LocalJobRunner: Records R/W=269514/1087881 > reduce\n",
      "16/02/04 08:21:48 INFO mapreduce.Job:  map 100% reduce 88%\n",
      "16/02/04 08:21:51 INFO mapred.LocalJobRunner: Records R/W=269514/1087881 > reduce\n",
      "16/02/04 08:21:54 INFO mapred.LocalJobRunner: Records R/W=269514/1087881 > reduce\n",
      "16/02/04 08:21:54 INFO mapreduce.Job:  map 100% reduce 89%\n",
      "16/02/04 08:21:56 INFO streaming.PipeMapRed: Records R/W=286340/1164718\n",
      "16/02/04 08:21:57 INFO mapred.LocalJobRunner: Records R/W=286340/1164718 > reduce\n",
      "16/02/04 08:22:00 INFO mapred.LocalJobRunner: Records R/W=286340/1164718 > reduce\n",
      "16/02/04 08:22:00 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/04 08:22:03 INFO mapred.LocalJobRunner: Records R/W=286340/1164718 > reduce\n",
      "16/02/04 08:22:03 INFO streaming.PipeMapRed: R/W/S=300000/1226025/0 in:2097=300000/143 [rec/s] out:8573=1226025/143 [rec/s]\n",
      "16/02/04 08:22:06 INFO mapred.LocalJobRunner: Records R/W=286340/1164718 > reduce\n",
      "16/02/04 08:22:06 INFO mapreduce.Job:  map 100% reduce 91%\n",
      "16/02/04 08:22:07 INFO streaming.PipeMapRed: Records R/W=305643/1248068\n",
      "16/02/04 08:22:09 INFO mapred.LocalJobRunner: Records R/W=305643/1248068 > reduce\n",
      "16/02/04 08:22:12 INFO mapred.LocalJobRunner: Records R/W=305643/1248068 > reduce\n",
      "16/02/04 08:22:12 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/02/04 08:22:15 INFO mapred.LocalJobRunner: Records R/W=305643/1248068 > reduce\n",
      "16/02/04 08:22:18 INFO mapred.LocalJobRunner: Records R/W=305643/1248068 > reduce\n",
      "16/02/04 08:22:18 INFO streaming.PipeMapRed: Records R/W=323637/1321600\n",
      "16/02/04 08:22:21 INFO mapred.LocalJobRunner: Records R/W=323637/1321600 > reduce\n",
      "16/02/04 08:22:21 INFO mapreduce.Job:  map 100% reduce 93%\n",
      "16/02/04 08:22:24 INFO mapred.LocalJobRunner: Records R/W=323637/1321600 > reduce\n",
      "16/02/04 08:22:24 INFO mapreduce.Job:  map 100% reduce 94%\n",
      "16/02/04 08:22:27 INFO mapred.LocalJobRunner: Records R/W=323637/1321600 > reduce\n",
      "16/02/04 08:22:27 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/02/04 08:22:28 INFO streaming.PipeMapRed: Records R/W=353056/1478580\n",
      "16/02/04 08:22:30 INFO mapred.LocalJobRunner: Records R/W=353056/1478580 > reduce\n",
      "16/02/04 08:22:33 INFO mapred.LocalJobRunner: Records R/W=353056/1478580 > reduce\n",
      "16/02/04 08:22:33 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/02/04 08:22:36 INFO mapred.LocalJobRunner: Records R/W=353056/1478580 > reduce\n",
      "16/02/04 08:22:38 INFO streaming.PipeMapRed: Records R/W=375824/1581571\n",
      "16/02/04 08:22:39 INFO mapred.LocalJobRunner: Records R/W=375824/1581571 > reduce\n",
      "16/02/04 08:22:39 INFO mapreduce.Job:  map 100% reduce 97%\n",
      "16/02/04 08:22:42 INFO mapred.LocalJobRunner: Records R/W=375824/1581571 > reduce\n",
      "16/02/04 08:22:43 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/02/04 08:22:45 INFO mapred.LocalJobRunner: Records R/W=375824/1581571 > reduce\n",
      "16/02/04 08:22:48 INFO mapred.LocalJobRunner: Records R/W=375824/1581571 > reduce\n",
      "16/02/04 08:22:48 INFO streaming.PipeMapRed: Records R/W=396352/1682944\n",
      "16/02/04 08:22:49 INFO mapreduce.Job:  map 100% reduce 99%\n",
      "16/02/04 08:22:49 INFO streaming.PipeMapRed: R/W/S=400000/1704208/0 in:2116=400000/189 [rec/s] out:9016=1704208/189 [rec/s]\n",
      "16/02/04 08:22:51 INFO mapred.LocalJobRunner: Records R/W=396352/1682944 > reduce\n",
      "16/02/04 08:22:54 INFO mapred.LocalJobRunner: Records R/W=396352/1682944 > reduce\n",
      "16/02/04 08:22:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:22:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:22:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:22:57 INFO mapred.Task: Task:attempt_local112720032_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:22:57 INFO mapred.LocalJobRunner: Records R/W=396352/1682944 > reduce\n",
      "16/02/04 08:22:57 INFO mapred.Task: Task attempt_local112720032_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:22:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_local112720032_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_5i/_temporary/0/task_local112720032_0001_r_000000\n",
      "16/02/04 08:22:57 INFO mapred.LocalJobRunner: Records R/W=396352/1682944 > reduce\n",
      "16/02/04 08:22:57 INFO mapred.Task: Task 'attempt_local112720032_0001_r_000000_0' done.\n",
      "16/02/04 08:22:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local112720032_0001_r_000000_0\n",
      "16/02/04 08:22:57 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:22:58 INFO mapreduce.Job: Job job_local112720032_0001 completed successfully\n",
      "16/02/04 08:22:58 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=327849780\n",
      "\t\tFILE: Number of bytes written=410317543\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6917034\n",
      "\t\tHDFS: Number of bytes written=35163286\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=411925\n",
      "\t\tMap output bytes=80680946\n",
      "\t\tMap output materialized bytes=81909485\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12593\n",
      "\t\tReduce shuffle bytes=81909485\n",
      "\t\tReduce input records=411925\n",
      "\t\tReduce output records=1754191\n",
      "\t\tSpilled Records=1235775\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=30\n",
      "\t\tTotal committed heap usage (bytes)=602931200\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum mapper calls=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=35163286\n",
      "16/02/04 08:22:58 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_5i\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/ProductPurchaseData.txt \\\n",
    "-output /user/hw3/output_3_5i \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:22:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_5i/part-00000 > stripes_unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put stripes_unsorted /user/hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Running a second map-reduce job to merge same pairs **\n",
    "\n",
    "**For example if there are two keys**\n",
    "\n",
    "**A:{'B':2,'C':1}**\n",
    "\n",
    "**B:{'A':1,'C':1}**\n",
    "\n",
    "**The first map reduce job would output the following**\n",
    "\n",
    "**A,B 2**\n",
    "\n",
    "**A,B 1**\n",
    "\n",
    "**etc...**\n",
    "\n",
    "**The second map reduce job would merge these two pairs to give**\n",
    "\n",
    "**A,B 3**\n",
    "\n",
    "**And the third map reduce job is to sort the counts!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    print \"%s\\t%s\" %(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "prev_id = None\n",
    "count = 0\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    if((prev_id !=None) and (line[0] !=prev_id)):\n",
    "        print \"%s\\t%s\" %(prev_id,count)\n",
    "        count = 0\n",
    "    count += int(line[1])\n",
    "    prev_id = line[0]\n",
    "print \"%s\\t%s\" %(prev_id,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:23:07 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:23:07 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:23:07 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:23:08 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:23:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:23:08 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1940809252_0001\n",
      "16/02/04 08:23:08 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:23:08 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:23:08 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:23:08 INFO mapreduce.Job: Running job: job_local1940809252_0001\n",
      "16/02/04 08:23:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:08 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:23:08 INFO mapred.LocalJobRunner: Starting task: attempt_local1940809252_0001_m_000000_0\n",
      "16/02/04 08:23:08 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:08 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:23:08 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/stripes_unsorted:0+35163286\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:23:08 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:23:08 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:23:08 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: Records R/W=13090/1\n",
      "16/02/04 08:23:09 INFO mapreduce.Job: Job job_local1940809252_0001 running in uber mode : false\n",
      "16/02/04 08:23:09 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:23:09 INFO streaming.PipeMapRed: R/W/S=100000/90762/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:10 INFO streaming.PipeMapRed: R/W/S=200000/189675/0 in:200000=200000/1 [rec/s] out:189675=189675/1 [rec/s]\n",
      "16/02/04 08:23:10 INFO streaming.PipeMapRed: R/W/S=300000/287689/0 in:300000=300000/1 [rec/s] out:287689=287689/1 [rec/s]\n",
      "16/02/04 08:23:11 INFO streaming.PipeMapRed: R/W/S=400000/391449/0 in:200000=400000/2 [rec/s] out:195724=391449/2 [rec/s]\n",
      "16/02/04 08:23:11 INFO streaming.PipeMapRed: R/W/S=500000/489556/0 in:250000=500000/2 [rec/s] out:244801=489603/2 [rec/s]\n",
      "16/02/04 08:23:12 INFO streaming.PipeMapRed: R/W/S=600000/588409/0 in:200000=600000/3 [rec/s] out:196136=588409/3 [rec/s]\n",
      "16/02/04 08:23:12 INFO streaming.PipeMapRed: R/W/S=700000/692204/0 in:233333=700000/3 [rec/s] out:230734=692204/3 [rec/s]\n",
      "16/02/04 08:23:13 INFO streaming.PipeMapRed: R/W/S=800000/790332/0 in:200000=800000/4 [rec/s] out:197583=790332/4 [rec/s]\n",
      "16/02/04 08:23:13 INFO streaming.PipeMapRed: R/W/S=900000/889223/0 in:180000=900000/5 [rec/s] out:177844=889223/5 [rec/s]\n",
      "16/02/04 08:23:14 INFO streaming.PipeMapRed: R/W/S=1000000/986522/0 in:200000=1000000/5 [rec/s] out:197304=986522/5 [rec/s]\n",
      "16/02/04 08:23:14 INFO mapred.LocalJobRunner: Records R/W=13090/1 > map\n",
      "16/02/04 08:23:14 INFO streaming.PipeMapRed: R/W/S=1100000/1091577/0 in:183333=1100000/6 [rec/s] out:181931=1091586/6 [rec/s]\n",
      "16/02/04 08:23:15 INFO streaming.PipeMapRed: R/W/S=1200000/1190049/0 in:200000=1200000/6 [rec/s] out:198341=1190049/6 [rec/s]\n",
      "16/02/04 08:23:15 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "16/02/04 08:23:16 INFO streaming.PipeMapRed: R/W/S=1300000/1288112/0 in:185714=1300000/7 [rec/s] out:184016=1288112/7 [rec/s]\n",
      "16/02/04 08:23:16 INFO streaming.PipeMapRed: R/W/S=1400000/1392571/0 in:200000=1400000/7 [rec/s] out:198939=1392577/7 [rec/s]\n",
      "16/02/04 08:23:17 INFO streaming.PipeMapRed: R/W/S=1500000/1490858/0 in:187500=1500000/8 [rec/s] out:186357=1490858/8 [rec/s]\n",
      "16/02/04 08:23:17 INFO streaming.PipeMapRed: R/W/S=1600000/1588905/0 in:200000=1600000/8 [rec/s] out:198614=1588918/8 [rec/s]\n",
      "16/02/04 08:23:17 INFO mapred.LocalJobRunner: Records R/W=13090/1 > map\n",
      "16/02/04 08:23:18 INFO streaming.PipeMapRed: R/W/S=1700000/1686989/0 in:188888=1700000/9 [rec/s] out:187444=1686999/9 [rec/s]\n",
      "16/02/04 08:23:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:23:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:23:18 INFO mapred.LocalJobRunner: Records R/W=13090/1 > map\n",
      "16/02/04 08:23:18 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:23:18 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:23:18 INFO mapred.MapTask: bufstart = 0; bufend = 35163286; bufvoid = 104857600\n",
      "16/02/04 08:23:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 19197636(76790544); length = 7016761/6553600\n",
      "16/02/04 08:23:18 INFO mapreduce.Job:  map 62% reduce 0%\n",
      "16/02/04 08:23:20 INFO mapred.LocalJobRunner: Records R/W=13090/1 > sort\n",
      "16/02/04 08:23:21 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/02/04 08:23:22 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:23:23 INFO mapred.Task: Task:attempt_local1940809252_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: Records R/W=13090/1\n",
      "16/02/04 08:23:23 INFO mapred.Task: Task 'attempt_local1940809252_0001_m_000000_0' done.\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local1940809252_0001_m_000000_0\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: Starting task: attempt_local1940809252_0001_r_000000_0\n",
      "16/02/04 08:23:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:23 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:23:23 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:23:23 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b1666d4\n",
      "16/02/04 08:23:23 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:23:23 INFO reduce.EventFetcher: attempt_local1940809252_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:23:23 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1940809252_0001_m_000000_0 decomp: 38671670 len: 38671674 to MEMORY\n",
      "16/02/04 08:23:23 INFO reduce.InMemoryMapOutput: Read 38671670 bytes from map-output for attempt_local1940809252_0001_m_000000_0\n",
      "16/02/04 08:23:23 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 38671670, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->38671670\n",
      "16/02/04 08:23:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:23:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:23:23 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:23:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:23:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 38671659 bytes\n",
      "16/02/04 08:23:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:23:24 INFO reduce.MergeManagerImpl: Merged 1 segments, 38671670 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:23:24 INFO reduce.MergeManagerImpl: Merging 1 files, 38671674 bytes from disk\n",
      "16/02/04 08:23:24 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:23:24 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:23:24 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 38671659 bytes\n",
      "16/02/04 08:23:24 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:23:24 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:23:24 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: Records R/W=6546/1\n",
      "16/02/04 08:23:24 INFO streaming.PipeMapRed: R/W/S=10000/2448/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:25 INFO streaming.PipeMapRed: R/W/S=100000/46455/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:26 INFO streaming.PipeMapRed: R/W/S=200000/95380/0 in:200000=200000/1 [rec/s] out:95380=95380/1 [rec/s]\n",
      "16/02/04 08:23:26 INFO streaming.PipeMapRed: R/W/S=300000/145073/0 in:150000=300000/2 [rec/s] out:72536=145073/2 [rec/s]\n",
      "16/02/04 08:23:27 INFO streaming.PipeMapRed: R/W/S=400000/196547/0 in:200000=400000/2 [rec/s] out:98276=196552/2 [rec/s]\n",
      "16/02/04 08:23:27 INFO streaming.PipeMapRed: R/W/S=500000/246046/0 in:166666=500000/3 [rec/s] out:82015=246046/3 [rec/s]\n",
      "16/02/04 08:23:28 INFO streaming.PipeMapRed: R/W/S=600000/294871/0 in:200000=600000/3 [rec/s] out:98290=294871/3 [rec/s]\n",
      "16/02/04 08:23:28 INFO streaming.PipeMapRed: R/W/S=700000/347012/0 in:175000=700000/4 [rec/s] out:86753=347012/4 [rec/s]\n",
      "16/02/04 08:23:29 INFO mapred.LocalJobRunner: Records R/W=6546/1 > reduce\n",
      "16/02/04 08:23:29 INFO streaming.PipeMapRed: R/W/S=800000/396688/0 in:200000=800000/4 [rec/s] out:99172=396688/4 [rec/s]\n",
      "16/02/04 08:23:29 INFO mapreduce.Job:  map 100% reduce 81%\n",
      "16/02/04 08:23:30 INFO streaming.PipeMapRed: R/W/S=900000/445584/0 in:180000=900000/5 [rec/s] out:89116=445584/5 [rec/s]\n",
      "16/02/04 08:23:30 INFO streaming.PipeMapRed: R/W/S=1000000/494472/0 in:166666=1000000/6 [rec/s] out:82412=494472/6 [rec/s]\n",
      "16/02/04 08:23:31 INFO streaming.PipeMapRed: R/W/S=1100000/546619/0 in:183333=1100000/6 [rec/s] out:91103=546619/6 [rec/s]\n",
      "16/02/04 08:23:31 INFO streaming.PipeMapRed: R/W/S=1200000/595534/0 in:171428=1200000/7 [rec/s] out:85076=595534/7 [rec/s]\n",
      "16/02/04 08:23:32 INFO mapred.LocalJobRunner: Records R/W=6546/1 > reduce\n",
      "16/02/04 08:23:32 INFO streaming.PipeMapRed: R/W/S=1300000/645224/0 in:162500=1300000/8 [rec/s] out:80653=645224/8 [rec/s]\n",
      "16/02/04 08:23:32 INFO mapreduce.Job:  map 100% reduce 90%\n",
      "16/02/04 08:23:33 INFO streaming.PipeMapRed: R/W/S=1400000/697433/0 in:175000=1400000/8 [rec/s] out:87179=697433/8 [rec/s]\n",
      "16/02/04 08:23:33 INFO streaming.PipeMapRed: R/W/S=1500000/746305/0 in:166666=1500000/9 [rec/s] out:82922=746305/9 [rec/s]\n",
      "16/02/04 08:23:34 INFO streaming.PipeMapRed: Records R/W=1588834/789491\n",
      "16/02/04 08:23:34 INFO streaming.PipeMapRed: R/W/S=1600000/795203/0 in:160000=1600000/10 [rec/s] out:79520=795203/10 [rec/s]\n",
      "16/02/04 08:23:35 INFO mapred.LocalJobRunner: Records R/W=1588834/789491 > reduce\n",
      "16/02/04 08:23:35 INFO streaming.PipeMapRed: R/W/S=1700000/847339/0 in:170000=1700000/10 [rec/s] out:84733=847339/10 [rec/s]\n",
      "16/02/04 08:23:35 INFO mapreduce.Job:  map 100% reduce 98%\n",
      "16/02/04 08:23:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:23:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:23:35 INFO mapred.Task: Task:attempt_local1940809252_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:23:35 INFO mapred.LocalJobRunner: Records R/W=1588834/789491 > reduce\n",
      "16/02/04 08:23:35 INFO mapred.Task: Task attempt_local1940809252_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:23:35 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1940809252_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/stripes_unsorted_int/_temporary/0/task_local1940809252_0001_r_000000\n",
      "16/02/04 08:23:35 INFO mapred.LocalJobRunner: Records R/W=1588834/789491 > reduce\n",
      "16/02/04 08:23:35 INFO mapred.Task: Task 'attempt_local1940809252_0001_r_000000_0' done.\n",
      "16/02/04 08:23:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1940809252_0001_r_000000_0\n",
      "16/02/04 08:23:35 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:23:36 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:23:36 INFO mapreduce.Job: Job job_local1940809252_0001 completed successfully\n",
      "16/02/04 08:23:36 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=77555164\n",
      "\t\tFILE: Number of bytes written=116788132\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=70326572\n",
      "\t\tHDFS: Number of bytes written=17637494\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1754191\n",
      "\t\tMap output records=1754191\n",
      "\t\tMap output bytes=35163286\n",
      "\t\tMap output materialized bytes=38671674\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=38671674\n",
      "\t\tReduce input records=1754191\n",
      "\t\tReduce output records=877096\n",
      "\t\tSpilled Records=3508382\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=14\n",
      "\t\tTotal committed heap usage (bytes)=526909440\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum mapper calls=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=35163286\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17637494\n",
      "16/02/04 08:23:36 INFO streaming.StreamJob: Output directory: /user/hw3/stripes_unsorted_int\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/stripes_unsorted \\\n",
    "-output /user/hw3/stripes_unsorted_int \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/stripes_unsorted_int/part-00000 > stripes_unsorted_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:23:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hw3/stripes_unsorted_int\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f /user/hw3/stripes_unsorted_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put stripes_unsorted_int /user/hw3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third Map reduce job to perform the sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[\\t]',line.strip())\n",
    "    print \"%s,%s\" %(line[0],line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:group,Num mapper calls,1\\n\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r'[,]',line.strip())\n",
    "    if(line[0]==\"*\"):\n",
    "        total = int(line[2]) \n",
    "    else:\n",
    "        rel_freq = float(line[2])/float(total)\n",
    "        print \"%s,%s,%s,%s,%s\" %(line[0],line[1],line[2],rel_freq,total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:23:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/04 08:23:46 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/02/04 08:23:46 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/02/04 08:23:46 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/02/04 08:23:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/02/04 08:23:47 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/02/04 08:23:47 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/02/04 08:23:47 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/02/04 08:23:47 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/02/04 08:23:47 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/02/04 08:23:47 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1642583775_0001\n",
      "16/02/04 08:23:47 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/02/04 08:23:47 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/02/04 08:23:47 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/02/04 08:23:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:47 INFO mapreduce.Job: Running job: job_local1642583775_0001\n",
      "16/02/04 08:23:47 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/02/04 08:23:47 INFO mapred.LocalJobRunner: Starting task: attempt_local1642583775_0001_m_000000_0\n",
      "16/02/04 08:23:47 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:47 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:23:47 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:23:47 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/hw3/stripes_unsorted_int:0+17637494\n",
      "16/02/04 08:23:47 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/02/04 08:23:48 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./mapper.py]\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/02/04 08:23:48 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: Records R/W=6532/1\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=10000/5717/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO streaming.PipeMapRed: R/W/S=100000/92927/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:48 INFO mapreduce.Job: Job job_local1642583775_0001 running in uber mode : false\n",
      "16/02/04 08:23:48 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/02/04 08:23:49 INFO streaming.PipeMapRed: R/W/S=200000/191519/0 in:200000=200000/1 [rec/s] out:191519=191519/1 [rec/s]\n",
      "16/02/04 08:23:50 INFO streaming.PipeMapRed: R/W/S=300000/294871/0 in:300000=300000/1 [rec/s] out:294871=294871/1 [rec/s]\n",
      "16/02/04 08:23:50 INFO streaming.PipeMapRed: R/W/S=400000/392616/0 in:200000=400000/2 [rec/s] out:196308=392616/2 [rec/s]\n",
      "16/02/04 08:23:50 INFO streaming.PipeMapRed: R/W/S=500000/490389/0 in:250000=500000/2 [rec/s] out:245194=490389/2 [rec/s]\n",
      "16/02/04 08:23:51 INFO streaming.PipeMapRed: R/W/S=600000/594719/0 in:200000=600000/3 [rec/s] out:198239=594719/3 [rec/s]\n",
      "16/02/04 08:23:51 INFO streaming.PipeMapRed: R/W/S=700000/692918/0 in:233333=700000/3 [rec/s] out:230977=692933/3 [rec/s]\n",
      "16/02/04 08:23:52 INFO streaming.PipeMapRed: R/W/S=800000/790307/0 in:200000=800000/4 [rec/s] out:197576=790307/4 [rec/s]\n",
      "16/02/04 08:23:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:23:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:23:52 INFO mapred.LocalJobRunner: \n",
      "16/02/04 08:23:52 INFO mapred.MapTask: Starting flush of map output\n",
      "16/02/04 08:23:52 INFO mapred.MapTask: Spilling map output\n",
      "16/02/04 08:23:52 INFO mapred.MapTask: bufstart = 0; bufend = 18514590; bufvoid = 104857600\n",
      "16/02/04 08:23:52 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 22706016(90824064); length = 3508381/6553600\n",
      "16/02/04 08:23:53 INFO mapred.MapTask: Finished spill 0\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: Records R/W=6532/1 > sort\n",
      "16/02/04 08:23:53 INFO mapred.Task: Task:attempt_local1642583775_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: Records R/W=6532/1\n",
      "16/02/04 08:23:53 INFO mapred.Task: Task 'attempt_local1642583775_0001_m_000000_0' done.\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local1642583775_0001_m_000000_0\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/02/04 08:23:53 INFO mapred.LocalJobRunner: Starting task: attempt_local1642583775_0001_r_000000_0\n",
      "16/02/04 08:23:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/02/04 08:23:53 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/02/04 08:23:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/02/04 08:23:53 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5d3bb454\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/02/04 08:23:54 INFO reduce.EventFetcher: attempt_local1642583775_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/02/04 08:23:54 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1642583775_0001_m_000000_0 decomp: 20268784 len: 20268788 to MEMORY\n",
      "16/02/04 08:23:54 INFO reduce.InMemoryMapOutput: Read 20268784 bytes from map-output for attempt_local1642583775_0001_m_000000_0\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 20268784, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->20268784\n",
      "16/02/04 08:23:54 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/02/04 08:23:54 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/02/04 08:23:54 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:23:54 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20268767 bytes\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: Merged 1 segments, 20268784 bytes to disk to satisfy reduce memory limit\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: Merging 1 files, 20268788 bytes from disk\n",
      "16/02/04 08:23:54 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/02/04 08:23:54 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/02/04 08:23:54 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 20268767 bytes\n",
      "16/02/04 08:23:54 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/02/04 08:23:54 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw3/./reducer.py]\n",
      "16/02/04 08:23:54 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/02/04 08:23:54 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/02/04 08:23:54 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:54 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:54 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/02/04 08:23:54 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:54 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:55 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/02/04 08:23:55 INFO streaming.PipeMapRed: Records R/W=10234/1\n",
      "16/02/04 08:23:56 INFO streaming.PipeMapRed: R/W/S=100000/89304/0 in:100000=100000/1 [rec/s] out:89304=89304/1 [rec/s]\n",
      "16/02/04 08:23:57 INFO streaming.PipeMapRed: R/W/S=200000/188808/0 in:100000=200000/2 [rec/s] out:94406=188813/2 [rec/s]\n",
      "16/02/04 08:23:58 INFO streaming.PipeMapRed: R/W/S=300000/288412/0 in:100000=300000/3 [rec/s] out:96137=288412/3 [rec/s]\n",
      "16/02/04 08:23:58 INFO streaming.PipeMapRed: R/W/S=400000/388315/0 in:100000=400000/4 [rec/s] out:97078=388315/4 [rec/s]\n",
      "16/02/04 08:23:59 INFO streaming.PipeMapRed: R/W/S=500000/488480/0 in:100000=500000/5 [rec/s] out:97696=488480/5 [rec/s]\n",
      "16/02/04 08:23:59 INFO mapred.LocalJobRunner: Records R/W=10234/1 > reduce\n",
      "16/02/04 08:24:00 INFO streaming.PipeMapRed: R/W/S=600000/588274/0 in:120000=600000/5 [rec/s] out:117654=588274/5 [rec/s]\n",
      "16/02/04 08:24:00 INFO mapreduce.Job:  map 100% reduce 86%\n",
      "16/02/04 08:24:01 INFO streaming.PipeMapRed: R/W/S=700000/688067/0 in:100000=700000/7 [rec/s] out:98295=688067/7 [rec/s]\n",
      "16/02/04 08:24:02 INFO mapred.LocalJobRunner: Records R/W=10234/1 > reduce\n",
      "16/02/04 08:24:03 INFO streaming.PipeMapRed: R/W/S=800000/787861/0 in:100000=800000/8 [rec/s] out:98482=787861/8 [rec/s]\n",
      "16/02/04 08:24:03 INFO mapreduce.Job:  map 100% reduce 96%\n",
      "16/02/04 08:24:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/02/04 08:24:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/02/04 08:24:04 INFO mapred.Task: Task:attempt_local1642583775_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/02/04 08:24:04 INFO mapred.LocalJobRunner: Records R/W=10234/1 > reduce\n",
      "16/02/04 08:24:04 INFO mapred.Task: Task attempt_local1642583775_0001_r_000000_0 is allowed to commit now\n",
      "16/02/04 08:24:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1642583775_0001_r_000000_0' to hdfs://localhost:9000/user/hw3/output_3_5_o/_temporary/0/task_local1642583775_0001_r_000000\n",
      "16/02/04 08:24:04 INFO mapred.LocalJobRunner: Records R/W=10234/1 > reduce\n",
      "16/02/04 08:24:04 INFO mapred.Task: Task 'attempt_local1642583775_0001_r_000000_0' done.\n",
      "16/02/04 08:24:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local1642583775_0001_r_000000_0\n",
      "16/02/04 08:24:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/02/04 08:24:04 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/02/04 08:24:04 INFO mapreduce.Job: Job job_local1642583775_0001 completed successfully\n",
      "16/02/04 08:24:04 INFO mapreduce.Job: Counters: 36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=40749400\n",
      "\t\tFILE: Number of bytes written=61583834\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=35274988\n",
      "\t\tHDFS: Number of bytes written=39406387\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=877096\n",
      "\t\tMap output records=877096\n",
      "\t\tMap output bytes=18514590\n",
      "\t\tMap output materialized bytes=20268788\n",
      "\t\tInput split bytes=103\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=877096\n",
      "\t\tReduce shuffle bytes=20268788\n",
      "\t\tReduce input records=877096\n",
      "\t\tReduce output records=877095\n",
      "\t\tSpilled Records=1754192\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=20\n",
      "\t\tTotal committed heap usage (bytes)=574619648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tgroup\n",
      "\t\tNum mapper calls=2\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=17637494\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=39406387\n",
      "16/02/04 08:24:04 INFO streaming.StreamJob: Output directory: /user/hw3/output_3_5_o\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.comparator.options=-k3,3nr \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-input /user/hw3/stripes_unsorted_int \\\n",
    "-output /user/hw3/output_3_5_o \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Computational Setup : **\n",
    "    \n",
    "    MacBook Air\n",
    "    Processor : 1.8 Ghz Intel Core i5 , 2 Cores\n",
    "    Memory : 4 GB 1600 Mhz DDR3\n",
    "    Disk : SSD (Flash Storage) 128 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The output is shown below*\n",
    "\n",
    "*Format = Pairs, Support Count, Support, Total number of baskets *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:24:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "DAI62779,ELE17451,3184,0.102376129385,31101\t\n",
      "FRO40251,SNA80324,2824,0.0908009388766,31101\t\n",
      "DAI75645,FRO40251,2508,0.0806404938748,31101\t\n",
      "FRO40251,GRO85051,2426,0.0780039227035,31101\t\n",
      "DAI62779,GRO73461,2278,0.0732452332722,31101\t\n",
      "DAI75645,SNA80324,2260,0.0726664737468,31101\t\n",
      "DAI62779,FRO40251,2140,0.0688080769107,31101\t\n",
      "DAI62779,SNA80324,1846,0.0593550046622,31101\t\n",
      "DAI62779,DAI85309,1836,0.0590334715926,31101\t\n",
      "ELE32164,GRO59710,1822,0.058583325295,31101\t\n",
      "DAI62779,DAI75645,1764,0.0567184334909,31101\t\n",
      "FRO40251,GRO73461,1764,0.0567184334909,31101\t\n",
      "DAI62779,ELE92920,1754,0.0563969004212,31101\t\n",
      "FRO40251,FRO92469,1670,0.0536960226359,31101\t\n",
      "DAI62779,ELE32164,1664,0.0535031027941,31101\t\n",
      "DAI75645,GRO73461,1424,0.0457863091219,31101\t\n",
      "DAI43223,ELE32164,1422,0.045722002508,31101\t\n",
      "DAI62779,GRO30386,1418,0.0455933892801,31101\t\n",
      "ELE17451,FRO40251,1394,0.0448217099129,31101\t\n",
      "DAI85309,ELE99737,1318,0.0423780585833,31101\t\n",
      "DAI62779,ELE26917,1300,0.0417992990579,31101\t\n",
      "GRO21487,GRO73461,1262,0.0405774733931,31101\t\n",
      "DAI62779,SNA45677,1208,0.0388411948169,31101\t\n",
      "ELE17451,SNA80324,1194,0.0383910485193,31101\t\n",
      "DAI62779,GRO71621,1190,0.0382624352915,31101\t\n",
      "DAI62779,SNA55762,1186,0.0381338220636,31101\t\n",
      "DAI62779,DAI83733,1172,0.0376836757661,31101\t\n",
      "ELE17451,GRO73461,1160,0.0372978360824,31101\t\n",
      "GRO73461,SNA80324,1124,0.0361403170316,31101\t\n",
      "DAI62779,GRO59710,1122,0.0360760104177,31101\t\n",
      "DAI62779,FRO80039,1100,0.0353686376644,31101\t\n",
      "DAI75645,ELE17451,1094,0.0351757178226,31101\t\n",
      "DAI62779,SNA93860,1074,0.0345326516832,31101\t\n",
      "DAI55148,DAI62779,1052,0.0338252789299,31101\t\n",
      "DAI43223,GRO59710,1024,0.0329249863348,31101\t\n",
      "ELE17451,ELE32164,1022,0.0328606797209,31101\t\n",
      "DAI62779,SNA18336,1012,0.0325391466512,31101\t\n",
      "ELE32164,GRO73461,972,0.0312530143725,31101\t\n",
      "DAI85309,ELE17451,964,0.0309957879168,31101\t\n",
      "DAI62779,FRO78087,964,0.0309957879168,31101\t\n",
      "DAI62779,GRO94758,958,0.030802868075,31101\t\n",
      "GRO85051,SNA80324,942,0.0302884151635,31101\t\n",
      "DAI62779,GRO21487,942,0.0302884151635,31101\t\n",
      "ELE17451,GRO30386,936,0.0300954953217,31101\t\n",
      "FRO85978,SNA95666,926,0.029773962252,31101\t\n",
      "DAI62779,FRO19221,924,0.0297096556381,31101\t\n",
      "DAI62779,GRO46854,922,0.0296453490241,31101\t\n",
      "DAI43223,DAI62779,918,0.0295167357963,31101\t\n",
      "ELE92920,SNA18336,910,0.0292595093405,31101\t\n",
      "DAI88079,FRO40251,892,0.0286807498151,31101\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/hw3/output_3_5_o/part-00000 | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/04 08:24:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/02/04 08:24:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
