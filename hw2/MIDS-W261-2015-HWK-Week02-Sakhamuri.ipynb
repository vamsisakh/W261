{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name : Vamsi Sakhamuri\n",
    "## E-mail : vamsi@ischool.berkeley.edu\n",
    "## Class Name : W261-3\n",
    "## Week Number : 2\n",
    "## Date of submission : 01/26/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*HW2.0.  \n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A race condition , in the context of parallel computation, can mean that the results of our computation will not be deterministic. It can occur when different threads attempt to modify a shared resource.\n",
    "\n",
    "    For example, if two threads want to multiply a variable by 2 and assign the result back to the variable.\n",
    "\n",
    "    So , if thread A computes X = X * 2\n",
    "\n",
    "    and thread B also computes X = X * 2\n",
    "\n",
    "    Assuming the initial value of X is 1, then there are two results possible.\n",
    "\n",
    "    1st Possibility : Both thread A and thread B read the previous value of X. In this case, the new value of X will be 2 after both the threads finish their computation.\n",
    "\n",
    "    2nd Possibility : Thread A reads the previous value of X and writes back the result to X (so X=2). Thread B reads this updated value of X. And so the new value of X will be 4 after both threads finish their computation.\n",
    "\n",
    "    Since the result is not deterministic (2 vs 4), there is a race condition. This can be eliminated by the use of mutex's where the threads need to acquire a lock on the shared resource before proceeding their respective computation.\n",
    "    \n",
    "\n",
    "* Map reduce is a programming model which provides an abstraction to the user by hiding away the system level details from the programmer. The programmer does not have to worry about setting up barriers or worry about race conditions or deadlocks and can solely focus on the job of writing the mappers,reducers,combiners and partitioners.\n",
    "\n",
    "\n",
    "* Hadoop is a framework whereas mapreduce is a programming model. Hadoop allows for the distributed processing of large-scale datasets over a cluster of commodity servers via a programming model, where the programming model is map-reduce.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple map-reduce program using hadoop which does a word count is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word = re.split(r'[\\s+]',line) \n",
    "    for w in word:\n",
    "        print w,1   #emit word,1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "\n",
    "prev_word = None\n",
    "count = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    w_t = re.split(r'[\\s+]',line)\n",
    "    if(prev_word !=None):\n",
    "        if(prev_word !=w_t[0]):\n",
    "            print prev_word,count \n",
    "            count = 0\n",
    "    count = count+1\n",
    "    prev_word = w_t[0]\n",
    "print prev_word,count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo \"hello hi hey hello hi hi hello hi hi hi hey\" > simple_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 1615. Stop it first.\n",
      "localhost: datanode running as process 1714. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 1353. Stop it first.\n",
      "16/01/30 03:45:40 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:45:43 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/vamsi/hw2\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -rm -r -f  /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -mkdir -p /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put simple_text.txt /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:45:49 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:45:49 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:45:49 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:45:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:45:50 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:45:50 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:45:50 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1399621981_0001\n",
      "16/01/30 03:45:50 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:45:50 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:45:50 INFO mapreduce.Job: Running job: job_local1399621981_0001\n",
      "16/01/30 03:45:50 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:45:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:45:50 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:45:50 INFO mapred.LocalJobRunner: Starting task: attempt_local1399621981_0001_m_000000_0\n",
      "16/01/30 03:45:50 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:45:50 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:45:50 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/simple_text.txt:0+44\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:45:50 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:45:50 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:45:51 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:45:51 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:45:51 INFO mapred.MapTask: bufstart = 0; bufend = 81; bufvoid = 104857600\n",
      "16/01/30 03:45:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214352(104857408); length = 45/6553600\n",
      "16/01/30 03:45:51 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:45:51 INFO mapred.Task: Task:attempt_local1399621981_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/01/30 03:45:51 INFO mapred.Task: Task 'attempt_local1399621981_0001_m_000000_0' done.\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local1399621981_0001_m_000000_0\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1399621981_0001_r_000000_0\n",
      "16/01/30 03:45:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:45:51 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:45:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:45:51 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72e75028\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:45:51 INFO reduce.EventFetcher: attempt_local1399621981_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:45:51 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1399621981_0001_m_000000_0 decomp: 107 len: 111 to MEMORY\n",
      "16/01/30 03:45:51 INFO reduce.InMemoryMapOutput: Read 107 bytes from map-output for attempt_local1399621981_0001_m_000000_0\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 107, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->107\n",
      "16/01/30 03:45:51 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:45:51 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:45:51 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 102 bytes\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: Merged 1 segments, 107 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: Merging 1 files, 111 bytes from disk\n",
      "16/01/30 03:45:51 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:45:51 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:45:51 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 102 bytes\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:45:51 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: Records R/W=12/1\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:45:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:45:51 INFO mapred.Task: Task:attempt_local1399621981_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:45:51 INFO mapred.Task: Task attempt_local1399621981_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:45:51 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1399621981_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_0/_temporary/0/task_local1399621981_0001_r_000000\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Records R/W=12/1 > reduce\n",
      "16/01/30 03:45:51 INFO mapred.Task: Task 'attempt_local1399621981_0001_r_000000_0' done.\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local1399621981_0001_r_000000_0\n",
      "16/01/30 03:45:51 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:45:51 INFO mapreduce.Job: Job job_local1399621981_0001 running in uber mode : false\n",
      "16/01/30 03:45:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:45:51 INFO mapreduce.Job: Job job_local1399621981_0001 completed successfully\n",
      "16/01/30 03:45:51 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212040\n",
      "\t\tFILE: Number of bytes written=773449\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=88\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=12\n",
      "\t\tMap output bytes=81\n",
      "\t\tMap output materialized bytes=111\n",
      "\t\tInput split bytes=104\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=111\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=24\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=568328192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=44\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/30 03:45:51 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_0\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/simple_text.txt \\\n",
    "-output /user/vamsi/hw2/output_2_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      " 1\t\n",
      "hello 3\t\n",
      "hey 2\t\n",
      "hi 6\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_0/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.1. Sort in Hadoop MapReduce*\n",
    "Given as input: Records of the form <integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generating a input file with 10000 random integers between 0 and 1,000,000\n",
    "#each line is of the form <integer,\"NA\">\n",
    "\n",
    "from random import randint\n",
    "\n",
    "N = 10000\n",
    "\n",
    "with open('numbers.txt', 'w+') as f:\n",
    "    for i in range(N):\n",
    "        x = \"<\" + str(randint(0,1000000)) + \",\" + \"\\\"NA\\\">\\n\"    #pick random numbers between 0 and 1000,000\n",
    "        f.write(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove the leading '<'\n",
    "    line = line.lstrip('<')\n",
    "    \n",
    "    # remove the ending ,\"NA\">\n",
    "    line = re.sub(',\"NA\">$',\"\",line)\n",
    "    line = line.strip()\n",
    "    print int(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    print \"<\" + line + \",\" + \"\\\"NA\\\">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:45:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put numbers.txt /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:46:02 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:46:02 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:46:02 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:46:02 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:46:02 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:46:02 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/30 03:46:02 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:46:02 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:46:02 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local600626759_0001\n",
      "16/01/30 03:46:03 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:46:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:03 INFO mapreduce.Job: Running job: job_local600626759_0001\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Starting task: attempt_local600626759_0001_m_000000_0\n",
      "16/01/30 03:46:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/numbers.txt:0+138923\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:46:03 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: Records R/W=9434/1\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: R/W/S=10000/746/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:03 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:46:03 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: bufstart = 0; bufend = 78923; bufvoid = 104857600\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/30 03:46:03 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:46:03 INFO mapred.Task: Task:attempt_local600626759_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Records R/W=9434/1\n",
      "16/01/30 03:46:03 INFO mapred.Task: Task 'attempt_local600626759_0001_m_000000_0' done.\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Finishing task: attempt_local600626759_0001_m_000000_0\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:46:03 INFO mapred.LocalJobRunner: Starting task: attempt_local600626759_0001_r_000000_0\n",
      "16/01/30 03:46:03 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:03 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:03 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:03 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7a5173dd\n",
      "16/01/30 03:46:03 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:46:03 INFO reduce.EventFetcher: attempt_local600626759_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:46:04 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local600626759_0001_m_000000_0 decomp: 98925 len: 98929 to MEMORY\n",
      "16/01/30 03:46:04 INFO reduce.InMemoryMapOutput: Read 98925 bytes from map-output for attempt_local600626759_0001_m_000000_0\n",
      "16/01/30 03:46:04 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98925, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98925\n",
      "16/01/30 03:46:04 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:04 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:46:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98916 bytes\n",
      "16/01/30 03:46:04 INFO reduce.MergeManagerImpl: Merged 1 segments, 98925 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:46:04 INFO reduce.MergeManagerImpl: Merging 1 files, 98929 bytes from disk\n",
      "16/01/30 03:46:04 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:46:04 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:04 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98916 bytes\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:46:04 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:46:04 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:46:04 INFO mapreduce.Job: Job job_local600626759_0001 running in uber mode : false\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:04 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:04 INFO mapred.Task: Task:attempt_local600626759_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:04 INFO mapred.Task: Task attempt_local600626759_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:46:04 INFO output.FileOutputCommitter: Saved output of task 'attempt_local600626759_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_1/_temporary/0/task_local600626759_0001_r_000000\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/30 03:46:04 INFO mapred.Task: Task 'attempt_local600626759_0001_r_000000_0' done.\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: Finishing task: attempt_local600626759_0001_r_000000_0\n",
      "16/01/30 03:46:04 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:46:05 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:46:05 INFO mapreduce.Job: Job job_local600626759_0001 completed successfully\n",
      "16/01/30 03:46:05 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=409674\n",
      "\t\tFILE: Number of bytes written=1068931\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=277846\n",
      "\t\tHDFS: Number of bytes written=148923\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=78923\n",
      "\t\tMap output materialized bytes=98929\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=9947\n",
      "\t\tReduce shuffle bytes=98929\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=567279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=138923\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=148923\n",
      "16/01/30 03:46:05 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_1\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-D mapred.text.key.comparator.options=-nr \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/numbers.txt \\\n",
    "-output /user/vamsi/hw2/output_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "<999987,\"NA\">\t\n",
      "<999717,\"NA\">\t\n",
      "<999664,\"NA\">\t\n",
      "<999618,\"NA\">\t\n",
      "<999608,\"NA\">\t\n",
      "<999586,\"NA\">\t\n",
      "<999575,\"NA\">\t\n",
      "<999536,\"NA\">\t\n",
      "<999408,\"NA\">\t\n",
      "<999397,\"NA\">\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_1/part-00000 | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "<1026,\"NA\">\t\n",
      "<830,\"NA\">\t\n",
      "<704,\"NA\">\t\n",
      "<657,\"NA\">\t\n",
      "<512,\"NA\">\t\n",
      "<350,\"NA\">\t\n",
      "<272,\"NA\">\t\n",
      "<264,\"NA\">\t\n",
      "<192,\"NA\">\t\n",
      "<91,\"NA\">\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_1/part-00000 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If there are multiple reducers, yes we would need additional steps. The output from these multiple reducers would \n",
    "need to be passed into another map-reduce task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2.  WORDCOUNT\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = []  #empty list for words\n",
    "    email   = re.split('\\t+',line)\n",
    "    if(len(email)==4):\n",
    "        subject = re.split(r'[\\s.,]+',email[2].strip())\n",
    "        body    = re.split(r'[\\s.,]+',email[3].strip())\n",
    "        for s in subject:\n",
    "            words.append(s)    #appending list of words occuring in the subject\n",
    "        for b in body:\n",
    "            words.append(b)       #appending list of words occuring in the body\n",
    "        for word in words:\n",
    "                if(re.search('\\w+',word)):\n",
    "                    print \"%s,1\" %word        #emit word,1 to the reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "\n",
    "token_prev = None\n",
    "\n",
    "token_count = 0\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    linea = re.split(r',',line)\n",
    "    \n",
    "    if(token_prev!=None):\n",
    "        if(token_prev != linea[0]):\n",
    "            print \"%s,%d\" %(token_prev,token_count)\n",
    "            token_count = 0\n",
    "    \n",
    "    token_count += int(linea[1])\n",
    "    token_prev = linea[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put enronemail_1h.txt /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:46:14 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:46:14 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:46:14 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:46:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:46:15 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:46:15 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:46:15 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:46:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1188899611_0001\n",
      "16/01/30 03:46:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:46:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:46:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:46:15 INFO mapreduce.Job: Running job: job_local1188899611_0001\n",
      "16/01/30 03:46:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:15 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:46:15 INFO mapred.LocalJobRunner: Starting task: attempt_local1188899611_0001_m_000000_0\n",
      "16/01/30 03:46:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/enronemail_1h.txt:0+203981\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: Records R/W=101/1\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:46:16 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: bufstart = 0; bufend = 275549; bufvoid = 104857600\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26092040(104368160); length = 122357/6553600\n",
      "16/01/30 03:46:16 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:46:16 INFO mapred.Task: Task:attempt_local1188899611_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: Records R/W=101/1\n",
      "16/01/30 03:46:16 INFO mapred.Task: Task 'attempt_local1188899611_0001_m_000000_0' done.\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: Finishing task: attempt_local1188899611_0001_m_000000_0\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: Starting task: attempt_local1188899611_0001_r_000000_0\n",
      "16/01/30 03:46:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:16 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@38399568\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:46:16 INFO reduce.EventFetcher: attempt_local1188899611_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:46:16 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1188899611_0001_m_000000_0 decomp: 336731 len: 336735 to MEMORY\n",
      "16/01/30 03:46:16 INFO reduce.InMemoryMapOutput: Read 336731 bytes from map-output for attempt_local1188899611_0001_m_000000_0\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 336731, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->336731\n",
      "16/01/30 03:46:16 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:46:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 336720 bytes\n",
      "16/01/30 03:46:16 INFO mapreduce.Job: Job job_local1188899611_0001 running in uber mode : false\n",
      "16/01/30 03:46:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: Merged 1 segments, 336731 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: Merging 1 files, 336735 bytes from disk\n",
      "16/01/30 03:46:16 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:46:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 336720 bytes\n",
      "16/01/30 03:46:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:46:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:17 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:17 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:17 INFO streaming.PipeMapRed: Records R/W=18181/1\n",
      "16/01/30 03:46:17 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:17 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:17 INFO mapred.Task: Task:attempt_local1188899611_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:17 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:17 INFO mapred.Task: Task attempt_local1188899611_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:46:17 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1188899611_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_2/_temporary/0/task_local1188899611_0001_r_000000\n",
      "16/01/30 03:46:17 INFO mapred.LocalJobRunner: Records R/W=18181/1 > reduce\n",
      "16/01/30 03:46:17 INFO mapred.Task: Task 'attempt_local1188899611_0001_r_000000_0' done.\n",
      "16/01/30 03:46:17 INFO mapred.LocalJobRunner: Finishing task: attempt_local1188899611_0001_r_000000_0\n",
      "16/01/30 03:46:17 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:46:17 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:46:17 INFO mapreduce.Job: Job job_local1188899611_0001 completed successfully\n",
      "16/01/30 03:46:17 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=885298\n",
      "\t\tFILE: Number of bytes written=1784543\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=69393\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=30590\n",
      "\t\tMap output bytes=275549\n",
      "\t\tMap output materialized bytes=336735\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6236\n",
      "\t\tReduce shuffle bytes=336735\n",
      "\t\tReduce input records=30590\n",
      "\t\tReduce output records=6235\n",
      "\t\tSpilled Records=61180\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=567279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=69393\n",
      "16/01/30 03:46:17 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_2\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/enronemail_1h.txt \\\n",
    "-output /user/vamsi/hw2/output_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "assistance,9\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_2/part-00000 | grep \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_2/part-00000 > word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.2.1  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = re.split(',',line)\n",
    "    print \"%s,%s\" %(line[0].strip(),line[1].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = re.split(',',line)\n",
    "    print \"%s,%s\" %(line[0].strip(),line[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -put word_counts /user/vamsi/hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:46:27 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:46:27 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:46:27 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:46:28 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:46:28 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:46:28 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "16/01/30 03:46:28 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/30 03:46:28 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:46:28 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:46:28 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local136905951_0001\n",
      "16/01/30 03:46:28 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:46:28 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:46:28 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:46:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:28 INFO mapreduce.Job: Running job: job_local136905951_0001\n",
      "16/01/30 03:46:28 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:46:28 INFO mapred.LocalJobRunner: Starting task: attempt_local136905951_0001_m_000000_0\n",
      "16/01/30 03:46:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/word_counts:0+69393\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: Records R/W=6235/1\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:46:29 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: bufstart = 0; bufend = 69393; bufvoid = 104857600\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26189460(104757840); length = 24937/6553600\n",
      "16/01/30 03:46:29 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:46:29 INFO mapred.Task: Task:attempt_local136905951_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: Records R/W=6235/1\n",
      "16/01/30 03:46:29 INFO mapred.Task: Task 'attempt_local136905951_0001_m_000000_0' done.\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local136905951_0001_m_000000_0\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: Starting task: attempt_local136905951_0001_r_000000_0\n",
      "16/01/30 03:46:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:29 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1fca353e\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:46:29 INFO reduce.EventFetcher: attempt_local136905951_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:46:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local136905951_0001_m_000000_0 decomp: 81865 len: 81869 to MEMORY\n",
      "16/01/30 03:46:29 INFO reduce.InMemoryMapOutput: Read 81865 bytes from map-output for attempt_local136905951_0001_m_000000_0\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 81865, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->81865\n",
      "16/01/30 03:46:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:46:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81854 bytes\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 81865 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: Merging 1 files, 81869 bytes from disk\n",
      "16/01/30 03:46:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:46:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 81854 bytes\n",
      "16/01/30 03:46:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:46:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: Records R/W=6235/1\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:29 INFO mapreduce.Job: Job job_local136905951_0001 running in uber mode : false\n",
      "16/01/30 03:46:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:46:30 INFO mapred.Task: Task:attempt_local136905951_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:30 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:30 INFO mapred.Task: Task attempt_local136905951_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:46:30 INFO output.FileOutputCommitter: Saved output of task 'attempt_local136905951_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_2_1/_temporary/0/task_local136905951_0001_r_000000\n",
      "16/01/30 03:46:30 INFO mapred.LocalJobRunner: Records R/W=6235/1 > reduce\n",
      "16/01/30 03:46:30 INFO mapred.Task: Task 'attempt_local136905951_0001_r_000000_0' done.\n",
      "16/01/30 03:46:30 INFO mapred.LocalJobRunner: Finishing task: attempt_local136905951_0001_r_000000_0\n",
      "16/01/30 03:46:30 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:46:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:46:30 INFO mapreduce.Job: Job job_local136905951_0001 completed successfully\n",
      "16/01/30 03:46:30 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=375554\n",
      "\t\tFILE: Number of bytes written=1020053\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=138786\n",
      "\t\tHDFS: Number of bytes written=69393\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=6235\n",
      "\t\tMap output records=6235\n",
      "\t\tMap output bytes=69393\n",
      "\t\tMap output materialized bytes=81869\n",
      "\t\tInput split bytes=100\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=6235\n",
      "\t\tReduce shuffle bytes=81869\n",
      "\t\tReduce input records=6235\n",
      "\t\tReduce output records=6235\n",
      "\t\tSpilled Records=12470\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=567279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=69393\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=69393\n",
      "16/01/30 03:46:30 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_2_1\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D stream.map.output.field.separator=, \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D map.output.key.field.separator=, \\\n",
    "-D mapred.text.key.comparator.options=-k2,2nr \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/word_counts \\\n",
    "-output /user/vamsi/hw2/output_2_2_1 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "the,1201\t\n",
      "to,894\t\n",
      "and,620\t\n",
      "of,535\t\n",
      "a,516\t\n",
      "in,402\t\n",
      "you,401\t\n",
      "your,379\t\n",
      "for,354\t\n",
      "on,251\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_2_1/part-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM\n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report.\n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see.\n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "   \n",
    "   Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#filename = sys.argv[1]\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = []  #empty list for words\n",
    "    email   = re.split('\\t+',line)\n",
    "    if(len(email)==4):\n",
    "        subject = re.split(r'[\\s.,]+',email[2].strip())\n",
    "        body    = re.split(r'[\\s.,]+',email[3].strip())\n",
    "        for s in subject:\n",
    "            words.append(s)    #appending list of words occuring in the subject\n",
    "        for b in body:\n",
    "            words.append(b)       #appending list of words occuring in the body\n",
    "        for word in words:\n",
    "                if(re.search('\\w+',word)):\n",
    "                    print \"%s,%s,%s\" %(email[0].strip(),email[1].strip(),word.strip())  \n",
    "                                    #emit email_ID,output_label,word to the reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "email = {}\n",
    "words = {}\n",
    "spam_ec = 0\n",
    "ham_ec = 0\n",
    "total_ec = 0\n",
    "spam_wc = 0\n",
    "ham_wc = 0\n",
    "total_spam_wc = 0\n",
    "total_ham_wc = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r',',line)\n",
    "    email_id = line[0].strip()\n",
    "    spam = line[1].strip()\n",
    "    word = line[2].strip()\n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count':0,'ham_count':0}    \n",
    "    \n",
    "    if email_id not in email.keys():\n",
    "        email[email_id] = {'spam':0,'words':[],'count':0}\n",
    "    \n",
    "    if(int(spam)==1):\n",
    "        words[word]['spam_count'] += 1\n",
    "        total_spam_wc +=1\n",
    "    elif(int(spam)==0):\n",
    "        words[word]['ham_count'] += 1\n",
    "        total_ham_wc +=1\n",
    "    email[email_id]['count'] += 1\n",
    "    email[email_id]['spam'] = spam\n",
    "    email[email_id]['words'].append(word) \n",
    "\n",
    "#Computing priors\n",
    "\n",
    "#P_prior_spam = Number of emails containing spam/total number of emails\n",
    "#P_prior_ham = Number of emails containing ham/total number of emails\n",
    "for e in email.keys():\n",
    "    spam_ec += int(email[e]['spam'])\n",
    "    total_ec += 1 \n",
    "\n",
    "P_prior_spam = float(spam_ec)/float(total_ec)\n",
    "P_prior_ham = 1 - P_prior_spam\n",
    "\n",
    "#Computing conditionals\n",
    "#P(word|spam) and P(word|ham)\n",
    "\n",
    "cond_probs = {}\n",
    "\n",
    "for w in words.keys():\n",
    "    wc_spam = words[w]['spam_count']\n",
    "    wc_ham = words[w]['ham_count']\n",
    "    p_w_spam = float(wc_spam)/(total_spam_wc)    #conditional probability of word given spam\n",
    "    p_w_ham = float(wc_ham)/(total_ham_wc)       #conditional probability of word given ham\n",
    "    cond_probs[w] = {'spam':p_w_spam,'ham':p_w_ham}\n",
    "\n",
    "#Now, onto predictions\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for e in email.keys():\n",
    "    p_spam_cond = 0\n",
    "    p_ham_cond = 0\n",
    "    for word in email[e]['words']:\n",
    "        if(cond_probs[word]['spam'] !=float(0) and cond_probs[word]['ham'] !=float(0)):\n",
    "            p_spam_cond += log(cond_probs[word]['spam'])         \n",
    "            p_ham_cond += log(cond_probs[word]['ham'])\n",
    "            \n",
    "    p_spam_given_word = log(P_prior_spam) + p_spam_cond\n",
    "    p_ham_given_word = log(P_prior_ham) + p_ham_cond\n",
    "    \n",
    "    if(p_spam_given_word > p_ham_given_word):\n",
    "        predict_spam = 1\n",
    "    else:\n",
    "        predict_spam = 0\n",
    "    \n",
    "    prediction.append([predict_spam,email[e]['spam'],e,p_spam_given_word,p_ham_given_word])\n",
    "\n",
    "correct =0\n",
    "total =0\n",
    "\n",
    "for p in prediction:\n",
    "    if(p[0] == int(p[1])):\n",
    "        correct += 1 \n",
    "    total +=1\n",
    "\n",
    "accuracy = 100*(float(correct)/total)\n",
    "    \n",
    "print \"The accuracy of the naive bayes classifier is %s\" %accuracy\n",
    "\n",
    "for p in prediction:\n",
    "    print p[0],p[1],p[2],p[3],p[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:46:36 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:46:36 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:46:36 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:46:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:46:37 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:46:37 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local43858998_0001\n",
      "16/01/30 03:46:37 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:46:37 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:46:37 INFO mapreduce.Job: Running job: job_local43858998_0001\n",
      "16/01/30 03:46:37 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:46:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:37 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:46:37 INFO mapred.LocalJobRunner: Starting task: attempt_local43858998_0001_m_000000_0\n",
      "16/01/30 03:46:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/enronemail_1h.txt:0+203981\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:46:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:46:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:46:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:46:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=100/7029/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:46:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:46:38 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:46:38 INFO mapred.MapTask: bufstart = 0; bufend = 999987; bufvoid = 104857600\n",
      "16/01/30 03:46:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26092040(104368160); length = 122357/6553600\n",
      "16/01/30 03:46:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:46:38 INFO mapred.Task: Task:attempt_local43858998_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:38 INFO mapreduce.Job: Job job_local43858998_0001 running in uber mode : false\n",
      "16/01/30 03:46:38 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
      "16/01/30 03:46:38 INFO mapred.Task: Task 'attempt_local43858998_0001_m_000000_0' done.\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local43858998_0001_m_000000_0\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: Starting task: attempt_local43858998_0001_r_000000_0\n",
      "16/01/30 03:46:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2102fea2\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:46:38 INFO reduce.EventFetcher: attempt_local43858998_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:46:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local43858998_0001_m_000000_0 decomp: 1061171 len: 1061175 to MEMORY\n",
      "16/01/30 03:46:38 INFO reduce.InMemoryMapOutput: Read 1061171 bytes from map-output for attempt_local43858998_0001_m_000000_0\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1061171, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1061171\n",
      "16/01/30 03:46:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:46:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: Merged 1 segments, 1061171 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: Merging 1 files, 1061175 bytes from disk\n",
      "16/01/30 03:46:38 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:46:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:46:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:46:38 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:46:38 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:38 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:39 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:46:42 INFO streaming.PipeMapRed: Records R/W=30590/1\n",
      "16/01/30 03:46:42 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:42 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:42 INFO mapred.Task: Task:attempt_local43858998_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:42 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:42 INFO mapred.Task: Task attempt_local43858998_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:46:42 INFO output.FileOutputCommitter: Saved output of task 'attempt_local43858998_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_3/_temporary/0/task_local43858998_0001_r_000000\n",
      "16/01/30 03:46:42 INFO mapred.LocalJobRunner: Records R/W=30590/1 > reduce\n",
      "16/01/30 03:46:42 INFO mapred.Task: Task 'attempt_local43858998_0001_r_000000_0' done.\n",
      "16/01/30 03:46:42 INFO mapred.LocalJobRunner: Finishing task: attempt_local43858998_0001_r_000000_0\n",
      "16/01/30 03:46:42 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:46:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:46:43 INFO mapreduce.Job: Job job_local43858998_0001 completed successfully\n",
      "16/01/30 03:46:43 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2334178\n",
      "\t\tFILE: Number of bytes written=3951839\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=5692\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=30590\n",
      "\t\tMap output bytes=999987\n",
      "\t\tMap output materialized bytes=1061175\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15355\n",
      "\t\tReduce shuffle bytes=1061175\n",
      "\t\tReduce input records=30590\n",
      "\t\tReduce output records=99\n",
      "\t\tSpilled Records=61180\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=568328192\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5692\n",
      "16/01/30 03:46:43 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_3\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/enronemail_1h.txt \\\n",
    "-output /user/vamsi/hw2/output_2_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "The accuracy of the naive bayes classifier is 90.8163265306\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_3/part-00000  | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_3/part-00000  > prediction.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile hist.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = sys.argv[1]\n",
    "i = 0\n",
    "\n",
    "spam_post = []\n",
    "ham_post = []\n",
    "with open(filename,'r') as f:\n",
    "    for line in f:\n",
    "        if(i==0):  #skip over the first line\n",
    "            i=i+1\n",
    "        else:\n",
    "            fields = line.split()\n",
    "            spam_post.append(float(fields[3]))\n",
    "            ham_post.append(float(fields[4]))\n",
    "\n",
    "spam_post_np = np.asarray(spam_post)\n",
    "ham_post_np = np.asarray(ham_post)\n",
    "\n",
    "#print type(spam_post_np)\n",
    "#print spam_post_np.shape\n",
    "# histogram of the spam posterior\n",
    "\n",
    "#plt.hist(spam_post_np)\n",
    "#plt.title('Class-SPAM Posterior Probability')\n",
    "#plt.show()\n",
    "\n",
    "# histogram of the ham posterior\n",
    "#n, bins, patches = plt.hist(ham_post_np, 50, normed=1, facecolor='green', alpha=0.75)\n",
    "\n",
    "#plt.xlabel('Class - HAM')\n",
    "#plt.ylabel('Posterior Probability')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python hist.py prediction.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.4 Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#filename = sys.argv[1]\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = []  #empty list for words\n",
    "    email   = re.split('\\t+',line)\n",
    "    if(len(email)==4):\n",
    "        subject = re.split(r'[\\s.,]+',email[2].strip())\n",
    "        body    = re.split(r'[\\s.,]+',email[3].strip())\n",
    "        for s in subject:\n",
    "            words.append(s)    #appending list of words occuring in the subject\n",
    "        for b in body:\n",
    "            words.append(b)       #appending list of words occuring in the body\n",
    "        for word in words:\n",
    "                if(re.search('\\w+',word)):\n",
    "                    print \"%s,%s,%s\" %(email[0].strip(),email[1].strip(),word.strip())  #emit word,1 to the reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "email = {}\n",
    "words = {}\n",
    "spam_ec = 0\n",
    "ham_ec = 0\n",
    "total_ec = 0\n",
    "spam_wc = 0\n",
    "ham_wc = 0\n",
    "total_spam_wc = 0\n",
    "total_ham_wc = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r',',line)\n",
    "    email_id = line[0].strip()\n",
    "    spam = line[1].strip()\n",
    "    word = line[2].strip()\n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count':0,'ham_count':0}    \n",
    "    \n",
    "    if email_id not in email.keys():\n",
    "        email[email_id] = {'spam':0,'words':[],'count':0}\n",
    "    \n",
    "    if(int(spam)==1):\n",
    "        words[word]['spam_count'] += 1\n",
    "        total_spam_wc +=1\n",
    "    elif(int(spam)==0):\n",
    "        words[word]['ham_count'] += 1\n",
    "        total_ham_wc +=1\n",
    "    email[email_id]['count'] += 1\n",
    "    email[email_id]['spam'] = spam\n",
    "    email[email_id]['words'].append(word) \n",
    "\n",
    "#Computing priors\n",
    "\n",
    "#P_prior_spam = Number of emails containing spam/total number of emails\n",
    "#P_prior_ham = Number of emails containing ham/total number of emails\n",
    "for e in email.keys():\n",
    "    spam_ec += int(email[e]['spam'])\n",
    "    total_ec += 1 \n",
    "\n",
    "P_prior_spam = float(spam_ec)/float(total_ec)\n",
    "P_prior_ham = 1 - P_prior_spam\n",
    "\n",
    "#Computing conditionals\n",
    "#P(word|spam) and P(word|ham)\n",
    "\n",
    "cond_probs = {}\n",
    "\n",
    "for w in words.keys():\n",
    "    wc_spam = words[w]['spam_count']\n",
    "    wc_ham = words[w]['ham_count']\n",
    "    p_w_spam = (float(wc_spam)+1)/(total_spam_wc+1)    #conditional probability of word given spam\n",
    "    p_w_ham = (float(wc_ham)+1)/(total_ham_wc+1)       #conditional probability of word given ham\n",
    "    cond_probs[w] = {'spam':p_w_spam,'ham':p_w_ham}\n",
    "\n",
    "#Now, onto predictions\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for e in email.keys():\n",
    "    p_spam_cond = 0\n",
    "    p_ham_cond = 0\n",
    "    for word in email[e]['words']:\n",
    "        #if(cond_probs[word]['spam'] !=float(0) and cond_probs[word]['ham'] !=float(0)):\n",
    "        p_spam_cond += log(cond_probs[word]['spam'])         \n",
    "        p_ham_cond += log(cond_probs[word]['ham'])\n",
    "            \n",
    "    p_spam_given_word = log(P_prior_spam) + p_spam_cond\n",
    "    p_ham_given_word = log(P_prior_ham) + p_ham_cond\n",
    "    \n",
    "    if(p_spam_given_word > p_ham_given_word):\n",
    "        predict_spam = 1\n",
    "    else:\n",
    "        predict_spam = 0\n",
    "    \n",
    "    prediction.append([predict_spam,email[e]['spam'],e,p_spam_given_word,p_ham_given_word])\n",
    "\n",
    "correct =0\n",
    "total =0\n",
    "\n",
    "for p in prediction:\n",
    "    if(p[0] == int(p[1])):\n",
    "        correct += 1 \n",
    "    total +=1\n",
    "\n",
    "accuracy = 100*(float(correct)/total)\n",
    "    \n",
    "print \"The accuracy of the naive bayes classifier is %s\" %accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:46:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:46:51 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:46:51 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:46:51 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:46:52 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:46:52 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:46:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local570888281_0001\n",
      "16/01/30 03:46:52 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:46:52 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:46:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:52 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:46:52 INFO mapreduce.Job: Running job: job_local570888281_0001\n",
      "16/01/30 03:46:52 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:46:52 INFO mapred.LocalJobRunner: Starting task: attempt_local570888281_0001_m_000000_0\n",
      "16/01/30 03:46:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/enronemail_1h.txt:0+203981\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:46:52 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:46:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:46:52 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=100/8082/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:46:53 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:46:53 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:46:53 INFO mapred.MapTask: bufstart = 0; bufend = 999987; bufvoid = 104857600\n",
      "16/01/30 03:46:53 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26092040(104368160); length = 122357/6553600\n",
      "16/01/30 03:46:53 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:46:53 INFO mapred.Task: Task:attempt_local570888281_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
      "16/01/30 03:46:53 INFO mapred.Task: Task 'attempt_local570888281_0001_m_000000_0' done.\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local570888281_0001_m_000000_0\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: Starting task: attempt_local570888281_0001_r_000000_0\n",
      "16/01/30 03:46:53 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:46:53 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:46:53 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:46:53 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@344c22e1\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:46:53 INFO reduce.EventFetcher: attempt_local570888281_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:46:53 INFO mapreduce.Job: Job job_local570888281_0001 running in uber mode : false\n",
      "16/01/30 03:46:53 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:46:53 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local570888281_0001_m_000000_0 decomp: 1061171 len: 1061175 to MEMORY\n",
      "16/01/30 03:46:53 INFO reduce.InMemoryMapOutput: Read 1061171 bytes from map-output for attempt_local570888281_0001_m_000000_0\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1061171, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1061171\n",
      "16/01/30 03:46:53 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:46:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: Merged 1 segments, 1061171 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: Merging 1 files, 1061175 bytes from disk\n",
      "16/01/30 03:46:53 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:46:53 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:46:53 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:46:53 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:46:53 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:46:53 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:53 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:54 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:46:57 INFO streaming.PipeMapRed: Records R/W=30590/1\n",
      "16/01/30 03:46:57 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:46:57 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:46:57 INFO mapred.Task: Task:attempt_local570888281_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:46:57 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:46:57 INFO mapred.Task: Task attempt_local570888281_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:46:57 INFO output.FileOutputCommitter: Saved output of task 'attempt_local570888281_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_4/_temporary/0/task_local570888281_0001_r_000000\n",
      "16/01/30 03:46:57 INFO mapred.LocalJobRunner: Records R/W=30590/1 > reduce\n",
      "16/01/30 03:46:57 INFO mapred.Task: Task 'attempt_local570888281_0001_r_000000_0' done.\n",
      "16/01/30 03:46:57 INFO mapred.LocalJobRunner: Finishing task: attempt_local570888281_0001_r_000000_0\n",
      "16/01/30 03:46:57 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:46:58 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:46:58 INFO mapreduce.Job: Job job_local570888281_0001 completed successfully\n",
      "16/01/30 03:46:58 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2334178\n",
      "\t\tFILE: Number of bytes written=3954851\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=30590\n",
      "\t\tMap output bytes=999987\n",
      "\t\tMap output materialized bytes=1061175\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15355\n",
      "\t\tReduce shuffle bytes=1061175\n",
      "\t\tReduce input records=30590\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=61180\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=492830720\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "16/01/30 03:46:58 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_4\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/enronemail_1h.txt \\\n",
    "-output /user/vamsi/hw2/output_2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:47:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "The accuracy of the naive bayes classifier is 98.9795918367\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_4/part-00000  | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With laplace-add one smoothing, the accuracy is almost ~99% whereas without smoothing it was around ~91%. In general,\n",
    "smoothing helps by preventing a overfit of the training dataset. However , in this case since we train and test on the\n",
    "same data-set , the reason for differing accuracy is a bit different.\n",
    "\n",
    "Without smoothing (2.3), we ignored tokens which did not occur in the email document to avoid log(0), which is undefined. \n",
    "With smoothing, we did not have to ignore those tokens because we would never hit log(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.5. Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#filename = sys.argv[1]\n",
    "# input comes from STDIN (standard input)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    words = []  #empty list for words\n",
    "    email   = re.split('\\t+',line)\n",
    "    if(len(email)==4):\n",
    "        subject = re.split(r'[\\s.,]+',email[2].strip())\n",
    "        body    = re.split(r'[\\s.,]+',email[3].strip())\n",
    "        for s in subject:\n",
    "            words.append(s)    #appending list of words occuring in the subject\n",
    "        for b in body:\n",
    "            words.append(b)       #appending list of words occuring in the body\n",
    "        for word in words:\n",
    "                if(re.search('\\w+',word)):\n",
    "                    print \"%s,%s,%s\" %(email[0].strip(),email[1].strip(),word.strip())  #emit word,1 to the reducer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import re\n",
    "import sys\n",
    "from math import log\n",
    "\n",
    "email = {}\n",
    "words = {}\n",
    "spam_ec = 0\n",
    "ham_ec = 0\n",
    "total_ec = 0\n",
    "spam_wc = 0\n",
    "ham_wc = 0\n",
    "total_spam_wc = 0\n",
    "total_ham_wc = 0\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    line = re.split(r',',line)\n",
    "    email_id = line[0].strip()\n",
    "    spam = line[1].strip()\n",
    "    word = line[2].strip()\n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count':0,'ham_count':0}    \n",
    "    \n",
    "    if email_id not in email.keys():\n",
    "        email[email_id] = {'spam':0,'words':[],'count':0}\n",
    "    \n",
    "    if(int(spam)==1):\n",
    "        words[word]['spam_count'] += 1\n",
    "        total_spam_wc +=1\n",
    "    elif(int(spam)==0):\n",
    "        words[word]['ham_count'] += 1\n",
    "        total_ham_wc +=1\n",
    "    \n",
    "    email[email_id]['count'] += 1\n",
    "    email[email_id]['spam'] = spam\n",
    "    email[email_id]['words'].append(word) \n",
    "\n",
    "#Computing priors\n",
    "\n",
    "#P_prior_spam = Number of emails containing spam/total number of emails\n",
    "#P_prior_ham = Number of emails containing ham/total number of emails\n",
    "for e in email.keys():\n",
    "    spam_ec += int(email[e]['spam'])\n",
    "    total_ec += 1 \n",
    "\n",
    "P_prior_spam = float(spam_ec)/float(total_ec)\n",
    "P_prior_ham = 1 - P_prior_spam\n",
    "\n",
    "#Computing conditionals\n",
    "#P(word|spam) and P(word|ham)\n",
    "\n",
    "cond_probs = {}\n",
    "\n",
    "for w in words.keys():\n",
    "    wc_spam = words[w]['spam_count']\n",
    "    wc_ham = words[w]['ham_count']\n",
    "    p_w_spam = (float(wc_spam)+1)/(total_spam_wc+1)    #conditional probability of word given spam\n",
    "    p_w_ham = (float(wc_ham)+1)/(total_ham_wc+1)       #conditional probability of word given ham\n",
    "    if((wc_spam+wc_ham)<3):\n",
    "        cond_probs[w] = {'spam':p_w_spam,'ignore_word':1,'ham':p_w_ham}\n",
    "    else:\n",
    "        cond_probs[w] = {'spam':p_w_spam,'ignore_word':0,'ham':p_w_ham}        \n",
    "\n",
    "#Now, onto predictions\n",
    "\n",
    "prediction = []\n",
    "\n",
    "for e in email.keys():\n",
    "    p_spam_cond = 0\n",
    "    p_ham_cond = 0\n",
    "    for word in email[e]['words']:\n",
    "        if(cond_probs[word]['ignore_word'] ==0):\n",
    "            p_spam_cond += log(cond_probs[word]['spam'])\n",
    "            p_ham_cond += log(cond_probs[word]['ham'])\n",
    "            \n",
    "    p_spam_given_word = log(P_prior_spam) + p_spam_cond\n",
    "    p_ham_given_word = log(P_prior_ham) + p_ham_cond\n",
    "    \n",
    "    if(p_spam_given_word > p_ham_given_word):\n",
    "        predict_spam = 1\n",
    "    else:\n",
    "        predict_spam = 0\n",
    "    \n",
    "    prediction.append([predict_spam,email[e]['spam'],e,p_spam_given_word,p_ham_given_word])\n",
    "\n",
    "correct = 0\n",
    "total =0\n",
    "\n",
    "for p in prediction:\n",
    "    if(p[0] == int(p[1])):\n",
    "        correct += 1 \n",
    "    total +=1\n",
    "\n",
    "accuracy = 100*(float(correct)/total)\n",
    "    \n",
    "print \"The accuracy of the naive bayes classifier is %s\" %accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:47:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 03:47:03 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 03:47:03 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 03:47:03 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 03:47:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 03:47:04 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 03:47:04 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/01/30 03:47:04 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/30 03:47:04 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1371474831_0001\n",
      "16/01/30 03:47:04 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 03:47:04 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 03:47:04 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 03:47:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:47:04 INFO mapreduce.Job: Running job: job_local1371474831_0001\n",
      "16/01/30 03:47:04 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 03:47:04 INFO mapred.LocalJobRunner: Starting task: attempt_local1371474831_0001_m_000000_0\n",
      "16/01/30 03:47:04 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:47:04 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:47:04 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:47:04 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/vamsi/hw2/enronemail_1h.txt:0+203981\n",
      "16/01/30 03:47:04 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./mapper.py]\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 03:47:05 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: Records R/W=73/1\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: R/W/S=100/6490/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:47:05 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: \n",
      "16/01/30 03:47:05 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: bufstart = 0; bufend = 999987; bufvoid = 104857600\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26092040(104368160); length = 122357/6553600\n",
      "16/01/30 03:47:05 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 03:47:05 INFO mapred.Task: Task:attempt_local1371474831_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: Records R/W=73/1\n",
      "16/01/30 03:47:05 INFO mapred.Task: Task 'attempt_local1371474831_0001_m_000000_0' done.\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: Finishing task: attempt_local1371474831_0001_m_000000_0\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: Starting task: attempt_local1371474831_0001_r_000000_0\n",
      "16/01/30 03:47:05 INFO mapreduce.Job: Job job_local1371474831_0001 running in uber mode : false\n",
      "16/01/30 03:47:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 03:47:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 03:47:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 03:47:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 03:47:05 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5d3bb454\n",
      "16/01/30 03:47:05 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 03:47:05 INFO reduce.EventFetcher: attempt_local1371474831_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 03:47:05 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1371474831_0001_m_000000_0 decomp: 1061171 len: 1061175 to MEMORY\n",
      "16/01/30 03:47:05 INFO reduce.InMemoryMapOutput: Read 1061171 bytes from map-output for attempt_local1371474831_0001_m_000000_0\n",
      "16/01/30 03:47:05 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1061171, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1061171\n",
      "16/01/30 03:47:05 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 03:47:05 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:47:05 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 03:47:05 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:47:05 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:47:06 INFO reduce.MergeManagerImpl: Merged 1 segments, 1061171 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 03:47:06 INFO reduce.MergeManagerImpl: Merging 1 files, 1061175 bytes from disk\n",
      "16/01/30 03:47:06 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 03:47:06 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 03:47:06 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1061141 bytes\n",
      "16/01/30 03:47:06 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/Vamsi/Documents/W261/hw2/./reducer.py]\n",
      "16/01/30 03:47:06 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 03:47:06 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:06 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 03:47:09 INFO streaming.PipeMapRed: Records R/W=30590/1\n",
      "16/01/30 03:47:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 03:47:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 03:47:10 INFO mapred.Task: Task:attempt_local1371474831_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 03:47:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 03:47:10 INFO mapred.Task: Task attempt_local1371474831_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 03:47:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1371474831_0001_r_000000_0' to hdfs://localhost:9000/user/vamsi/hw2/output_2_5/_temporary/0/task_local1371474831_0001_r_000000\n",
      "16/01/30 03:47:10 INFO mapred.LocalJobRunner: Records R/W=30590/1 > reduce\n",
      "16/01/30 03:47:10 INFO mapred.Task: Task 'attempt_local1371474831_0001_r_000000_0' done.\n",
      "16/01/30 03:47:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local1371474831_0001_r_000000_0\n",
      "16/01/30 03:47:10 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 03:47:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 03:47:10 INFO mapreduce.Job: Job job_local1371474831_0001 completed successfully\n",
      "16/01/30 03:47:10 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2334178\n",
      "\t\tFILE: Number of bytes written=3957863\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=407962\n",
      "\t\tHDFS: Number of bytes written=61\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=101\n",
      "\t\tMap output records=30590\n",
      "\t\tMap output bytes=999987\n",
      "\t\tMap output materialized bytes=1061175\n",
      "\t\tInput split bytes=106\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=15355\n",
      "\t\tReduce shuffle bytes=1061175\n",
      "\t\tReduce input records=30590\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=61180\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=567279616\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=203981\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=61\n",
      "16/01/30 03:47:10 INFO streaming.StreamJob: Output directory: /user/vamsi/hw2/output_2_5\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop jar /Users/Vamsi/Downloads/hadoop-2.7.1/bin/hadoop-*streaming*.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.reduce.tasks=1 \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input /user/vamsi/hw2/enronemail_1h.txt \\\n",
    "-output /user/vamsi/hw2/output_2_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 03:47:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "The accuracy of the naive bayes classifier is 98.9795918367\t\n"
     ]
    }
   ],
   "source": [
    "!/Users/Vamsi/Downloads/hadoop-2.7.1/bin/hdfs dfs -cat /user/vamsi/hw2/output_2_5/part-00000  | head -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scikit_bm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scikit_bm.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import *\n",
    "import numpy as np\n",
    "\n",
    "filename = sys.argv[1]\n",
    "corpus = []   #empty list for words\n",
    "y_label = [] # output labels\n",
    "\n",
    "#Getting data in a format acceptable to scikit learn Multinomial Naive Bayes library\n",
    "with open(filename,\"r\") as f:\n",
    "    for line in f:\n",
    "        words_p = []\n",
    "        email   = re.split('\\t+',line)\n",
    "        if(len(email)==4):\n",
    "            entire_email = email[2] + email[3]\n",
    "            corpus.append(entire_email)\n",
    "            y_label.append(email[1])\n",
    "\n",
    "mNB = MultinomialNB()\n",
    "cv = CountVectorizer()\n",
    "\n",
    "corpus_np = np.asarray(corpus)\n",
    "y_label_np = np.asarray(y_label)\n",
    "sp_matrix = cv.fit_transform(corpus_np)\n",
    "\n",
    "model_mNB = mNB.fit(sp_matrix,y_label_np)\n",
    "\n",
    "pred = mNB.predict(sp_matrix)\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(pred)):\n",
    "    if(pred[i] == y_label_np[i]):\n",
    "        correct = correct+1\n",
    "    total=total+1\n",
    "    \n",
    "accuracy = 100*(float(correct)/float(total))\n",
    "print \"Accuracy using SciKit learn Multinomial Naive Bayes is %s\",accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using SciKit learn Multinomial Naive Bayes is %s 100.0\r\n"
     ]
    }
   ],
   "source": [
    "!python scikit_bm.py enronemail_1h.txt "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
